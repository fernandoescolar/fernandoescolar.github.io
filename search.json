[
  
    {
      "title"    : "Azure Functions con .NET 5",
      "category" : "",
      "tags"     : "azure-functions, azure, functions, net5, dotnet",
      "url"      : "/2021/03/24/azure-functions-net-5",
      "date"     : "2021-03-24 02:21:02Z",
      "content"  : "Ha tardado mas de lo que esperabamos pero ya esta aqui. Despues de meses de espera y escondiendo el anuncio dentro de un roadmap de las proximas versiones de .Net las dotnet Isolated Functions pasan a RTM. Y con ellas llega el soporte de .Net 5.0 en Azure Functions. Pero esta vez la migracion no va a ser tan sencilla como simplemente subir la version del runtime.Para conseguir compatibilidad con .Net 5 se ha cambiado la estrategia. En lugar de actualizar todos los paquetes y crear una nueva version de Azure Functions se ha creado un nuevo worker llamado dotnetisolated. Esto es una suerte de host que lanza nuestro ensamblado de funciones como un proceso aislado. Para la comunicacion entre el proceso del host y el de las funciones se ha utilizado un canal gRPC. La idea es que con este modelo podremos incluir todo lenguaje y framework para trabajar con Azure Functions. Aunque la realidad es que hoy en dia solo soporta .Net 5.Este nuevo modelo de desarrollo nos va a suponer unos cuantos cambios en nuestros desarrollos. Vamos a echar un vistazoTOCQuick StartPara empezar a trabajar con Azure Functions para .Net 5 tendremos que crear o adaptar un proyecto ya existente.Necesitaremos un archivo host.json    version 2.0Si ya tenemos el archivo local.settings.json tendremos que reemplazar el tipo de worker a dotnetisolated    IsEncrypted false    Values        FUNCTIONS_WORKER_RUNTIME dotnet        FUNCTIONS_WORKER_RUNTIME dotnetisolated        AzureWebJobsStorage UseDevelopmentStoragetrue        disabled falseO bien crear un archivo nuevo.Sobre el tipo de proyecto nos servira uno de tipo consola de .Net 5 anadiendo las referencias necesarias      net5.0    Exe    v3    true                          PreserveNewest              PreserveNewest      Never       Aqui definiremos la version 3 de Azure Functions y una serie de paquetes de nuget que nos proveeran del entorno necesario para ejecutar nuestro proyecto al amparo del proceso anfitrion de las funcionesMicrosoft.Azure.Functions.WorkerMicrosoft.Azure.Functions.Worker.SdkFinalmente tendremos que anadir o bien editar el archivo Program.cs para crear el host de las Isolated Functions y ejecutarlostatic async Task Mainstring args  var host  new HostBuilder                  .ConfigureFunctionsWorkerDefaults                  .Build  await host.RunAsyncPara anadir funciones lo primero que tenemos que saber es que los paquetes de atributos han cambiado. Ahora lo que se usa es Microsoft.Azure.Functions.Worker.Extensions.xxxxx don la xxxxx es el nombre o tipo de servicio queremos usar Http Timer ServiceBus EventHubs Storages...Puedes encontrar un listado completo aqui.Para nuestro ejemplo vamos a anadir una referencia al paquete Microsoft.Azure.Functions.Worker.Extensions.Http. De esta forma tendremos disponibles los bindings para el protocolo HTTP y podremos crear nuestra primera funcionpublic static class HttpFunction  FunctionnameofHttpFunction  public static HttpResponseData Run    HttpTriggerAuthorizationLevel.Anonymous get post Route  null    HttpRequestData req    FunctionContext context        var logger  context.GetLoggerHttpFunction      logger.LogInformationrequest arrived      var response  req.CreateResponseHttpStatusCode.OK      response.Headers.AddContentType textplain charsetutf8      response.WriteStringMis jugadores han corrido hoy por el campo como pollos sin cabeza. John Toshack      return response  A primera vista encontraremos nuevos tipos para definir un desencadenador HTTPEl tipo de resultado de la funcion ya no es IActionResult. Ahora es un objeto de tipo HttpResponseData.El tipo de objeto que recibe el trigger ya no es HttpRequest. Ahora es un nuevo objeto acorde con el tipo de respuesta HttpRequestData.El parametro de tipo FunctionContext. Desde este parametro vamos a tener acceso al contexto de ejecucion de la funcion. De esta manera si por ejemplo necesitamos el ILogger para escribir unas trazas podemos recogerlo de ahi.Para ver como podemos definir bindings de salida vamos a usar el paquete Microsoft.Azure.Functions.Worker.Extensions.Storage que entre otras cosas nos permitira acceder a colas de Azure Storage Accountpublic static class QueueFunction  FunctionnameofQueueFunction  QueueOutputoutqueue Connection  StorageConnectionString  public static string Run    QueueTriggerinqueue Connection  StorageConnectionString    string message    FunctionContext context      return Estoy tan feliz como uno puede estar. Pero he estado mas feliz. Ugo Ehiogu  Como podemos ver en el codigo anteriorEl parametro de salida de la funcion ira redirigido a una cola llamada outqueue.El desencadenador es un mensaje que viene de una cola llamada inqueue.Por lo tanto basta con anadir el out binding a la funcion para que el valor que se devuelve sea redirigido a un lugar concreto. Pero que hago para tener mas de un parametro de salidaPara obtener mas de un out binding tendremos que crear una clase nueva donde definiremos los diferentes tipos de outputs usando los mismos atributospublic class FunctionResult    QueueOutputoutqueue Connection  StorageConnectionString    public string Message  get set     public HttpResponseData HttpReponse  get set Despues solo tendremos que devolver una nueva instancia de este objeto que hemos creadopublic static class HttpMultiOutputFunction  FunctionnameofHttpMultiOutputFunction  public static FunctionResult Run    HttpTriggerAuthorizationLevel.Anonymous get post Route  null    HttpRequestData req    FunctionContext context        var response  req.CreateResponseHttpStatusCode.OK      response.Headers.AddContentType textplain charsetutf8      response.WriteStringSolo hay una posibilidad victoria derrota o empate. Franz Beckenbauer      return new FunctionResult              Message  Si llega a entrar el balon es gol. Michel Gonzalez        HttpReponse  response        DebuggingUna vez tenemos nuestras funciones creadas nos interesara probarlas. Para ejecutar este nuevo tipo de aplicacion no vamos a poder usar los tipicos comandos de ejecucion de Visual Studio o el dotnet run de siempre. Necesitaremos descargar si no la tenemos ya una tool llamada Azure Funcions Core Tools. Una vez instalada nos tendremos que dirigir a la ruta de nuestro proyecto y ejecutar el siguiente comandofunc startAl usar nuestro navegador para dirigirnos a una de nuestras funciones HTTP encontraremosDependency InjectionUna de las cosas mas engorrosas relacionadas con Azure Functions es la de anadir la inyeccion de dependencias de dotnet core. Hay que anadir un paquete especial y forzar una clase de arranque de la aplicacion donde entonces creas todo lo necesario...La buena noticia es que en este nuevo modelo de aplicacion ya tenemos la capacidad de usar el IServiceProvider de serie.Para probarlo vamos a crear la interfaz de un serviciopublic interface IQuotesService  string GetQuoteOfTheDayLa idea es que nos devuelva la frase del dia asi que vamos a crear una implementacion con varias frases y que nos devuelva una aleatoriapublic class QuotesService  IQuotesService  private static readonly string _quotes  new       Gaste mucho dinero en coches alcohol y mujeres. El resto lo he malgastado. George Best    A medida que uno va ganando cosas se hamburguesa. Carlos Tevez    Como todo equipo africano Jamaica sera un rival dificil. Edinson Cavani    Perdimos porque no ganamos. Ronaldo Nazario    El futbol es como el ajedrez pero sin dados. Lukas Podolski    No hay nada entre medio o eres bueno o eres malo. Nosotros estuvimos entre medio. Gary Lineker    Jugamos como nunca y perdimos como siempre. Alfredo Di Stefano    A veces en futbol tienes que marcar goles. Thierry Henry    El problema es que no ha entrado el balon. Sergio Ramos    public string GetQuoteOfTheDay        var index  new Random                      .Next0 _quotes.Length      return _quotesindex  Si quisieramos inyectar este servicio a una funcion hariamos algo parecido a estopublic class QuoteOfTheDayFunction  private readonly IQuotesService _service  private readonly ILogger _logger  public QuoteOfTheDayFunctionIQuotesService service ILogger logger      _service  service    _logger  logger    FunctionnameofQuoteOfTheDayFunction  public HttpResponseData Run      HttpTriggerAuthorizationLevel.Anonymous get post Route  null      HttpRequestData req        _logger.LogInformationgetting quote of the day    var response  req.CreateResponseHttpStatusCode.OK    response.Headers.AddContentType textplain charsetutf8    response.WriteString_service.GetQuoteOfTheDay    return response  Pasamos las dependencias como parametros de entrada de la funcion. Como detalle especial hemos quitado las referencias al FunctionContext y hemos anadido el ILogger en el constructor.Para anadir los servicios al IServiceCollection usaremos el metodo extensor ConfigureServices en la creacion del host en el archivo Program.csstatic async Task Mainstring args  var host  new HostBuilder                  .ConfigureFunctionsWorkerDefaults                  .ConfigureServicesservices                                         services.AddScoped                                    .Build  await host.RunAsyncAhora podremos volver a lanzar nuestra aplicacionfunc startY comprobaremos que hemos llamado correctamente al servicio de frases del diaMiddlewaresOtra de las sorpresas que traen consigo las dotnetisolated es la posibilidad de usar middlewares de una forma semejante a como los usamos en una aplicacion de aspnet core.Para ello crearemos un simple middleware implementando la interfaz IFunctionsWorkerMiddlewarepublic class DummyMiddleware  IFunctionsWorkerMiddleware  public async Task InvokeFunctionContext context FunctionExecutionDelegate next      var logger  context.GetLogger    logger.LogInformationMy Funcion context.FunctionDefinition.Name    await nextcontext  Desde el Invoke de un middleware tendremos acceso a FunctionContext y al delegado de la proxima ejecucion.Para usar este middleware nos dirigiremos al metodo ConfigureFunctionsWorkerDefaults del Main y lo anadiremos al pipeline de ejecucion con el metodo UseMiddlewarestatic async Task Mainstring args  var host  new HostBuilder                  .ConfigureFunctionsWorkerDefaultsapp                                       app.UseMiddleware                                    .Build  await host.RunAsyncTips  TricksComo presentacion todo esto esta muy bien pero seguro que en cuanto empeceis a enredar os entraran dudas. Entonces seguro que os vienen bien estos truquillosMejorando el debuggingHemos visto que podemos lanzar la ejecucion de nuestras funciones con el comando func start. Y si queremos realizar un debug tendremos que attachar Visual Studio al proceso dotnet que lanzamos mediante ese comando.El caso es que nos puede interesar que el proceso se espere a que haya un debugger en marcha para que asi no se nos cuele una ejecucion fuera de este contexto. Para ello usaremos el parametro dotnetisolateddebug.Y si tambien tenemos interes en tener mas trazas sobre las ejecuciones de nuestras funciones usaremos el parametro verbose.Asi que la llamada que posiblemente cubra nuestras expectativas a la hora de probar seria mas bienfunc start dotnetisolateddebug verboseAnadir variables de entornoExiste un problema cuando nos llevamos nuestras funciones a Azure y es que no se cargan por defecto las variables de entorno. Este tipo de variables son importantes porque es la forma que usa Azure para cargar la famosa configuracion que ponemos desde el portal.Es de suponer que esto lo cambiaran en un futuro proximo pero mientras tanto puedes usar el metodo ConfigureAppConfiguration y dentro de este AddEnvironmentVariables para poder anadirlasstatic async Task Mainstring args  var host  new HostBuilder                  .ConfigureFunctionsWorkerDefaults                  .ConfigureAppConfigurationconfig                                       config.AddEnvironmentVariables                                    .Build  await host.RunAsyncUsar la configuracionMuchas veces cuando estamos configurando el inyector de dependencias de nuestra aplicacion queremos que ciertos parametros los cargue de la configuracion. A ese fin usaremos una de las sobre escrituras del metodo ConfigureServices que vimos anteriormente. Si en lugar de pasarle un solo parametro usamos dos en el primero tendremos el contexto de configuracion. Desde ahi no nos costara buscar una seccion concreta y usarla como en cualquier otro desarrollostatic async Task Mainstring args  var host  new HostBuilder                  .ConfigureFunctionsWorkerDefaults                  .ConfigureServicescontext services                                         services.AddQuotesop  context.Configuration                                                      .GetSectionQuotes                                                      .Bindop                                    .Build  await host.RunAsyncPublicar en AzureSe pueden publicar las Azure Functions en .Net 5 usando la tool que usamos para lanzarla. La forma de llamarla seriafunc azure functionapp publish Tambien podemos usar metodos tradicionales como el web deploy subir un archivo zip o vincular un repositorio.Pero todo esto no nos garantiza que la funcion que estamos deployando tenga instalado .Net 5. Para conseguirlo podemos ejecutar el siguiente comando de azcliaz functionapp config set netframeworkversion v5.0 name  resourcegroup ConclusionesCreo que mas que las Azure Functions para .Net 5 que es una mejora totalmente esperada logica y obvia la gran noticia es el uso de un proceso aislado para su ejecucion.Tal cual lo veo en cuanto a las dotnetisolated todo son ventajasMenos conflictos un proceso aislado te permite tener dependencias diferentes a las del proceso anfitrion.Control total del proceso poder gestionar el inicio el final y lo que sucede en medio middlewares de la ejecucion de nuestras funciones.Inyeccion de dependencias sin malabarismos.Posible compatibilidad futura con cualquier plataforma sin necesidad de cambiar la version del host. Como por ejemplo .Net 6 o lo que este por venir.Bienvenidas sean."
    } ,
  
    {
      "title"    : "EF Core vs. Records",
      "category" : "",
      "tags"     : "ef, efcore, entity-framework, record, net5, dotnet",
      "url"      : "/2021/03/10/efcore-vs-records",
      "date"     : "2021-03-10 03:21:02Z",
      "content"  : "No cabe duda de que los tipos registro son la funcionalidad mas molona de C 9. Es el abrazo definitivo que necesitabamos para poder mezclar de una vez por todas el paradigma funcional con el orientado a objetos. Es la expresion con nombre propio de la inmutabilidad en el mundo .Net. Es una oda de rima libre que une record con with y alli donde no llega aparece init a la forma en la que nos gustaria programar.Todo son palabras bonitas a la hora de hablar de record. Porque ha venido para quedarse y utilizarse en todos los lugares posibles de nuestros desarrollos. Y te preguntaras en todosBueno existe una pequena aldea poblada de irreductibles ensamblados que aun no hemos probado. Nos referimos a Entity Framework Core EF. Y en este articulo lo vamos a poner a prueba.TLDR todo funciona bien pero el ChangeTracker no es fiable del todo debido a la inmutabilidad.TOCPrimeros pasosPara empezar a probar esta dupla de records con EF comenzaremos creando una Entity llamada Itempublic record ItemGuid Id string Name bool IsDonePara integrar este objeto con EF crearemos un nuevo DbContextpublic class TodoContext  DbContext  public TodoContextDbContextOptions options     baseoptions      public DbSet Items  get set Para probar nuestro escenario en un entorno lo mas aproximado al mundo real usaremos una instancia de Sql Express que tenemos instalada en la maquina localconst string SqlConnectionString  Server.SQLEXPRESSDatabaseTestTrusted_ConnectionTrueMultipleActiveResultSetstrueprivate TodoContext CreateContext  var options  new DbContextOptionsBuilder  options.UseSqlServerSqlConnetionString  return new TodoContextoptions.OptionsY finalmente nos aseguraremos de que creamos la base de datos que va a necesitar nuestra aplicacion con el metodo EnsureDatabaseIsCreatedprivate void EnsureDatabaseIsCreated  using var context  CreateContext  if context  null  context.Database  null      context.Database.EnsureCreated  Ahora ya estamos listos para empezar a probar las diferentes funcionalidadesInsertLa operacion de insertar en EF es muy sencilla. Consiste en instanciar la entidad que queremos anadir y llamar al metodo Add del DbSet correspondiente en nuestro contextoprivate const string TodoItemName  publish a new postprivate async Task CreateItemAsyncGuid id  using var context  CreateContext  var item  new Itemid TodoItemName false  context.Items.Additem  await context.SaveChangesAsyncPara comprobar que hemos creado la instancia en la base de datos vamos a realizar una comprobacion de lectura comprobando que el estado en la base de datos es el mismo que creamos en el metodo anteriorprivate async Task AssertItemAsyncGuid id bool isDone  using var context  CreateContext  var item  await context.Items.FindAsyncid  Assert.EqualTodoItemName item.Name  Assert.EqualisDone item.IsDonePara terminar creamos una prueba de integracion donde crearemos la base de datos un nuevo id y un Item en base de datos para comprobar que esta operacion ha tenido exitoFactpublic async Task IntegrationTest  EnsureDatabaseIsCreated  var id  Guid.NewGuid  await CreateItemAsyncid  await AssertItemAsyncid falseY al ejecutar Sorpresa Todo funciona correctamente si ningun problema Igual que si hubieramos usado class en lugar de record.DeleteLa operacion de borrar es otra de esas que EF nos facilita muchisimo. Su funcionamiento es igual que el de anadir una entidad cargamos la entidad y llamamos al metodo Remove dentro del DbSet correspondiente a nuestro objeto en el contextoprivate async Task DeleteItemAsyncGuid id  using var context  CreateContext  var item  await context.Items.FindAsyncid  context.Items.Removeitem  await context.SaveChangesAsyncUna vez lo hemos borrado podemos comprobar que al intentar volver a cargar la misma entidad obtendremos un valor nullprivate async Task AssertItemDoesNotExistsAsyncGuid id  using var context  CreateContext  var item  await context.Items.FindAsyncid  Assert.NullitemSi completamos nuestra prueba inicial con la operacion de borrado tendremos un codigo como el siguienteFactpublic async Task IntegrationTest  EnsureDatabaseIsCreated  var id  Guid.NewGuid  await CreateItemAsyncid  await AssertItemAsyncid false  await DeleteItemAsyncid  await AssertItemDoesNotExistsAsyncidY al ejecutar Sorpresa de nuevo Todo funciona correctamente si ningun problema Podemos crear y borrar entidades de la misma forma que haciamos antes.UpdateEstamos en racha dos de dos. Vamos a modificar el contenido de una entidad existente.El caso es que un record es un objeto inmutable. Para modificar cualquier propiedad de este objeto tenemos que crear una copia del mismo modificando el valor de la propiedad que queremos cambiar. Aqui es donde entra en juego la palabra clave withprivate async Task MarkItemAsDoneAsyncGuid id  using var context  CreateContext  var item  await context.Items.FindAsyncid  item  item with  IsDone  true   await context.SaveChangesAsyncAhora vamos a anadir a la prueba el marcar una tarea como terminada y ademas comprobaremos que en base de datos hemos realizado ese cambio con el metodo de comprobacion que creamos al principio AssertItemAsyncFactpublic async Task IntegrationTest  EnsureDatabaseIsCreated  var id  Guid.NewGuid  await CreateItemAsyncid  await AssertItemAsyncid false  await MarkItemAsDoneAsyncid  await AssertItemAsyncid true  await DeleteItemAsyncid  await AssertItemDoesNotExistsAsyncidAl ejecutar veremos que esta comprobacion falla. La prueba esperaba que despues de llamar a MarkItemAsDoneAsync el valor de la propiedad IsDone fuera true pero se ha encontrado con que no se han realizado cambios.Entity Framework Core usa un artefacto llamado ChangeTracker que se dedica a observar las entidades que estamos usando y a detectar los cambios que realizamos en ellas. Cuando llamamos a SaveChangesAsync sondea estos cambios y actua en consecuencia.Claro Como vamos a realizar la operacion de modificacion creando una nueva instancia de nuestro Item si no informamos debidamente a EF de que hemos cambiado la instanciaPara ello anadiremos la llamada al metodo Attach que nos ayudara a indicarle al ChangeTracker que hemos modificado esta entidadprivate async Task MarkItemAsDoneAsyncGuid id  using var context  CreateContext  var item  await context.Items.FindAsyncid  item  item with  IsDone  true   context.Items.Attachitem.State  EntityState.Modified  await context.SaveChangesAsyncAl ejecutarSystem.InvalidOperationException The instance of entity type Item cannot be tracked because another instance with the same key value for Id is already being tracked. When attaching existing entities ensure that only one entity instance with a given key value is attached.Resulta que como hemos cargado una instancia de nuestra entidad desde el contexto de EF y luego hemos anadido otra instancia que representa el mismo objeto el ChangeTracker ha decidido que esto no puede ser.La solucion esta en no usar el ChangeTracker cuando vamos a realizar modificaciones en una entidad de tipo record. Esto lo conseguiremos con el metodo AsNoTracking cuando recogemos valores desde la base de datosprivate async Task MarkItemAsDoneAsyncGuid id  using var context  CreateContext  var item  await context.Items                          .AsNoTracking                          .SingleOrDefaultAsyncx  x.Id  id  item  item with  IsDone  true   context.Items.Attachitem.State  EntityState.Modified  await context.SaveChangesAsyncPor fin funcionaEjemplo complejoMuy bonito el ejemplo simple de una sola entidad. Pero que pasaria si tuvieramos un grafo un poco mas complejoPara enfrentarnos con un escenario mas complejo vamos a anadir a un Item un listado de ItemTask con una relacion de Many To Onepublic record ItemTaskGuid Id string Namepublic record ItemGuid Id string Name bool IsDone ReadOnlyCollection TasksSi somos consecuentes con el comportamiento de un record deberiamos crear una ReadOnlyCollection para almacenar child items. Pero anadir una propiedad de navegacion en el constructor no le va a gustar demasiado a EF. Afortunadamente es casi lo mismo separarlopublic record ItemTaskGuid Id string Namepublic record ItemGuid Id string Name bool IsDone  public ReadOnlyCollection Tasks  get init El problema ahora lo tendremos con la ReadOnlyCollection. A EF no le gusta este tipo de objetos para hacer propiedades de navegacion. Lo que si que nos permitira es usar List o Collection. Para poder seguir siendo inmutables se nos ha ocurrido esta implementacionpublic record ItemTaskGuid Id string Namepublic record ItemGuid Id string Name bool IsDone  private List _tasks  new List  public IEnumerable Tasks      get  new ReadOnlyCollection_tasks    init  _tasks  value.ToList  Para anadir una ItemTask a un Item existente se nos ha complicado un poco. Tendremos que crear una nueva lista de ItemTask a partir de la lista de solo lectura que ya existe anadirle nuestro nuevo objeto y crear una copia del objeto Item que tenga este listado y no el anterior. Evidentemente como todo son objetos inmutables el ChangeTracker no se va a enterar de nada asi que tendremos que ignorarlo y adjuntar finalmente tanto el objeto padre como el objeto hijo que estamos creandoprivate async Task AddItemTaskAsyncGuid id  using var context  CreateContext  var item  await context.Items                          .AsNoTracking                          .IncludenameofItem.Tasks                          .SingleOrDefaultAsyncx  x.Id  id  var task  new ItemTaskGuid.NewGuid TodoItemTaskName  var list  item.Tasks.ToList  list.Addtask  item  item with  Tasks  list   context.Items.Attachitem.State  EntityState.Modified  context.Set.Attachtask.State  EntityState.Added  await context.SaveChangesAsyncY para modificar alguna de las propiedades de una ItemTask existente tendremos que volver a ignorar el ChangeTracker y actuar de la misma manera que en el ejemplo mas simpleprivate async Task ModifyItemTaskAsyncGuid id  using var context  CreateContext  var item  await context.Items                          .AsNoTracking                          .IncludenameofItem.Tasks                          .SingleOrDefaultAsyncx  x.Id  id  var task  item.Tasks.FirstOrDefault  task  task with  Name  TodoItemTaskUpdatedName   context.Set.Attachtask.State  EntityState.Modified  await context.SaveChangesAsyncSi os interesa el ejemplo completo podeis echarle un vistazo aquiCodigo fuente completo en gistConclusionesEn lo relacionado con record y Entity Framework Core parece ser que podemos anadir y borrar entidades o entidades hijas sin necesidad de cambiar nuestro codigo. Pero si queremos modificar una entidad existente tendremos que dejar de lado el ChangeTracker. Fin."
    } ,
  
    {
      "title"    : "ChorraTip: rutas windows en git bash",
      "category" : "",
      "tags"     : "bash, git, windows",
      "url"      : "/2021/03/03/chorra-tip-1-wcd-command",
      "date"     : "2021-03-03 13:20:00Z",
      "content"  : "No te ha pasado nunca que estas navegando por carpetas en el explorador de windows y copias la ruta para pegar en el terminal A mi si. Y resulta que esto no funciona con el bash de Git for Windows. Asi que he decidido hacer que si que funcione. Porque infravaloramos el poder que tiene el simple deseo de ser mas vago. Es una de las fuerzas mas importantes que dan impulso a la evolucion de la humanidad.Asi pues en el chorraTip del dia de hoy vamos a crear el comando wcd para el bash de Git for Windows. Solo tienes que seguir estos pasosCrea un archivo en CProgram FilesGitusrbin llamado wincd.Anade este contenidousrbinenv bashdirecho 1  sed e sg e s e sCccd dirAbre tu terminal de bash.Situate en tu carpeta HOME cd Crea un alias para hacer que el comando wincd se ejecute en el mismo proceso que el bash echo alias wcd. wincd  .bashrcYa puedes copiar la ruta del explorador de windows y llamar a wcd para situarte ahi directamente wcd CprojectsConsoleApp1binMenuda chorrada..."
    } ,
  
    {
      "title"    : "Raiders of the lost leak",
      "category" : "",
      "tags"     : "net5, dotnet, tools, monitoring, dump, memory",
      "url"      : "/video/2021/02/27/raiders-od-the-lost-leak",
      "date"     : "2021-02-27 02:16:54Z",
      "content"  : "Esto que has terminado la primera fase de tu proyecto. Subes la aplicacion a azure. Lo pones en produccion. Empiezan a llegar visitas. Todo genial. Se empieza a caer la aplicacion. Aleatoriamente. Miras la herramienta de monitoring y tienes la memoria a tope. Miras Application Insights y tienes un monton de excepciones de tipo OutOfMemoryException. Ay yai yai Tienes un memory leak.Video de la sesion de la Virtual NetCoreConf 2021 Raiders of the lost leak"
    } ,
  
    {
      "title"    : ".Net 5 Just Talking",
      "category" : "",
      "tags"     : "net5, dotnet, csharp",
      "url"      : "/video/2020/12/09/net-5-just-talking",
      "date"     : "2020-12-09 21:02:25Z",
      "content"  : "Al fin ha llego el dia de la publicacion de .Net 5  A unified platform. Y todo el mundo tiene dudas Es tan unificada como decian Es facil actualizar a esta nueva version Cuando sacan la 5.1 Al final mola tanto C 9 como parecia Que novedades trae todo esto Estamos ante un cambio de ciclo.Asi que la gente de CATzure nos junto a unos cuantos a debatir sobre el tema. En este video tengo el placer de debatir sobre .Net 5 junto unos pedazo de cracksEdu TomasCarlos LanderasDavid Rodriguez"
    } ,
  
    {
      "title"    : "Personalización de Windows Terminal",
      "category" : "",
      "tags"     : "terminal, bash, powershell, windows, git, cmd, cmder",
      "url"      : "/2020/11/18/custom-windows-terminal",
      "date"     : "2020-11-18 01:06:54Z",
      "content"  : "Todos sabemos que para conseguir el respeto de tus companeros y jefes solo hace falta tener un terminal que mole. Da igual si luego escribes usando solo los dedos indices. Si tu terminal parece 3173 es que eres 3173. Incluso si lo que usas en realidad es un producto de Microsoft con el nombre Windows delante.Si estoy hablando de Windows Terminal. Una herramienta digna de Neo aka senor Anderson cuando aun no tenia ni idea de que era Matrix fondos de pantalla molones colores tipo hacker e incluso la linea de prompt personalizada.Y hoy estas de suerte. En este articulo le vamos a dar un repaso desde la instalacion hasta la configuracion pasando por su integracion con otros terminales como bash o CmderOs presentamos la guia definitiva de como molar con Windows TerminalInstalar Windows TerminalSettingsProfilesSchemesKey bindingsIntegracionesPowerShellCommand PromptGit BashCmderDeveloper Command Prompt for VS 2019WSL  Azure Cloud ShellConclusionessettings.jsonInstalar Windows TerminalLa forma mas facil de conseguir Windows Terminal es tener un Windows 10 ir a la Microsoft Store e instalarlo desde ahi. Aunque para los mas atrevidos tambien podemos descargar el codigo fuente desde su pagina de github.Aunque seguro que tu que has llegado hasta este articulo oculto en la deep web preferiras instalarlo usando alguna tool de consola como wingetwinget install idMicrosoft.WindowsTerminal e chocolateychoco install microsoftwindowsterminal scoopscoop install windowsterminalUna vez instalado tendremos que dirigirnos en la barra superior al menu emergente que aparece al pulsar sobre el simbolo de flechita hacia abajo al lado del simbolo de sumaAsi encontraremos la opcion de Settings. Al presionar sobre ella se nos abrira un editor de texto o codigo depende de lo que tengas instalado en tu maquina donde podremos editar un archivo llamado settings.json.SettingsEl archivo de configuracion es bastante sencillo. En un principio lo que nos deberia interesar son las siguientes propiedades  schema httpsaka.msterminalprofilesschema  theme dark  defaultProfile 75de88dca61a4f9d8f17b820620e7163  profiles  ...   schemes  ...   keybindings  ... El schema es estatico y tiene que tener ese valor. Si navegamos a esa URL veremos mas detalles sobre las propiedades que vamos a comentar aqui y muchas otras que obviaremos.En cuanto al theme se puede elegir entre dark o light. Si eres un buen hacker la opcion correcta es dark.Y la propiedad defaultProfile la usaremos para escribir el guid del profile que se abrira por defecto.ProfilesEn profiles encontraremos un listado de los perfiles que estamos usando. Es decir los tipos de terminales que podemos abrir. Esos que aparecen encima de la opcion de Settings en el menu emergente.Cada uno de estos perfiles tendra mas o menos estos campos   datos generales   guid 0caa0dad35be5f56a8ffafceeeaa6101  commandline SystemRootSystem32cmd.exe  startingDirectory Cworkspace  name Command Prompt  icon Cpicturescmd.ico  hidden false   transparencia de ventana   useAcrylic true  acrylicOpacity 0.75   imagen de fondo   backgroundImage Cpicturessf2.gif  backgroundImageOpacity 0.2  backgroundImageStretchMode uniformToFill   colores   colorScheme Campbell   cursor   cursorShape bar  cursorColor FFFFFF   fuente   fontFace Cascadia Mono  fontSize 12   margenes   padding 8 8 8 8   retro effect   experimental.retroTerminalEffect trueEvidentemente hay mas pero aqui exponemos los que hacen que tu terminal mole. Los demas no te van a aportar tanto efecto Wow.Lo primero que definiremos seran los datos generales del perfilguid el codigo con el que vamos a identificar este perfil. Este es el valor que usaremos en defaultProfile para marcar el perfil que se abre por defecto en nuestro terminal.commandline el comando que ejecuta nuestro terminal.startingDirectory el directorio de inicio.name el nombre que aparece en el menu para definir este perfil.icon el icono que aparece en el menu para representar este perfil.hidden si queremos ocultar este perfil en el menu. Esta opcion sera util para una serie de perfiles que se crean automaticamente.Para definir si la ventana del terminal es medio transparente usaremosuseAcrylic true.Y para definir el grado de transparenciaacrylicOpacity con un numero decimal menor de 1.Si nuestra intencion es poner una imagen de fondo para el perfilbackgroundImage la ruta a la imagen de fondo que queremos usar. Y si puede ser un gif animado.backgroundImageOpacity un numero decimal menor de 1 que marca la opacidad.backgroundImageStretchMode puede tener los valores none fill uniform o uniformToFill.Los terminales pueden modificar los colores por defecto del sistema por unos que nos parezcan mas agradables o con una tematica semejante a la que esperamos para ello usaremos el campocolorScheme el nombre de alguno de los esquemas por defecto o de uno personalizado que hayamos definido en la seccion de schemes.El cursor que se muestra parpadeando en pantalla donde se indica que vamos a escribir es una parte fundamental de nuestro terminal. Aqui podremos elegir entre un punado de formas que son las mas comunes que podemos encontrarcursorShape  puede tener los valores bar underscore vintage filledBox o emptyBox.cursorColor especifica un color para el cursor.Y como no la fuente. Es un detalle muy importante de nuestro terminal poder determinar la fuente que vamos a usarfontFace el nombre de la fuente a usarfontSize el tamano en pixels de la fuente.Para terminar un punto importante. A veces nuestra consola puede aparecer demasiado cerca del borde de la ventana. Para ello contaremos con el siguiente campopadding aqui definiremos 4 numeros que representan la distancia con el borde de arriba derecho abajo e izquierdo.Y como bonus una caracteristica que podrian borrar en el futuro espero que no el efecto retro. Crea un efecto tipo televisor CRT con scanlines y brillosexperimental.retroTerminalEffect true o false.SchemesPor defecto Windows Terminal viene con una serie de esquemas de color por defecto. Pero eso no es para los 337 de verdad. Nosotros nos configuramos nuestros propios colores.Para definir un color en un terminal estandard podemos definir 8 colores y sus variantes oscuras tanto para el color del texto como el de fondo. Los esquemas de color nos dejaran definir estos 16 mas el color de fondo por y del texto por defecto.Unos ejemplos de esos esquemas de colores molones serianUbuntu Legit  name UbuntuLegit  foreground EEEEEE  background 2C001E  black 4E9A06  blue 3465A4  cyan 06989A  green 300A24  purple 75507B  red CC0000  white D3D7CF  yellow C4A000  brightBlack 555753  brightBlue 729FCF  brightCyan 34E2E2  brightGreen 8AE234  brightPurple AD7FA8  brightRed EF2929  brightWhite EEEEEE  brightYellow FCE94FDracula  name Dracula  foreground F8F8F2  background 282A36  black 21222C  blue BD93F9  cyan 8BE9FD  green 50FA7B  purple FF79C6  red FF5555  white F8F8F2  yellow F1FA8C  brightBlack 6272A4  brightBlue D6ACFF  brightCyan A4FFFF  brightGreen 69FF94  brightPurple FF92DF  brightRed FF6E6E  brightWhite FFFFFF  brightYellow FFFFA5Pero seguro que tienes en mente alguno mejor.Como se puede observar el valor de name es el que usaremos en el campo colorScheme  de nuestro terminal para referenciarlo.Key bindingsEsta parte es muy interesante. Aqui podemos definir combinaciones de teclas para realizar acciones de nuestro terminal.El listado completo de las acciones las podras encontrar en esta pagina.Esta seccion personalmente la uso para simular tmux con la salvedad de que mejor no usar el shortcut ctrl b porque hace que escribas la letra B varias veces antes de que se ejecute el comando que esperas. En mi caso me he decido por algo extrano como alt shift letra.Aqui podreis encontrar los comandos relacionados con duplicar y dividir en dos tanto horizontal como verticalmente los terminaleskeybindings       command       action splitPane      split auto      splitMode duplicate        keys alt shift d        command       action splitPane      split horizontal        keys alt shift h        command       action splitPane      split vertical        keys alt shift v        command closePane    keys alt shift q  El resultado final puede ser algo tan llamativo como estoIntegracionesLo mejor de ser un super hacker es usar un terminal que mole. Y el que mas mola es linux. Pero como estamos en una plataforma Windows vamos a listar unos cuantos que conozco a ver si alguno de ellos te sirvePowerShellEste perfil viene por defecto asi que tanto el guid como el resto de sus propiedades las he sacado de ahi  guid 0caa0dad35be5f56a8ffafceeeaa6101  name Windows PowerShell  commandline SystemRootSystem32WindowsPowerShellv1.0powershell.exe  startingDirectory USERPROFILE  icon msappxProfileIcons61c54bbdc2c6527196e7009a87ff44bf.pngPero el prompt por defecto de PowerShell digamos que no es lo mas molon del mundo. Si quieres parecer un tipo que sabe lo que esta haciendo lo mejor es cambiarlo.Para ello existen un par de modulos que tienes que instalar desde el propio PowerShellInstallModule poshgit Scope CurrentUserInstallModule ohmyposh Scope CurrentUserLuego podras activar estos modulosSetPromptY finalmente elegir el tema que mas te gusteSetTheme ParadoxEn la pagina web del modulo encontraras un listado completo de temas ademas de mas detalles sobre como crear uno propio personalizado.Para automatizar este setup puedes crearte un archivo de PROFILEif TestPath Path PROFILE   NewItem Type File Path PROFILE Force notepad PROFILEy en el editor de archivo que se abre anadeImportModule poshgitImportModule ohmyposhSetTheme ParadoxCommand PromptOtro de los perfiles que vienen por defecto es el cmd de toda la vida  guid 0caa0dad35be5f56a8ffafceeeaa6101  name Command Prompt  commandline cmd.exe  startingDirectory USERPROFILE  icon msappxProfileIcons0caa0dad35be5f56a8ffafceeeaa6101.pngLo que mas te interesa al respecto de este perfil es anadir la propiedad hidden la ponemos a true y listo.Pero si aun asi quieres seguir adelante te comentaremos como intentar hacer todo esto un poco mas bonito. Basicamente vamos a cambiar el prompt.Si escribes esto en el command prompt te encontraras con una respuesta con simbolos rarosC echo PROMPTPGPara descifrar estos simbolos podemos usar esta tabla  A    Ampersand  B    pipe o tuberia  C    parentesis izquierdo  D   Fecha actual  E   Escape code ASCII code 27  F    parentesis derecho  G    el simbolo de mayor que  H   Backspace borra el caracter anterior  L    el simbolo de menor que  N   La unidad actual  P   La unidad y la ruta actual  Q    simbolo de igual  S     espacio  T   Hora actual  V   Version de Windows  _   Salto de linea      el simbolo del dolarAsi que deducimos que lo que estamos mostrando es la unidad y la ruta actual seguido de un simbolo mayor queCFolderEn este punto podriamos cambiar esta variable para que pusiera otra cosaC set PROMPTE31mfer E33msays E32mPE37m_SPara jugar con los colores y algunas cosas mas tendremos que echar un vistazo los ANSI escape codes.Para cambiarlo de forma global tienes que ir a las propiedades de My Pc de ahi ir a Advanced system settings en la ventana que aparece seleccionar la pestana Advanced y dentro de esta buscar el boton Environment Variables.Ahora se abrira una ventana donde podemos anadir o editar variables del sistema. Si no existe tendras que crear una nueva con nombre PROMPT y el valor que mas te gusteLa proxima vez que arranques un cmd veras los resultados.Git BashAqui es donde empieza lo bueno. Si no tienes Git for Windows para de leer y cierra la ventana del navegador.Existe una forma de tener nuestro shell favorito dentro de Windows Terminal  guid 75de88dca61a4f9d8f17b820620e7163  name Bash  commandline PROGRAMFILESgitusrbinbash.exe i l  startingDirectory USERPROFILE  icon CProgram FilesGitmingw64sharegitgitforwindows.icoEl guid me lo he inventado pero ya lo estas poniendo en la propiedad defaultProfile de la raiz del settings.json.Los demas parametros encajaran si tienes todo instalado en las carpetas por defecto si no tendras que modificar las rutas.Para cambiar el prompt tendremos que abrir un bash y ejecutar estos comandoscd etcprofile.dnotepad gitprompt.shAhora se abrira la aplicacion de notepad con el shell script de arranque por defecto. Lo importante es modificar el valor de la variable PS1 que es la que contiene el prompt...PS1PS103332m        change to greenPS1PS1fer                 ferPS1PS103335m        change to purplePS1PS1says                show saysPS1PS103333m        change to brownish yellowPS1PS1w                  current working directory...Este codigo a mitad del archivo creara el prompt tan guay que veis en la introduccion de este articuloPara jugar con los colores y algunas cosas mas tendremos que echar un vistazo los ANSI escape codes.CmderMuchos aficionados a Windows ya usan terminales alternativos como Cmder. La buena noticia es que tambien puedes usar estas shells en Windows Terminal. En mi caso tengo instalado Cmder en la carpeta CCmder y la configuracion del perfil queda asi  guid 32b1ecb13e1546069683e0d644adb2ea  name Cmder  commandline cmd.exe k call CCmdervendorinit.bat  startingDirectory USERPROFILE  icon CCmdericonscmder.icoPara personalizar el prompt nos tendremos que dirigir al directorio donde tenemos instalado Cmder entrar en la carpeta vendor y editar el archivo clink.lua.Ahi encontraremos una funcion llamada set_prompt_filter donde viene la creacion del prompt de Cmder. Hay que tener en cuenta que viene en LUA. Si dejamos esta funcion con este codigolocal function set_prompt_filter  local old_prompt  clink.prompt.value  local cwd  old_promptmatch..  if cwd  nil then cwd  clink.get_cwd end  local env  old_promptmatch. .   if env  nil then env  old_promptmatch. .  end  local blackbgx1b140m  local green  x1b132m  local yellow  x1b133m  local magenta  x1b135m  local cyan  x1b136m  local default  x1b139m  local reset  x1b0m  local cmder_prompt  blackbg..green..fer ..magenta..in ..yellow..cwd ..cyan..githgsvn n..default..lamb ..reset  local lambda    cmder_prompt  string.gsubcmder_prompt cwd verbatimcwd  if env  nil then    lambda  ..env.. ..lambda  end  clink.prompt.value  string.gsubcmder_prompt lamb verbatimlambdaendObtendremos un terminal semejante a estePara jugar con los colores y algunas cosas mas tendremos que echar un vistazo los ANSI escape codes.Developer Command Prompt for VS 2019Si eres desarrollador y usas Visual Studio seguro que quieres tener dentro del terminal el famoso Developer Command Prompt for VS 2019. Pues es muy facil  guid d192740d3258445c800f5258a0a4bb81  name Developer Command Prompt for VS 2019  commandline cmd.exe k CProgram Files x86Microsoft Visual Studio2019CommunityCommon7ToolsVsDevCmd.bat  startingDirectory USERPROFILE  icon msappxProfileIcons0caa0dad35be5f56a8ffafceeeaa6101.pngEl guid es de mi propia cosecha y la forma de gestionar el prompt es la misma que para el Command Prompt.WSL  Azure Cloud ShellHay una serie de perfiles que apareceran Windows Terminal si tienes instalados los componentes necesarios. Estos son el WSL Windows Subsystem for Linux y el Azure Cloud Shell.Si por alguna razon los necesitas y no aparecen en el listado este codigo te ayudara a tenerlos  guid c6eaf9f432a75fdcb5cf066e8a4b1e40  name Ubuntu18.04  source Windows.Terminal.Wsl  guid b453ae624e3d5e58b9890a998ec441b8  name Azure Cloud Shell  source Windows.Terminal.AzureConclusionesAqui no estan ni mucho menos todas las opciones de configuracion y personalizacion que nos permite Windows Terminal. Solo las que te van a resultar mas importantes para poder fardar.Sigue estos consejos y la proxima vez que compartas escritorio abre un Windows Terminal. Deja que tus companeros admiren lo chulo que es. Hazles saber que tu eres 337 y que ellos son unos p n00bs.O si no copiate mi settings.json que es mas rapido desde este enlace."
    } ,
  
    {
      "title"    : "Workshop Azure 100M",
      "category" : "",
      "tags"     : "azure, webapp, appservice",
      "url"      : "/video/2020/10/19/workshop-azure",
      "date"     : "2020-10-19 02:16:54Z",
      "content"  : "Tengo una aplicacion Web que podria calificarse en todo sentido como una web normal. Una WebApp que vive en un pacifico hosting en un data center empresarial donde se ha desarrollado al igual que otras tantas aplicaciones. Pero ese dia no fue para nada normal nuestra aplicacion deberia subir a la nube y aguantar 100 millones de usuarios.Aqui una guia con las cosas que aprendimos.Video del workshop de la Virtual NetCoreConf The Hitchhikers Guide to Azure WebApps."
    } ,
  
    {
      "title"    : "SOLID menos mola",
      "category" : "",
      "tags"     : "solid, best-practices",
      "url"      : "/video/2020/10/15/solid-menos-mola",
      "date"     : "2020-10-15 02:16:54Z",
      "content"  : "He de reconocer que el trabajo de Uncle Bob Robert C. Martin me ayuda a ser mejor programador. Cada vez que leo uno de sus libros o veo una de sus charlas aprendo algo. Incluso si no es la primera vez que lo hago. Y los principios S.O.L.I.D. no son una excepcion.No obstante despues de varios anos obsesionado con aplicar cada uno de esos principios de la forma mas adecuada he ido descubriendo que no todo encaja tan bien como esperaba. Algo no funciona como deberia cuando puedes mejorar tu codigo saltandote esas cinco normas.Aqui podras encontrar los articulos originales que escribio Uncle Bob. Me he permitido copiarlos de web archiveSRPOCPLSPISPDIPY aqui los articulos que escribi en este mismo sitio sobre cada uno de ellosSRP menos molaOCP menos molaLSP menos molaISP menos molaDIP menos molaXP. Simple Design. SOLID.In that order"
    } ,
  
    {
      "title"    : "SOLID menos mola (D)",
      "category" : "",
      "tags"     : "solid, best-practices",
      "url"      : "/2020/10/14/solid-menos-mola-d",
      "date"     : "2020-10-14 02:16:54Z",
      "content"  : "La D de SOLID se refiere al principio de inversion de dependencia o DIP por sus siglas en ingles Dependency Inversion Principle. Se puede resumir con que una clase debe depender de las abstracciones no de las concreciones. Aunque Robert C. Martin es mucho mas especifico y realiza una definicion dividida en dos partesLos modulos de alto nivel no deben depender de modulos de bajo nivel. Ambos deben depender de las abstracciones.Las abstracciones no deben depender de los detalles. Los detalles deben depender de las abstracciones.Uncle Bob lo explica mediante un ejemplo en su ensayo sobre DIP que voy a intentar traducir a C a continuacionpublic class Copier  private const char EndOfFile  default  public void Copy      char c    while c  ReadCharFromConsole  EndOfFile      WriteCharInPrinterc    public char ReadCharFromConsole  ...   public void WriteCharInPrinterchar c  ... Tenemos una clase llamada Copier. Esta clase realiza la copia char a char de lo que escribes en consola con direccion la impresora. En este caso nuestra clase tiene una dependencia directa con los modulos de bajo nivel para la lectura de la consola y para la escritura en la impresora. En .Net esas capacidades las encontramos en los artefactos System.Console y System.Drawing.Printing.PrintDocument.Si quisieramos darle la capacidad de escribir en la impresora o en un archivo en disco podriamos ampliar nuestro codigo conpublic enum OutputDevice  Printer Disk OutputDevice es una enumeracion que nos aportara el destino de los caracteres que estoy escribiendo. Asi que tendremos que modificar nuestra clase original con algo parecido a estopublic void CopyOutputDevice device  char c  while c  ReadCharFromConsole  EndOfFile    if device  OutputDevice.Printer      WriteCharInPrinterc    else      WriteCharInDiskcpublic char ReadCharFromConsole  ... public void WriteCharInPrinterchar c  ... public void WriteCharInDiskchar c  ... Este cambio tambien implicaria que estariamos anadiendo una dependencia de System.IO.File el artefacto mediante el cual podemos gestionar archivos en disco.Al usar tantos modulos externos dentro de nuestra clase Copier vamos a tener un problema a la hora de realizar los unit tests. Una prueba unitaria no deberia crear archivos en disco o realizar una impresion. Y sobre todo una prueba unitaria no deberia quedarse a la espera de que el usuario escriba algo en la consola.Ademas tenemos el problema de que cada nuevo comportamiento que le queramos anadir sera una nueva dependencia un nuevo metodo y una nueva condicion en el bucle.Todo esto se soluciona como dice el punto A del principio de inversion de dependencia disenando abstraccionespublic interface ICharReader    char Readpublic interface ICharWriter    void Writechar cY haciendo que nuestros modulos dependan de estaspublic class Copier  private const char EndOfFile  defaultchar  private readonly ICharReader _reader  private readonly ICharWriter _writer  public CopierICharReader reader ICharWriter writer        _reader  reader      _writer  writer    public void Copy      char c    while c  _reader.Read  EndOfFile      _writer.Writec  Gracias a esta implementacion podriamos hacer pruebas unitarias. Abstraernos de las dependencias seria sencillo usando test doublesFactpublic void Copier_Does_not_write_When_reads_EOF  var reader  new Mock  var writer  new MockMockBehavior.Strict  reader.Setupx  x.Read.Returnsdefaultchar  var target  new Copierreader.Object writer.Object  target.Copy  writer.VerifyY tambien anadimos la posibilidad de realizar composicion. Esto nos ayuda a dotar a nuestro codigo de mas opciones de lectura y escriturapublic class ConsoleCharReader  ICharReader  ... public class PrinterCharWriter  ICharWriter  ... public class FileCharWriter  ICharWriter  ... var copyToPrinter  new Copiernew ConsoleCharReader new PrinterCharWritervar copyToFile     new Copiernew ConsoleCharReader new FileCharWriterPor lo tanto podemos deducir que los modulos de alto y bajo nivel dependan de las abstracciones aporta mucho valor a nuestro codigo. Desacopla artefactos los hace testeables y nos ayuda a aplicar patrones con mas facilidad.Pero que pasaria si en lugar de leer desde la consola quisiera leer los caracteres que me vienen en una conexion remota Posiblemente tendria que crear una nueva implementacion de ICharReaderpublic class SocketCharReader  ICharReader IDisposable  private readonly NetworkStream _reader  public SocketCharReaderSocket socket      _reader  new NetworkStreamsocket    public char Read      int i    if i  _reader.ReadByte  0      return chari    return default    public void Dispose      _reader.Dispose  El caso es que si pensamos en sacar un buen rendimiento a la hora de leer desde un socket escribir en un archivo o enviar a la impresora todo este codigo es ineficiente. Para este tipo de funcionalidades es mejor realizar lecturas y escrituras en bloque. Por ejemplo usando un buffer Readerpublic int Readbyte buffer  return _streamReader.Readbuffer 0 buffer.Length Writerpublic void Writebyte buffer int length  _streamWriter.Writebuffer 0 length Copierpublic void Copy  var buffer  new byte2048  int read  while read  _reader.Readbuffer 0 buffer.Length  0      _writer.Writebuffer 0 read  Pero nuestra abstraccion esta pensada para leer caracter a caracter conforme se van pulsado teclas desde la consola del sistema. Asi que eso de que las abstracciones no deben depender de los detalles quiza si buscamos realizar esta operacion en un tiempo razonable no encajaria del todo.Y es que la eficiencia y el buen rendimiento se encuentra en los detalles. Y esta es la parte que creo que no funciona del todo dentro de DIP.PatronesPara aplicar el principio de inversion de dependencia encontramos una serie de patrones de diseno que nos pueden ayudar. Los mas comunes en este ambito sonService LocatorEste patron consiste en crear un artefacto donde almacenar todas las dependencias de nuestros modulospublic class ServiceLocator  private static readonly Dictionary _services  static ServiceLocator      _services  new Dictionary           typeofICharReader new ConsoleCharReader        typeofICharWriter new PrinterCharWriter         public static T GetService      if _services.ContainsKeytypeofT      return T_servicestypeofT    throw new ArgumentExceptionnameofT Type not found  De esta forma en nuestras clases podemos ir a buscar las dependencias a ServiceLocator y de esta forma no depender de concreciones solo de abstraccionespublic class Copier  private const char EndOfFile  defaultchar  private readonly ICharReader _reader  private readonly ICharWriter _writer  public Copier        _reader  ServiceLocator.GetService      _writer  ServiceLocator.GetService  Todo esto salvo por una particularidad si aplicamos este patron el propio ServiceLocator sera una dependencia a lo largo de todo nuestro codigo haciendo que todos nuestros modulos dependan de el.Dependency InjectionEl design pattern de inyeccion de dependencias nos ayuda a crear objetos sin necesidad de tener que definir sus dependencias. Estas seran inyectadas por un constructor o una fabrica.Existen diferentes formas de inyectar dependenciasDI ConstructorLa mas comun que nos podemos encontrar es por el constructor. La idea es que el constructor de nuestra clase contenga una referencia a las abstracciones de las que dependepublic class Copier   ...  public CopierICharReader reader ICharWriter writer        _reader  reader      _writer  writer     ...Y luego estas seran inyectadas al ser instanciadapublic class CopierFactory  public Copier CreateConsoleToPrinterCopier      return new Copiernew ConsoleCharReader new PrinterCharWriter  DI PropiedadEsta es una idea semejante al constructor pero usando propiedades. La idea es crear tantas propiedades como dependencias tenga un modulopublic class Copier   ...  public ICharReader Reader  get set   public ICharWriter Writer  get set    ...Y asignarlas nada mas crear la instanciapublic class CopierFactory  public Copier CreateConsoleToPrinterCopier      return new Copier          Reader  new ConsoleCharReader      Writer  new PrinterCharWriter      DI MetodoY por ultimo esta la inyeccion de dependencias via metodo en la que crearemos metodos que nos permiten asignar las dependencias de nuestro modulo que luego llamaremos al instanciarlopublic class Copier   ...  public void SetReaderICharReader reader  ...   public void SetWriterICharWriter writer  ...    ...public class CopierFactory  public Copier CreateConsoleToPrinterCopier      var copier  new Copier    copier.SetReadernew ConsoleCharReader    copier.SetWriternew PrinterCharWriter    return copier  Si ahora cogemos el patron de service locator lo mezclamos con el de dependency injectionpublic interface IServiceLocator  T GetServicepublic interface IServiceRegister  void RegisterFunc factorypublic class DependencyInjector  IServiceLocator IServiceRegister  private static readonly ConcurrentDictionary _services  new   ConcurrentDictionary  public T GetService      if _services.ContainsKeytypeofT      return T_servicestypeofT    throw new ArgumentExceptionnameofT Type not found    public void RegisterFunc factory      var instance  factorythis    _services.AddOrUpdatetypeofT instance a b  instance  Ahora podriamos tener un inicio de nuestra aplicacion parecido a este build dependency treevar dependencyInjector  new DependencyInjectordependencyInjector.Register_  new ConsoleCharReaderdependencyInjector.Register_  new PrinterCharWriterdependencyInjector.RegisterserviceLocator  new CopierserviceLocator.GetService serviceLocator.GetService run applicationvar copier  dependencyInjector.GetServicecopier.CopyInversion of ControlOtra vuelta de tuerca sobre la misma idea y la que mas personas conoceran es el patron de inversion de control o IoC.En este patron la idea es tener un contenedor que se asemeje a la clase DependencyInjector que hemos implementado anteriormente. A este contenedor le anadimos la capacidad de crear reutilizar y destruir estas dependencias que registramos. Y terminamos controlando el flujo de nuestra aplicacion mediante su uso.Asi que si pensamos en que control se invierte cuando usamos IoC nos referimos a que en lugar de controlar nosotros el ciclo de vida de los objetos de nuestra aplicacion delegamos esto en el contenedor de IoC.En dotnet core este patron ya forma parte del propio framework usando el paquete de nuget Microsoft.Extensions.DependencyInjection. Pero seguro que muchos ya lo habeis visto en librerias de terceros como Autofac Ninject Unity ...Para ponerlo a prueba usando la libreria del framework de dotnet core primero usariamos un objeto de tipo ServiceCollection para registrar nuestras dependencias y su ciclo de vidavar serviceCollection  new ServiceCollectionserviceCollection.AddSingletonserviceCollection.AddScopedserviceCollection.AddTransientComo se puede observar en este ejemploConsoleCharReader ha sido declarado como singleton es decir que solo existira una instancia lo usemos las veces que lo usemos.PrinterCharWriter se instanciara uno por ambito. Si por ejemplo estamos en asp.net core el ambito seria lo que dure una request. Pero podemos crear nuevos ambitos usando el metodo serviceProvider.CreateScope.Copier se instanciara uno por cada llamada que hagamos independientemente del ambito o de que una dependencia sea singleton.Despues creariamos un objeto tipo service locator al que han llamado IServiceProvider a partir del cual podriamos resolver nuestros serviciosvar serviceProvider  serviceCollection.BuildServiceProviderserviceProvider.GetServicecopier.CopyEste tipo de patrones nos obligan a ser muy cuidadosos configurando el ciclo de vida de nuestros artefactos porque podemos tener muchos problemas si no cuidamos la jerarquia de duracion. Es decir que los objetos solo dependan de artefactos con un ciclo de vida igual o mayor al que estamos usando y no al reves.Hay vida despues de DIPHe de reconocer que el trabajo de Uncle Bob Robert C. Martin me ayuda a ser mejor programador. Cada vez que leo uno de sus libros o veo una de sus charlas aprendo algo. Incluso si no es la primera vez que lo hago. Y los principios SOLID y en concreto el DIP no son una excepcion. Mi consejo es queSi consideras que este principio es una mierda acostumbrate a seguirlo siempre.Si siempre lo sigues y consideras que este articulo es una mierda sigue aplicandolo.Y si lo has aplicado hasta la extenuacion y te surgen dudas sigue leyendo.Depender de abstraccionesComo hemos podido ver depender de abstracciones viene muy bien a nuestro codigo. Es muy util y nos aporta muchas ventajas. El problema que encontramos aqui es como se consigue aplicar este patron. Por lo general todo el mundo usa IoC y quiza no sea necesario siempre. Por ejemplo una Azure Function o un programa de consola mono hilo es muy posible que por usar inversion de control estemos anadiendo una complejidad innecesaria a nuestra aplicacion.En mi opinion siempre que podamos es mejor depender de abstracciones y no de concreciones. Sobre todo si estamos interesados en tener pruebas unitarias o codigo desacoplado. Pero no debemos caer el en antipatron del martillo dorado IoC no es siempre la respuesta a DIP es solo una de ellas.Las abstracciones no deben depender de los detallesEsta claro que si hacemos que las abstracciones no dependan de los detalles nos va a aportar muchos beneficios. Sobre todo que nuestro codigo sera mas susceptible a ser reutilizado en otras partes o incluso en otros proyectos. Pero quiza este objetivo no deberia ser lo primero que debemos buscar a la hora de realizar un programa. Quiza la reutilizacion de codigo deberia surgir como un subproducto de dos proyectos que usan las mismas tecnologias. Y quiza debamos dejar de obsesionarnos con lo que sucedera manana y prepararnos para lo que tenemos hoy.En mi opinion esta parte del principio de DIP tiene muchos casos en los que es mejor no aplicarla ya que nos puede obligar a tener un mal diseno y por tanto una pobre mantenibilidad y sobre todo un mal rendimiento.El caso es que esta es mi opinion a dia de hoy. Lo que me pueda parecer manana ya lo veremos..."
    } ,
  
    {
      "title"    : "SOLID menos mola (I)",
      "category" : "",
      "tags"     : "solid, best-practices",
      "url"      : "/2020/10/06/solid-menos-mola-i",
      "date"     : "2020-10-06 02:16:54Z",
      "content"  : "La I de SOLID se refiere al principio de segregacion de interfaz o ISP por sus siglas en ingles Interface Segregation  Principle. Se puede definir como que muchas interfaces especificas son mejores que una interfaz de proposito general. O como diria Robert C. Martin los clientes no deben ser forzados a depender de interfaces que no utilizan.Uncle Bob lo explica mediante un ejemplo en su ensayo sobre ISP que voy a intentar simplificar y traducir a C a continuacion.Estamos desarrollando la interfaz del cajero automatico. Este cajero tiene diferentes formas de comunicarse con un ser humanoUn formato visual.Una interfaz hablada.Y otra en braille.Lo ideal seria tener este conjunto de artefactosinterface IUserInterface  ... class ScreenUI  IUserInterface  ... class SpeechUI  IUserInterface  ... class BrailleUI  IUserInterface  ... Ademas en nuestro cajero podemos realizar operaciones de deposito de dinero transferencia o retirada de efectivointerface ITransaction  ... class Deposit  ITransaction  public DepositIUserInterface ui  ...   ...class Transfer  ITransaction  public TransferIUserInterface ui  ...   ...class Withdraw  ITransaction  public WithdrawIUserInterface ui  ...   ...Entonces aqui es muy probable que la interfaz IUserInterface tenga un contrato semejante ainterface IUserInterface    void RequestDepositAmount...    void RequestTransferAmount...    void RequestWithdrawlAmount...La coyuntura con la que nos enfrentamos es que una Transfer no tendria por que conocer el metodo RequestWithdrawlAmount. Y lo mismo con RequestDepositAmount. Tampoco le interesaria a Withdraw conocer el metodo RequestTransferAmount. Y asi sucesivamente. Para darle solucion lo que propone Uncle Bob es que cojamos IUserInterface y la segreguemos en interfaces mas pequenasinterface IDepositUI    void RequestDepositAmount...interface ITransferUI    void RequestTransferAmount...interface IWithdrawlUI    void RequestWithdrawlAmount...interface IUserInterface  IDepositUI ITransferUI IWithdrawlUIDe tal forma que nuestras operaciones quedarian comoclass Deposit  ITransaction  public DepositIDepositUI ui  ...   ...class Transfer  ITransaction  public DepositITransferUI ui  ...   ...class Withdraw  ITransaction  public DepositIWithdrawlUI ui  ...   ...De esta manera nos desacoplariamos. O al menos conseguiriamos que una clase no referenciara a una interfaz que tuviera metodos que no se usan en dicha clase. Y por lo tanto cumpliriamos con la norma de no depender de contratos que no vamos a usar.Pero ahora vamos a completar un poco este ejemplo porque en un cajero automatico tenemos muchas mas interacciones con el ser humano de las que hemos expuestoBienvenidaSeleccion de operacionSeleccion de cuentaOperacion realizada con exitoError en la operacionSeleccion de si se desea reciboSeleccion de si se desea realizar otra operacionAsi que en realidad tendriamos que anadir estas interfacesinterface IWelcomeUI    void WelcomeUser...interface IOperationSelectionUI    void SelectOperation...interface IAccountSelectionUI    void SelectAccount...interface ISuccessUI    void OperationSucceeded...interface IErrorUI    void OperationFailed...interface IReceiptUI    bool PrintReceipt...interface IFinishOperationUI    bool ShouldExit...Asi que por ejemplo nuestra clase Deposit deberia serclass Deposit  ITransaction  public DepositIDepositUI ui                 IWelcomeUI welcomeUI                 IAccountSelectionUI accountSelectionUI                 ISuccessUI successUI                 IErrorUI errorUI                 IFinishOperationUI finishUI      ...    ...De esta forma tendriamos cada interfaz independiente y jamas ocurriria que tuvieramos disponible un metodo de una dependencia que no usaramos.Lo malo de este codigo es que es muy posible que todas esas interfaces de tipo ui sean en realidad la misma instancia. Una forma de solucionar esto seria crear una implementacion para cada una de ellas teniendo en cuenta que tenemos 3 formas de comunicarnos con los usuarios. Por lo que tendriamos estos artefactosclass ScreenWelcomeUI  WelcomeUI  ... class SpeechWelcomeUI  WelcomeUI  ... class BrailleWelcomeUI  WelcomeUI  ... class ScreenOperationSelectionUI  IOperationSelectionUI  ... class SpeechOperationSelectionUI  IOperationSelectionUI  ... class BrailleOperationSelectionUI  IOperationSelectionUI  ... class ScreenAccountSelectionUI  IAccountSelectionUI  ... class SpeechAccountSelectionUI  IAccountSelectionUI  ... class BrailleAccountSelectionUI  IAccountSelectionUI  ... class ScreenSuccessUI  ISuccessUI  ... class SpeechSuccessUI  ISuccessUI  ... class BrailleSuccessUI  ISuccessUI  ... class ScreenErrorUI  IErrorUI  ... class SpeechErrorUI  IErrorUI  ... class BrailleErrorUI  IErrorUI  ... class ScreenReceiptUI  IReceiptUI  ... class SpeechReceiptUI  IReceiptUI  ... class BrailleReceiptUI  IReceiptUI  ... class ScreenFinishOperationUI  IFinishOperationUI  ... class SpeechFinishOperationUI  IFinishOperationUI  ... class BrailleFinishOperationUI  IFinishOperationUI  ... Pero este tipo de soluciones es posible que alerte a mas de uno. El caso es que existe un code smell llamado Parallel Inheritance Hierarchies. Esto ocurre cuando tenemos un arbol de dependencias entre clases con varias ramificaciones en las que las dependencias de unas con otras van en paralelo. Y en este caso seria facil terminar teniendo ese tipo de patron de herencias.Otra solucion que quiza nos haga sentir mas comodos seria crear una especie de grafo de interfaces con una interfaz base que aglutine otras interfaces como por ejemplointerface IOperationUI  IWelcomeUI                         IAccountSelectionUI                         ISuccessUI                         IErrorUI                         IFinishOperationUIAsi podriamos hacer que nuestras interfaces dependieran de esa interfaz comuninterface IDepositUI  IOperationUI    void RequestDepositAmount...interface ITransferUI  IOperationUI    void RequestTransferAmount...interface IWithdrawlUI  IOperationUI    void RequestWithdrawlAmount...interface IUserInterface  IDepositUI ITransferUI IWithdrawlUIPero imaginemos que una Transfer no espera un resultado de la operacion porque se realiza en segundo plano y se le envia un email al usuario para comunicar el resultado. Esto nos llevaria tener diferentes interfaces comunesinterface IOperationUI  IWelcomeUI                         IAccountSelectionUI                         IFinishOperationUIinterface IOperationWithResultUI  IOperationUI                                   ISuccessUI                                   IErrorUIinterface IDepositUI  IOperationWithResultUI    void RequestDepositAmount...interface ITransferUI  IOperationUI    void RequestTransferAmount...interface IWithdrawlUI  IOperationWithResultUI    void RequestWithdrawlAmount...No obstante en este ejemplo no estamos usando las interfaces en mas de un lugar. En caso de que una misma agrupacion de interfaces se usara en otra clase donde no se llamara a alguno de sus metodos tendriamos que seguir dividiendola en mas interfacesSi seguimos esta tendencia en nuestro desarrollo estariamos generando muchas herencias y un codigo un tanto caotico. Ademas esto de crear agrupaciones no iria en contra de segregar interfaces Pero claro muy dividido no estaria perdiendo cohesionNo se si mereceria la pena dejar alguna interfaz con algun metodo que pueda o no ser usado siempre y cuando mantenga coherencia. Por ejemplo en .Net encontramos el objeto System.Console. Puede que no sea la mejor implementacion del mundo y que sea una clase estatica. Puede que no cumpla con los principios SOLID en absoluto. Pero todos sabemos que ahi vamos a encontrar todo lo necesario para interactuar con el terminal write read colores limpieza... y lo tenemos todo a pesar de que una aplicacion simple de consola no lea nada del terminal. Y tampoco es que me genere demasiados problemas tenerlo.Solo opino que aunque hay implementaciones mejores y mucho mas completas para acceder al terminal System.Console te aporta un comportamiento basico muy util. Y que es suficiente facil de usar e intuitivo a pesar de haberse saltado el ISP y otros tantos principios.Hay vida despues de ISPHe de reconocer que el trabajo de Uncle Bob Robert C. Martin me ayuda a ser mejor programador. Cada vez que leo uno de sus libros o veo una de sus charlas aprendo algo. Incluso si no es la primera vez que lo hago. Y los principios SOLID y en concreto el ISP no son una excepcion. Mi consejo es queSi consideras que este principio es una mierda acostumbrate a seguirlo siempre.Si siempre lo sigues y consideras que este articulo es una mierda sigue aplicandolo.Y si lo has aplicado hasta la extenuacion y te surgen dudas sigue leyendo.Debemos ser criticos con todo lo que hacemos y encontrar esos puntos debiles en las reglas que seguimos al programarLa solucion generica es crear una interfaz por cada metodoTener muchas interfaces muy pequenas es siempre mejor que unas pocas mas grandesY si nos ponemos a dividir mucho una interfaz para desencapsular Podriamos estar ignorando activamente una de las propiedades de la orientacion a objetos como es la cohesionEl principio de segregacion de interfaz no es el mal. Es parecido al principio de responsabilidad unica. Creo que ambos buscan simplificar el codigo fuente. Aunque si los seguimos sin sentido podriamos terminar consiguiendo lo contrario de lo que buscabamos.Tal cual lo veo a ISP le pasaria lo mismo que a SRP. Considero que es necesario cumplir una serie de requisitos antes de aplicarlo sin consecuencias negativassimplicity de los valores en los que se basa XP.Reveals intention y Fewest elements de las four rules of simple design de Kent Beck.El caso es que hoy en dia me parece mas importante aplicar estos valores y reglas que el principio de segregacion de interfaz. Pero eso no quiere decir que no tenga en cuenta este ultimo.Y lo que me pueda parecer manana ya lo veremos...Ejemplo en el mundo realLamentablemente hoy no traigo una de las famosas sesiones de code review que solemos hacer. Asi que os propongo hacerla online y ahora mismo.Actualmente tenemos una serie de aplicaciones en las que usamos configuraciones especificas a partir del pais. A este fin creamos una libreria generica donde poner todo en orden y normalizar las gestiones con paises a todos estos proyectos. Esta libreria entre otras cosas expone esta interfazpublic interface ICountryStore  Task LoadAllCountriesAsync  Task LoadByCountryIsoCodeAsyncstring countryIsoCode  Task LoadByPhonePrefixAsyncstring internationalPhoneCode  Task ExistsAsyncstring countryIsoCodeEste artefacto expone la operativa esencialRecoger todas las configuraciones de paises algunos programas necesitan cargar todos los paisesal principio para determinar cuales estan disponibles para su uso.Devolver un pais por ISO Code las requests que generalmente se reciben suelen contener el codigo ISO para el pais donde deben ejecutarse.Devolver un pais por prefijo de telefono existen diferentes normas de validacion de un numero de telefono en dependencia del pais con esto podemos saber de cual se trata.Conocer si un pais existe para saber si se soporta un pais especifico.Y la forma que tenemos de exponerlo es mediante el IoC de dotnet core y su IServiceCollectionservices.AddScopedAhora bien despues de leer en que consiste ISP lo que deducimos es que deberiamos crear esta division de interfaces o al menos una parecidapublic interface IAllCountriesLoader  Task LoadAllCountriesAsyncpublic interface IByIsoCodeCountryLoader  Task LoadByCountryIsoCodeAsyncstring countryIsoCodepublic interface IByPhonePrefixCountryLoader  Task LoadByPhonePrefixAsyncstring internationalPhoneCodepublic interface ICountryExistsChecker  Task ExistsAsyncstring countryIsoCodepublic interface ICountryStore  IAllCountriesLoader                                 IByIsoCodeCountryLoader                                 IByPhonePrefixCountryLoader                                 ICountryExistsCheckerNo hay problema ya que es solo anadir mas codigo a lo que ya tengo hecho. Aunque un pequeno detalle ahora tambien deberiamos exponer estas instancias para que se puedan usar con el IServiceProvider de dotnet core. Asi que tendriamos que hacer algo asiservices.AddScopedservices.AddScopedservices.AddScopedservices.AddScopedservices.AddScopedPero haciendo esto directamente o mediante algun metodo automatizado que explote System.Reflection nos va a generar un problema y es que si necesito dos de esas interfaces al haberlas declaradas como Scoped durante el mismo scope se me generaran dos instancias del mismo objeto. Asi que lo ideal seria cambiar la declaracion a algo como estoservices.AddScopedservices.AddScopeds  s.GetServiceservices.AddScopeds  s.GetServiceservices.AddScopeds  s.GetServiceservices.AddScopeds  s.GetServiceAhora ya tendria finalizado el refactoring de este artefacto pero el resultado final es mejor resultado o en realidad anade una capa de abstraccion que no necesitamosSi por ejemplo ahora no quiero que sean instancias Scoped si no que las quiero Transient tendria que volver a modificar todas estas lineas. Mientras que con el codigo anterior era solo una.Tambien es verdad que desgranar en interfaces me da mas capacidad para ser eficiente al realizar una sola operacionclass Demo  public DemoIByPhonePrefixCountryLoader loader  ... Pero si necesito dos uso la ICountryStore o las dos interfaces que necesiteclass Demo  public DemoIByPhonePrefixCountryLoader loader ICountryExistsChecker existsChecker  ...  vs.class Demo  public DemoICountryStore store  ... Incluso si se trata de la misma instanciaEn mi opinion este codigo ya esta bien como estaba. Y no solo eso funciona correctamente. Guarda cohesion porque todos los metodos de carga y de existencia estan en un lugar. No tengo que conocer 4 interfaces ni elegir una en dependencia de la operacion que quiera realizar. Me vale con ICountryStore. Ademas si sigo la misma forma de nombrar objetos y quiero buscar por ejemplo clientes puedo imaginar que tendre una interfaz IClientStore donde existira un abanico de posibilidades de busqueda. Es un comportamiento bastante coherente de la aplicacion. Y por estas razones tiraria para atras lo que hemos hecho y lo dejaria como estaba."
    } ,
  
    {
      "title"    : "SOLID menos mola (L)",
      "category" : "",
      "tags"     : "solid, best-practices",
      "url"      : "/2020/09/30/solid-menos-mola-l",
      "date"     : "2020-09-30 02:16:54Z",
      "content"  : "La L de SOLID se refiere al principio de sustitucion de Liskov o LSP por sus siglas en ingles Liskov Substitution  Principle. Se puede definir como que cada clase que hereda de otra puede usarse como su padre sin necesidad de conocer las diferencias entre ellas. Una definicion muy compleja para un concepto mucho mas simple. Pero con una historia algo truculenta detras.Barbara Liskov es una reconocida cientifica de computacion del MIT. Como grandes logros encontramos que recibio la Medalla John von Neumann y el premio Turing el Nobel de la informatica en 2004 y 2008 respectivamente. Pero lo que hoy nos interesa es un ensayo que escribio en Octubre de 1987 titulado Data Abstraction and Hierarchy. En este documento la Doctora Liskov investiga la utilidad de dos de las caracteristicas de la programacion orientada a objetos la abstraccion y la herencia.Cuando empieza a hablar de herencia de tipos escribeLo que se desea aqui es algo como la siguiente propiedad de sustitucion Si para cada objeto o1 de tipo S hay un objeto o2 de tipo T tal que para todos los programas P definidos en terminos de T el comportamiento de P no cambia cuando o1 sustituye a o2 entonces S es un subtipo de T.Entonces Uncle Bob 8 anos despues utiliza esta frase para acunar el principio de sustitucion de Liskov comoLas funciones que usan punteros o referencias a clases base deben de poder usar objetos de clases derivadas sin tener conocimiento de ellos.Y lo complementa con un ejemplo que me he permitido la libertad de traducir a Cclass Rectangle  public virtual int Height  get set   public virtual int Width  get set class Square  Rectangle  public override int Height      get  base.Height    set          base.Height  value      base.Width  value        public override int Width      get  base.Width    set          base.Height  value      base.Width  value      Como podemos observar tenemos un objeto Rectangle con dos propiedades para almacenar sus dimensiones Width y Height. Entonces hemos creado un objeto Square de hereda de Rectangle. Y como un cuadrado es un rectangulo con la altura y la anchura iguales sobrescribimos el set de las propiedades para que tengan esto en cuenta.Ahora imaginemos que tenemos tambien un codigo asipublic static void OperateRectangleRectangle r  r.Width  4  r.Height  5  Assert.AreEqualr.Width  r.Height 20Lo que pasara es que si le pasamos un objeto Rectangle este codigo funcionara pero si le pasamos un objeto Square la linea del Assert lanzara un error.Por lo tanto estariamos violando el LSP. Porque como menciona el enunciado de este principioLas funciones que usan punteros o referencias a clases base OperateRectangle deben de poder usar objetos de clases derivadas Square hereda de Rectangle sin tener conocimiento de ellos.Quiza con la definicion no este del todo claro pero entiendo que como el codigo ha dado un error en tiempo de ejecucion no esta bien. A partir de este punto Uncle Bob empieza a divagar sobre DbC Design by Contract os acordais de Code Contracts y como esa asercion que hemos anadido al codigo nos ayuda a prevenir un mal uso de este principio.El problema aqui es que se ha malinterpretado el texto de Barbara Liskov donde si traducimos la formula comoSi para cada objeto o1 de tipo S Square hay un objeto o2 de tipo T Rectangle tal que para todos los programas P OperateRectangle definidos en terminos de T Rectangle el comportamiento de P OperateRectangle no cambia cuando o1 de tipo Square sustituye a o2 de tipo Rectangle entonces S Square es un subtipo de T Rectangle.Si nos paramos a leer con atencion lo que aqui dice la Doctora Liskov es que si usamos cualquier objeto Square para sustituir a cualquier objeto Rectangle y no tenemos que cambiar nada en OperateRectangle significa que Square es un subtipo de Rectangle. Mientras que LSP nos expone este concepto del reves cuando Square es un subtipo de Rectangle tenemos que poder intercambiarlos sin que OperateRectangle conozca sus implementaciones.En el ejemplo aplican las dos reglas por lo que podemos deducir que C funciona bien como lenguaje orientado a objetos y cumple a la perfeccion con la caracteristica de la herencia. Quiza el problema es que este tipo de herencia no sea el mas apropiado. Pero aqui lo que se expone como incorrecto es que una clase derivada tenga diferentes resultados que una clase base ante las mismas operaciones.Os imaginais un mundo donde en la programacion orientada a objetos no se pudiera modificar el comportamiento respetando el contrato de una clase en otra derivada A esto se le llama herencia y polimorfismo y son dos de las caracteristicas de la POO.Lo mejor de todo esto es que era el propio Robert C. Martin quien nos proponia el uso exhaustivo de objetos derivados para cambiar el comportamiento de un programa en la definicion del OCP.Y ademas Liskov jamas ideo un principio simplemente estaba describiendo en que consistia la herencia de tipos. De hecho recibio el premio Turing por su contribucion a los fundamentos teoricos y practicos en el diseno de lenguajes de programacion y sistemas especialmente relacionados con la abstraccion de datos tolerancia a fallos y computacion distribuida.Hay vida despues de LSPHe de reconocer que el trabajo de Uncle Bob Robert C. Martin me ayuda a ser mejor programador. Cada vez que leo uno de sus libros o veo una de sus charlas aprendo algo. Incluso si no es la primera vez que lo hago. Y los principios SOLID incluido LSP no son una excepcion. Mi consejo es queOlvidate de LSP y piensa en como definir bien la herencia de tus objetos.Si no sabes como mira la formula de la Doctora Liskov porque de eso va la herencia de tipos.Quiza te interesen temas como la composicion o la inmutabilidad.Creo que este principio fue una campana de marketing para conseguir el acronimo SOLID. Debio ser sinceramente dificil encontrar un principio con la letra L y se tuvo que recurrir a lo que se pudo.Aunque una interpretacion bastante libre de este principio creo que si que me llevo a aprender una leccion. Y es que no uses metodos en una base que luego no vayas a usar en las derivadas. Porque esto seria una mala abstraccion. Un ejemplo sobre el mismo temaclass Shape  public virtual int Height  get set   public virtual int Width  get set   public virtual int Radious  get set class Rectangle  Shape  public override int Radious      get  throw new InvalidOperationException    set  throw new InvalidOperationException  class Circle  Shape  public override int Height      get  throw new InvalidOperationException    set  throw new InvalidOperationException    public override int Width      get  throw new InvalidOperationException    set  throw new InvalidOperationException  En este caso tendriamos una mala abstraccion del objeto Shape. Ya que en lugar de tratarlo como una clase base lo estamos usando como una agregacion donde encontramos todas las propiedades de sus clases derivadas.La forma de prevenir estos usos poco canonicos por asi decirlo podria estar en otro lado. En conceptos como la inmutabilidad y la composicionInmutabilidadLa inmutabilidad es la capacidad de un objeto de permanecer en el mismo estado en el que se encuentra siempre. Es decir un objeto que no puede cambiar de estado. Es una capacidad tradicional de paradigmas de programacion funcional. Para conseguir este comportamiento tenemos varias herramientas en el lenguaje Cpublic class Rectangle  private readonly int _width  private readonly int _height  public Rectangleint width int height        _width  width      _height  height    public int Width  _width  public int Height  _heightUsando la palabra clave readonly propiedades de solo lectura y constructores podemos conseguir que una clase sea inmutable. Y si queremos trabajar con colecciones inmutables entonces podemos descargarnos el paquete de nuget System.Collections.Immutable.El caso es que este tipo de implementaciones son muy practicas porqueSon artefactos muy faciles de entender.Son threadsafe que para aplicaciones multihilo como asp.net vienen genial.Siempre estan en un estado valido.Y basandonos en esto una forma de solucionar los problemas que exponia Robert C. Martin seriapublic class Rectangle  private readonly int _width  private readonly int _height  public Rectangleint width int height        _width  width      _height  height    public RectangleRectangle r int width int height         copy other rectangle properties here      _width  width      _height  height    public int Width  _width  public int Height  _heightpublic class Square  Rectangle  public Squareint size  basesize size    public static Rectangle OperateRectangleRectangle r  return new Rectangler 4 5De esta forma siempre funcionara correctamente y no tendriamos ningun problema como los que senala Uncle Bob.Composicion vs HerenciaEs posible que no sea la primera vez que escuchas o lees sobre este concepto. Una idea que nace teniendo en cuenta las grandes virtudes de la herencia en un lenguaje orientado a objetosRapido de programarSencillo de disenarEs un nexo de union basico con otras caracteristicas como el polimorfismo o la abstraccion.Pero tambien teniendo en cuenta sus defectosUna aplicacion exhaustiva lleva a que sea muy dificil conocer que es exactamente lo que hace un objeto que herede de una larga cadena de bases.Es muy facil realizar malas herencias que nos pasaran factura con el tiempo.Es mas dificil probar el codigo.En los lenguajes modernos de programacion no existe la multiherencia.Una forma de prevenir estos problemas seria usar la composicion. Un ejemplo rapido herenciaabstract class Thing  ... class MovableThing  Thing  void Move... class SolidThing  Thing  bool Collide... class MovableSolidThing  MovableThing  bool Collide... Como podemos ver en este ejemplo sobre herencia tenemos un objeto base del que heredan dos objetos uno que se puede mover y otro que es solido que puede chocar. Cuando queremos crear un objeto con ambas propiedades encontramos un problema que tenemos que volver a implementar una de ellas. Ademas si seguimos haciendo esto nuestro codigo va a ser muy dificil de mantener porque tendremos muchas caracteristicas y finalmente sera muy complicado seguir como funciona realmente un metodo.La propuesta de la composicion seria composicioninterface IMoveable  void Move... class Moveable  IMoveable  ... class Static  IMoveable  ... interface ISolid  void Collide... class Solid  ISolid  ... class NotSolid  ISolid  ... class Thing  private readonly IMoveable _moveable  private readonly ISolid _solid  public ThingIMoveable moveable ISolid solid        _moveable  moveable      _solid  solid    public void Move...      _moveable.Move...    public bool Collide...        return _solid.Collide...  De esta forma podemos crear diferentes sabores del mismo objetovar staticNotSolidThing    new Thingnew Static   new NotSolidvar staticSolidThing       new Thingnew Static   new Solidvar moveableNotSolidThing  new Thingnew Moveable new NotSolidvar moveableSolidThing     new Thingnew Moveable new SolidTambien hay que tener en cuenta que no es oro todo lo que reluce. La composicion tiene sus problemas como por ejemploEs mucho mas complicado de disenar.Es muy facil caer en metodos vacios o implementaciones banales.No siempre encaja hay veces que es mucho mejor usar herencia.De cualquier forma es una buena practica a tener en cuenta cuando desarrollamos."
    } ,
  
    {
      "title"    : "SOLID menos mola (O)",
      "category" : "",
      "tags"     : "solid, best-practices",
      "url"      : "/2020/09/23/solid-menos-mola-o",
      "date"     : "2020-09-23 02:16:54Z",
      "content"  : "La O de SOLID se refiere al principio de abiertocerrado OCP por sus siglas en ingles OpenClosed Principle. Se puede definir como que una clase debe estar abierta a la extension y cerrada a la modificacion. Fue acunado por primera vez por Bertrand Meyer. Pero no fue hasta que Robert C. Martin lo reformulo e introdujo dentro del acronimo SOLID que se popularizo.Siguiendo la definicion de Uncle Bob el OCP se rige por dos propiedadesOpen For Extension esto significa que el comportamiento de un modulo puede ser extendido. Y expone que esto se consigue gracias a una de las propiedades de la programacion orientada a objetos el polimorfismo.Closed for Modification que dice que el codigo fuente de un modulo es inviolable. Que nadie deberia poder realizar cambios en el.El truco es mas simple de lo que parece consiste en que todo objeto tenga una clase abstracta base que podamos sobrescribir y aplicar patrones conocidos.Si tuvieramos un sistema que dibuja figurasclass Line  ... class Circle  ... class Rectangle  ... class Drawer  public static void Drawobject o      if o is Line l  Draw line stuff     if o is Circle c  Draw circle stuff     if o is Rectangle r  Draw rectangle stuff   Podriamos hacerlo extensible creando una clase base abstracta con el contrato de dibujado y una implementacion para cada una de las figurasabstract class Shape  public abstract void Drawclass Line  Shape  public override void Draw  Draw line stuff class Circle  Shape  public override void Draw  Draw circle stuff class Rectangle  Shape  public override void Draw  Draw rectangle stuff class Drawer  public static void Drawobject o      if o is Shape s s.Draw  Pero en un futuro podriamos querer anadir por ejemplo un recuadro rojo cuando algo este seleccionado. Para esto podriamos poner todos los metodos como extensibles usando la palabra clave virtual y crear nuevas clases tipoclass Line  Shape  public override void Draw  Draw line stuff class Circle  Shape  public override void Draw  Draw circle stuff class Rectangle  Shape  public override void Draw  Draw rectangle stuff class RedBorderedLine  Line  public override void Draw      Draw red border    base.Draw  class RedBorderedCircle  Circle  public override void Draw      Draw red border    base.Draw  class RedBorderedRectangle  Rectangle  public override void Draw      Draw red border    base.Draw  O podriamos optar por el patron Decorator y asi prevendriamos futuros cambios compuestos como por ejemplo dibujar un borde rojo fondo azul o un emote junto con mi figura.Asi que creariamos nuestros decoradoresabstract class Decorator  public abstract void DecorateShape shapeclass RedBorderedDecorator  Decorator  public override void DecorateShape shape  Draw red border class LineDecorator  Decorator  public override void DecorateShape shape  Draw line stuff class CircleDecorator  Decorator  public override void DecorateShape shape  Draw circle stuff class RectangleDecorator  Decorator  public override void DecorateShape shape  Draw rectangle stuff Y modificariamos nuestro original para que utilizara los decoradores que hemos creadoabstract class Shape  private readonly Decorator _decorator  public ShapeDecorator decorator        _decorator  decorator    public bool IsSelected  get set   public virtual void Draw        if IsSelected                new RedBorderedDecorator.Decoratethis            _decorator.Decoratethis  class Line  Shape  public Line  basenew LineDecorator  class Circle  Shape  public Circle  basenew CircleDecorator  class Rectangle  Shape  public Rectangle  basenew RectangleDecorator  Pero aqui hemos hecho trampa. Si os fijais hemos anadido una propiedad a Shape llamada IsSelected y quiza en un futuro esa propiedad no deba de estar en nuestro codigo asi que lo mejor seria refactorizarlo y por ejemplo usar un patron Visitor para cambiar el estado de nuestra figuraabstract class Visitor  public abstract void VisitShape shapeclass SelectVisitor  Visitor  public override void VisitShape shape        shape.AddDecoratornew RedBorderedDecorator  class UnselectVisitor  Visitor  public override void VisitShape shape        shape.RemoveDecoratortypeofRedBorderedDecorator  abstract class Shape  private readonly List _decorators  public ShapeDecorator decorator        _decorators  new List  decorator     public void AddDecoratorDecorator d  ...   public void RemoveDecoratorType t  ...   public virtual void Draw        _decorators.ForEachd  d.Decoratethis  Para hacer que estos objetos visiten nuestras figuras necesitaremos otro tipo de elemento como por ejemplo un Command. Aplicando este patron creariamos acciones que se pueden ejecutar en nuestro sistemaabstract class Command  public abstract void ExecuteManager manager Shape shapeclass SelectCommand  Command  private readonly Visitor _selectVisitor  new SelectVisitor  private readonly Visitor _unselectVisitor  new UnselectVisitor  public override void ExecuteManager manager Shape shape      manager.Shapes.ToList.ForEachx  _unselectVisitor.Visitx    _selectVisitor.Visitshape  Y finalmente para orquestar esos Commands crearemos otra clase que nos servira de ayudaclass Manager  private readonly List _shapes  new List  private readonly IDictionary _commands  public ManagerIDictionary commands      _commands  commands    public IEnumerable Commands  get  _commands.Keys   public IEnumerable Shapes  get  _shapes   public void AddShapeShape s        _shapes.Adds    public void RemoveShapeShape s        _shapes.Removes    public void DoCommandstring commandKey Shape s        if _commands.ContainsKeycommandKey                _commandscommandKey.Executethis s        De esta manera nuestro programa estara abierto a la extensionSi quisieramos anadir o modificar una figura anadiriamos un nuevo objeto que heredara de Shape y otro que lo hiciera de Decorator para dibujarla en pantalla.Si quisieramos anadir o modificar una caracteristica de dibujado creariamos un nuevo objeto de tipo Decorator.Si quisieramos anadir o modificar una caracteristica de dibujado condicional ademas de lo que hicimos en el paso anterior creariamos una serie de objetos de tipo Visitor para cambiar el listado de decoradores de nuestra figura.Si quisieramos anadir acciones a realizar en una lista de figuras creariamos un nuevo objeto tipo Command.Si quisieramos borrar alguno de los objetos que ya existen solo tendriamos que ignorarlos.Por lo que tambien estara cerrado a la modificacion porque todo cambio consistira en crear nuevos objetos yo ignorar los que ya existen.Si bien es verdad que se ha incrementado un poco la complejidad del sistema como estamos usando patrones conocidos a nuestro equipo de programadores no les importara demasiado. Seria pagar un poco de complejidad a cambio de que todo lo que esta hecho se tenga la seguridad de que no se va a tener que tocar jamas Quien no firmaria esto ahora mismo para sus desarrollosAhora surge un nuevo problema empieza un sprint y tenemos una nueva featureAnadir colores al pintado de la figuraSiguiendo el OCP para poder pintar en colores no podemos modificar el codigo existente luego tendriamos que crear un nuevo conjunto de decoradoresabstract class ColoredDecorator  Decorator  protected ColoredDecoratorColor color        Color  color    protected Color Color  get private set class ColoredBorderedDecorator  ColoredDecorator  public ColoredBorderedDecoratorColor color basecolor    public override void DecorateShape shape  Draw red border class ColoredLineDecorator  ColoredDecorator  public ColoredLineDecoratorColor color basecolor    public override void DecorateShape shape  Draw line stuff class ColoredCircleDecorator  ColoredDecorator  public ColoredCircleDecoratorColor color basecolor    public override void DecorateShape shape  Draw circle stuff class ColoredRectangleDecorator  ColoredDecorator  public ColoredRectangleDecoratorColor color basecolor    public override void DecorateShape shape  Draw rectangle stuff Tambien tendriamos que crear nuevas figuras para que usaran estos nuevos decoradores. Y lo mismo para los Visitors y los Commands. Tendriamos que crear alguna forma de que los Commands recibieran el parametro de color de forma dinamica asi que al final terminariamos reescribiendo la implementacion completa del Manager llamandolo ColoredManager o recubriendola en un wrapper.El caso es que en un desarrollo normal terminaremos o bien haciendo Copy  Paste de todo nuestro codigo cientos de veces para crear un objeto nuevo con una nueva capacidad o bien extendiendo de unas bases que en un principio no contenian toda la complejidad del sistema.Os imaginais un ano un equipo de 4 personas desarrollando este proyecto para ir dotandolo de mas caracteristicas Imagino una cantidad de objetos inabarcable. Algo que a una persona nueva en el proyecto le seria practicamente incomprensible por lo distribuida que estaria la informacion.Y es que hemos caido en nuestra propia trampa no podemos modificar solo extender. Esto implica que tenemos que programar haciendo que nuestro codigo sea extensible en todos aquellos puntos en los que podria tener que ser modificado en el futuro. Pero hasta donde se nunca he conocido un programador adivino que vea como va a evolucionar un producto en el futuro. Al menos no en su totalidad.Por esta razon Uncle Bob decidio reformular en 2014 su propuesta de OCP. En este nuevo documento nos habla del uso de sistemas de plugins como los que usan nuestros IDEs preferidos para poner en practica este principio. Y aunque creo que es muy interesante esta propuesta tambien creo que son pocas las aplicaciones que he programado en las que anadir un sistema de complementos vaya a impactar directamente en la mejora de la calidad de su codigo fuente.El ejemplo que hemos expuesto es un buen codigo. Sigue a rajatabla OCP. Pero llega un momento en el que en dependencia de la nueva feature que tengamos que implementar creo que mejoraria mas modificando ciertos objetos que anadiendo extensiones derivaciones o plugins.Porque el desarrollo de software la gestion del ciclo de vida de una aplicacion las metodologias agiles las practicas XP eXtreme Programming y todo lo que lo rodea va en direccion de adaptarse al cambio no a preverlo. Y cuando un requisito del sistema cambia significa que tu codigo no es valido y tienes que reemplazarlo.Hay vida despues de OCPHe de reconocer que el trabajo de Uncle Bob Robert C. Martin me ayuda a ser mejor programador. Cada vez que leo uno de sus libros o veo una de sus charlas aprendo algo. Incluso si no es la primera vez que lo hago. Y los principios SOLID y en concreto el OCP no son una excepcion. Mi consejo es queSi consideras que este principio es una mierda acostumbrate a seguirlo siempre.Si siempre lo sigues y consideras que este articulo es una mierda sigue aplicandolo.Y si lo has aplicado hasta la extenuacion y te surgen dudas sigue leyendo.Debemos ser criticos con todo lo que hacemos y encontrar esos puntos debiles en las reglas que seguimos al programarSi tengo que estar cerrado a la modificacion Como puedo predecir que codigo va a cambiar para hacerlo extensibleSi intento prever todos los cambios que se pueden dar en un sistema Que pasa con eso de adaptarse al cambio que promueve el desarrollo agileEs este principio una oposicion directa a YAGNIEs mas importante estar abierto a la extension y cerrado a la modificacion que el principio DRYEs mejor tener un sistema abierto a la extension o una aplicacion que se cina a hacer lo que dice que debe hacerSi quiero aplicar OCP en un codigo que nunca lo ha tenido en cuenta podria entonces modificar las clases existentesEl principio de abiertocerrado creo que busca reducir el uso de bloques condicionales como switch e if aplicando patrones y usando las capacidades de un lenguaje orientado a objetos. Esto es importante porque los bloques condicionales hacen que nuestro codigo sea mas dificil de entender y por lo tanto de mantener. Aunque seguirlo sin sentido nos llevara a conseguir todo lo contrario un codigo en el que todo esta tan desperdigado y hay tantas herencias que resulta muy dificil de comprender.Afortunadamente no todo en este mundo es SOLID. Existen multitud de principios y reglas de programacion que son muy validas tambien. Antes que seguir el OCP creo que seria interesante pensar en uno de los valores en los que se basa XP eXtreme Programmingsimplicity Es mejor hacer una cosa simple hoy y pagar un poco mas manana para cambiarlo que hacer una cosa mas complicada hoy que jamas vaya a ser utilizada.Y despues tendria en cuenta seguir el abiertocerrado procurando no contradecir dos de las four rules of simple design de Kent Beck No duplication y Fewest elements. Las cuatro me parecen muy importantes pero una aplicacion agresiva de OCP podria llevarnos a duplicar mucho codigo con la excusa de no poder modificarlo y encontrarnos con muchos artefactos con una alta probabilidad de tener el smell de Parallel Inheritance Hierarchies.Segun la opinion que tengo hoy en dia y usando lenguaje de acronimos OCP deberia estar supeditado por otros principios como KISS DRY y YAGNI. Pero eso no quiere decir que sea equivocado o que no lo apliquemos.Y lo que me pueda opinar manana ya lo veremos...Ejemplo en el mundo realEn otro de los famosos code reviews en los que tuve la suerte de poder asistir salio un codigo que nos viene al pelopublic static dynamic FromObjectdynamic obj  if obj is ExpandoObject return obj  var expando  new ExpandoObject  if obj is Document d      obj  JsonConvert.DeserializeObjectd.ToString    var dictionary  IDictionaryexpando  if obj is JObject      JObjectobj.Children.ToList.ForEachx           dictionary.Addx.Name x.Value        return expando    obj.GetType as Type.GetProperties.ToList.ForEachp       dictionaryp.Name  p.GetValueobj    if dictionary.Keys.Allx  x  Payload  obj.GetType as Type.FullName  Microsoft.Azure.Documents.QueryResult      return FromObjectdictionaryPayload    return expandoEs un ejemplo real de algo que conocemos vulgarmente como spaghetticode. Y lo mejor es que el autor de semejante atentado contra la humanidad fui yo mismo. Asi que primero me sirvio para bajarme los humos y segundo para ser critico con mi propio trabajo. Por muy bueno que se considere un programador siempre cometera errores. Y la mejor forma de aprender y mejorar es que tu codigo aparezca en un code review.Como este codigo era un conjunto de bugfixes hechos con el menor rigor profesional lo primero que tuvimos que hacer fue explicar que hacia y por que. Asi que os lo voy a dejar a continuacion en formato de comentario convierte un objeto cualquiera en un ExpandoObject que es un tipo dinamico que nos permite anadir nuevas propiedades al vuelo en este caso se usa para crear un objeto al que le vamos anadiendo ciertas propiedades en tiempo de ejecucion y que sera almacenado en una base de datos de tipo documental sin esquemapublic static dynamic FromObjectdynamic obj   si el objeto de entrada ya es un ExpandoObject lo devolvemos tal cual  if obj is ExpandoObject return obj   nuestra funcion devolvera finalmente este expando  var expando  new ExpandoObject   si el objeto de entrada es de tipo Document de cosmosDB lo deserializamos para convertirlo en un JObject  if obj is Document d      obj  JsonConvert.DeserializeObjectd.ToString     para poder interactuar de una forma sencilla con un ExpandoObject lo podemos convertir en un diccionario  var dictionary  IDictionaryexpando   si el objeto de entrada es un JObject  if obj is JObject       copiamos las propiedades de este objeto    JObjectobj.Children.ToList.ForEachx           dictionary.Addx.Name x.Value         devolvemos el ExpandoObject    return expando     si no copiamos las propiedades de un objeto cualquiera  obj.GetType as Type.GetProperties.ToList.ForEachp       dictionaryp.Name  p.GetValueobj     si despues de copiar los datos observamos que solo existe la propiedad Payload y que el objeto de entrada es de tipo QueryResult   QueryResult es un objeto interno al que no se tiene acceso por eso comparamos con el nombre del tipo  if dictionary.Keys.Allx  x  Payload  obj.GetType as Type.FullName  Microsoft.Azure.Documents.QueryResult       lanzamos otra vez la funcion de convertir pero usando solo la propiedad Payload    return FromObjectdictionaryPayload     devolvemos el ExpandoObject  return expandoComo se puede observar por los comentarios estamos creando un objeto ExpandoObject a partir de otro objeto haciendo diferenciacion de si este objeto es de tipo Document JObject QueryResult o cualquier otro tipo.Los primeros comentarios que surgieron en el code review proponian montar un Command Pattern o algo parecido para gestionar esto. Pero nadie sabia como empezar a hacerlo. Asi que decidimos empezar por los pequenos pasos seguros que teniamos y a partir de ahi ver como evolucionaba el codigo.Evita el uso de dynamic en contratosCreo que todos tendremos grabado a fuego que no es recomendable usar el tipo dynamic en C. Y menos como parametros de entrada y salida de un metodo o funcionpublic static dynamic FromObjectdynamic objRealmente estabamos construyendo un objeto de tipo ExpandoObject a partir de cualquier otro object. Por lo que la firma sin usar dynamic nos simplificaria lo que vendria despuespublic static ExpandoObject FromObjectobject objSimplifica los condicionalesAl final de codigo teniamos esta comprobacionif dictionary.Keys.Allx  x  Payload  obj.GetType as Type.FullName  Microsoft.Azure.Documents.QueryResult  return FromObjectdictionaryPayloadAqui estamos hackeando el sistema para realizar una conversion de un tipo internal del SDK 2.0 de cosmosDB pero tambien estamos comprobando que tenga una propiedad llamada Payload. Quiza nos podriamos ahorrar esta comprobacion si ya sabemos cual es el tipoif obj.GetType.FullName  Microsoft.Azure.Documents.QueryResult  var payload  obj.GetType.GetPropertyPayload  return FromObjectpayloadEste cambio tiene un efecto secundario ya no hace falta que sea la ultima condicion porque nos hemos quitado la dependencia de diccionary. Asi que podria situarse en las comprobaciones iniciales.Empieza por extraer metodosCuando ya te quedas sin ideas lo que puedes hacer con un codigo espagueti es empezar a extraer metodos en busca de algo que te llame la atencion.En este caso nos fijamos en la asignacion de propiedades que se podria hacer de dos formas diferentes una en si es un JObject y otra si noprivate static ExpandoObject PopulatePropertiesobject obj  var expando  new ExpandoObject  var dictionary  IDictionaryexpando  PopulateJsonPropertiesdictionary obj  PopulateDefaultPropertiesdictionary obj  return expandoprivate static void PopulateJsonPropertiesIDictionary dictionary object obj  if obj is JObject o return  o.Children.ToList.ForEachx             dictionary.Addx.Name x.Value    private static void PopulateDefaultPropertiesIDictionary dictionary object obj  if obj is JObject return  obj.GetType.GetProperties.ToList.ForEachp         dictionaryp.Name  p.GetValueobj  Encontrando el patronEntonces empezamos a extraer a metodos los bloques if que teniamosprivate static ExpandoObject FromExpandoObjectobject obj  if obj is ExpandoObject e return e  return defaultPrimero para ver si era un ExpandoObject y luego para cuando era un QueryResultprivate const string QueryResultTypeName  Microsoft.Azure.Documents.QueryResultprivate const string PayloadPropertyName  Payloadprivate static ExpandoObject FromQueryResultobject obj  if obj.GetType.FullName  QueryResultTypeName      var payload  obj.GetType.GetPropertyPayloadPropertyName    return FromObjectpayload    return defaultY ya teniamos un patron a la vista crear funciones con la firma que acepten un parametro de entrada de tipo object y devuelvan ExpandoObjectFuncAsi que terminamos con el ultimo bloque if que nos quedaba para cuando se trataba de un objeto de tipo Documentprivate static ExpandoObject FromDocumentobject obj  if obj is Document d      var json  JsonConvert.DeserializeObjectd.ToString    return FromObjectjson    return defaultY despues refactorizamos los metodos que creamos en el paso anterior para asignacion de propiedades con la misma forma que habiamos estandarizadoprivate static ExpandoObject FromJObjectobject obj  if obj is JObject o        var expando  new ExpandoObject      var dictionary  IDictionaryexpando      o.Children.ToList.ForEachx               dictionary.Addx.Name x.Value            return expando    return defaultprivate static ExpandoObject FromAnyObjectobject obj  var expando  new ExpandoObject  var dictionary  IDictionaryexpando  obj.GetType.GetProperties.ToList.ForEachp       dictionaryp.Name  p.GetValueobj    return expandoFinalmente nuestra funcion quedaria como una llamada encadenada a cada una de las funciones que intentaban realizar la conversion a ExpandoObjectpublic static ExpandoObject FromObjectobject obj  var result  FromExpandoObjectobj  result  FromQueryResultobj  result  FromDocumentobj  result  FromJObjectobj  result  FromAnyObjectobj  return resultResultado finalAqui surgio de nuevo si merecia la pena implementar un Strategy Pattern para separar todas estas funciones en clases completas abierto a la extension pero cerrado a la modificacion. Pero teniendo en cuenta que era el codigo de una clase de apoyo un Helper que no se espera que cambie nunca el equipo decidio que una solucion intermedia era suficiente creando una lista de funciones y realizando un recorrido por todas ellasprivate static readonly Func _strategies  new Func   FromExpandoObject  FromQueryResult  FromDocument  FromJObject  FromAnyObjectpublic static ExpandoObject FromObjectobject obj  return _strategies.Selectx  xobj                    .FirstOrDefaultx  x  nullEs posible que el codigo hubiera mejorado con el patron de estrategia o con cualquier otro patron de los que hemos visto en el primer ejemplo. Pero el equipo considero que ya habia realizado una buena refactorizacion y que dejarian esa decision para el futuro. En ese momento ya habiamos conseguido el objetivo de la reunion refactorizar el horrible codigo inicial y hacerlo legible."
    } ,
  
    {
      "title"    : "SOLID menos mola (S)",
      "category" : "",
      "tags"     : "solid, best-practices",
      "url"      : "/2020/09/16/solid-menos-mola-s",
      "date"     : "2020-09-16 02:16:54Z",
      "content"  : "La S de SOLID se refiere al principio de responsabilidad unica o SRP por sus siglas en ingles Single Responsibility Principle. Se puede definir como que una clase debe tener una sola responsabilidad o como diria Robert C. Martin una clase debe tener solo una razon para cambiar.Es un concepto muy simple. Algo que todo el mundo entiende. Y por lo tanto un concepto muy fuerte. Una clase una responsabilidad. Y que demonios un metodo una sola responsabilidad tambien. Pero siempre dentro de una clase con una responsabilidad.Si definimos una responsabilidad como un eje de un cambio podemos determinar que cuantas mas responsabilidades asuma una clase esta sera mas susceptible al cambio. Y cuando las clases de nuestra aplicacion son muy susceptibles al cambio decimos que nuestro codigo esta acoplado.Asi que desacoplar el codigo implica separar las responsabilidades y crear un objeto para cada una de ellas. De esta forma nuestro codigo sera mas legible y en consecuencia mas sencillo de mantener.Pero vamos a ver todo esto en codigopublic class Order    public Guid Id  get     public Customer Customer  get     public IEnumerable Lines  get     public decimal TotalAmount  get     public decimal Taxes  get     public void AddLineProduct product int quantity  ...     public void RemoveLineOrderLine line  ...     public void CalculateAmountAndTaxes  ...     public bool Validate  ...     public void Load  ...     public void Save  ...     public Invoice CreateInvoice  ... Si pensaramos en responsabilidades podriamos decir que la clase Order tiene las siguientesAlmacenar los datos de un pedido en las propiedades Customer Lines TotalAmount y Taxes.Anadir lineas del pedido con el metodo AddLine.Borrar lineas del pedido con el metodo RemoveLine.Calculo del importe total del pedido con el metodo CalculateAmountAndTaxes.Calculo de los impuestos con el mismo metodo que el anterior CalculateAmountAndTaxes.Validacion de un pedido con el metodo Validate.Carga de un pedido desde la base de datos con el metodo Load.Guardado de un pedido en la base de datos con el metodo Save.Creacion de una factura para el pedido con el metodo CreateInvoice.Aplicando el principio de responsabilidad unica no seria una locura decir que cada una de estas responsabilidades las podriamos separar en un objecto diferente. Porque son responsabilidades unicas todas ellas. Asi que dividiriamos nuestro codigo enOrder clase de tipo POCO que almacena los datos de un pedido.OrderAddLineService clase de tipo servicio que anade una linea a un pedido.OrderRemoveLineService clase de tipo servicio que borra una linea a un pedido.CalculateAmountService clase de tipo servicio para el calculo del importe total del pedido.CalculateTaxesService clase de tipo servicio para el calculo de los impuestos.OrderValidator clase de tipo validador de un pedido.OrderStoreReader clase para cargar de un pedido de la base de datos.OrderStoreWriter clase para el guardado de un pedido en la base de datos.InvoiceFactory clase de tipo factory para la creacion de una factura.Cuando empecemos a usar este conjunto de artefactos es posible que anadamos una clase de tipo facade para poder orquestarlo todo OrderFacade. Aunque seguro que mas de uno esta pensando que se nos ha ido de las manos...Es muy dificil determinar que es una responsabilidad. Uncle Bob la define como una razon para cambiar. Se me ocurren dos razones para cambiar codigo en un proyecto bug fixing o new feature. Y podriamos tener que cambiar uno o varios artefactos por cualquiera de ellas.Es por eso que en 2014 escribio un nuevo articulo sobre el tema. Aqui menciona que Este principio es sobre las personas. Donde habla de que hay que preguntarse por quien es el responsable de ese codigo en cuestion. Si es un tema del director de finanzas del director de tecnologia del de operaciones... Y en mi caso personal puedo afirmar sin miedo a equivocarme que el responsable siempre es el desarrollador. Y a poder ser el que ya no esta en la empresa.Comentarios jocosos aparte otra leccion que podemos extraer de este ultimo articulo es que las responsabilidades en realidad no estan pensadas para tener tanta granularidad como representamos anteriormente. Todo este codigo quedaria mucho mejor agrupando ciertas caracteristicas por responsabilidades a mas alto nivelOrder clase que almacena los datos de un pedido. Tendra ademas los metodos que la modifican AddLine y RemoveLine.OrderService aqui montaremos una clase de tipo servicio que nos ayudara a operar con nuestro pedido. Tendra los metodos para calcular el importe los impuestos y para generar las facturas.OrderValidator esta clase la dejaremos como esta ya que valida nuestro objeto Order.OrderStore una clase que nos permitira guardar y cargar objectos Order de la base de datos.Podriamos describirlo en codigo como algo asipublic class Order    public Guid Id  get     public Customer Customer  get     public IEnumerable Lines  get     public void AddLineProduct product int quantity  ...     public void RemoveLineOrderLine line  ... public class OrderService    public decimal CalculateAmountOrder order  ...     public decimal CalculateTaxesOrder order  ...     public Invoice CreateInvoiceOrder order  ... public class OrderValidator    public ValidationResult ValidateOrder order  ... public class OrderStore    public void LoadGuid id  ...     public void SaveOrder order  ... Supongo que esta ultima implementacion sera con la que la mayoria estara mas de acuerdo. Es una forma de crear artefactos muy comun que divide muy bien los conceptos que genera poco acoplamiento y que hace nuestro codigo mas mantenible. No hay duda de que seguir el Single Responsibility Principle nos ha ayudado a crear un mejor codigo.Pero tal vez tengais en mente una separacion de responsabilidades mucho mejor de las que hemos propuesto aqui...Es muy dificil determinar que es una responsabilidad. Y creo que aun mas despues del tema este de lo de las personas. Son definiciones muy vagas que llevan a confusion que generan diferentes puntos de vista y diferentes verdades encontradas. Y es muy dificil llegar a un acuerdo cuando sobre un mismo tema hay dos interpretaciones que son validas al mismo tiempo.A lo que hace el primer enfoque algunos lo llamaran sobrearquitectura o sobreingenieria. A lo que hace el segundo otros lo senalaran como codigo acoplado que no se rige por el SRP. Y habra quien piense que ambos ejemplos son basura y que la implementacion deberia ser totalmente diferente. Lo mejor es que todos tienen razon.Hay vida despues de SRPHe de reconocer que el trabajo de Uncle Bob Robert C. Martin me ayuda a ser mejor programador. Cada vez que leo uno de sus libros o veo una de sus charlas aprendo algo. Incluso si no es la primera vez que lo hago. Y los principios SOLID y en concreto el SRP no son una excepcion. Mi consejo es queSi consideras que este principio es una mierda acostumbrate a seguirlo siempre.Si siempre lo sigues y consideras que este articulo es una mierda sigue aplicandolo.Y si lo has aplicado hasta la extenuacion y te surgen dudas sigue leyendo.Debemos ser criticos con todo lo que hacemos y encontrar esos puntos debiles en las reglas que seguimos al programarQue es exactamente una responsabilidad unica Resulta un concepto demasiado abierto a todo tipo de interpretaciones.Si una responsabilidad es una razon para cambiar Como puedo predecir que codigo va a cambiarSi una responsabilidad es sobre las personas Como puedo determinar quien es el responsable de esa porcion de codigo si me informo por un backlogTener muchas clases muy pequenas es siempre mejor que una sola mas grandeY si nos ponemos a dividir mucho una clase para desencapsular Podriamos estar ignorando activamente una de las propiedades de la orientacion a objetos como es la cohesionEl principio de responsabilidad unica creo que es una forma de intentar simplificar el codigo. Aunque seguirlo sin sentido puede llevarnos a conseguir todo lo contrario.Afortunadamente no todo en este mundo es SOLID. Existen multitud de principios y reglas de programacion que son muy validas tambien. Antes que seguir el SRP creo que seria interesante pensar en uno de los valores en los que se basa XP eXtreme Programmingsimplicity Es mejor hacer una cosa simple hoy y pagar un poco mas manana para cambiarlo que hacer una cosa mas complicada hoy que jamas vaya a ser utilizada.Y despues tendria en cuenta seguir el principio de responsabilidad unica procurando no contradecir dos de las four rules of simple design de Kent Beck Reveals intention y Fewest elements. Las cuatro me parecen muy importantes pero una aplicacion agresiva de SRP podria llevarnos a ocultar la intencion y comportamiento de nuestro codigo por encontrarse demasiado dividida en muchos artefactos.El caso es que hoy en dia me parece mas importante aplicar estos valores y reglas que el principio de responsabilidad unica. Pero eso no quiere decir que no tenga en cuenta este ultimo.Y lo que me pueda parecer manana ya lo veremos...Ejemplo en el mundo realHace poco hicimos un code review en un proyecto en el que usamos Vertical Slice como arquitectura. Cuando trabajamos con este tipo de arquitectura y creamos una API cada feature al menos debe implementar cuatro artefactosRequest una peticion a nuestro sistema.Handler el artefacto que gestiona una peticion y responde correctamente.Response la respuesta a la peticion.Controller para gestionar el flujo de navegacion. Desde que llega una peticion hasta que se envia la respuesta.Pero tener estos objetos es solo la base a partir de aqui vamos anadiendo todo lo que necesitemos. En este contexto se anadioValidator para validar la RequestIRepository y Repository para recoger datos de la base de datos.Mapper para transformar los datos de base de datos en la Response del Handler.Asi que la Feature que analizamos tenia esta estructuraGetProductByIdController.csHandler.csIRepository.csMap.csRepository.csRequest.csResponse.csValidator.csEl artefacto donde transcurre la accion de nuestra feature es el Handler y este fue el codigo que empezamos a analizarpublic class Handler IRequestHandler    private readonly IRepository _repository    public HandlerIRepository repository            _repository  repository        public async Task HandleRequest request CancellationToken cancellationToken            var product  await GetProductrequest        var dto  Map.MapToDtoproduct        return GenerateResponsedto        private CommandResult GenerateResponseProductDto product         CommandResult.Successnew Responseproduct    public Task GetProductRequest request         _repository.GetProductrequest.ProductIdEste codigo desde un punto de vista de SRP es muy bueno. Pero quiza hemos complicado demasiado ciertas partes que hacen que tengamos que recorrer mas camino del necesario para terminar haciendo lo mismoSi una funcion es una linea de codigo tal vez no haga falta una funcionEstamos hablando de este metodopublic Task GetProductRequest request       _repository.GetProductrequest.ProductIdNos referimos a ese tipo de funciones que llaman a otro metodo con el mismo nombre dentro de otro artefacto y que su gran valor es envolver una llamada algo mas larga. Esto nos llamo la atencion y pensamos que no era muy diferente ponervar product  await GetProductrequestQuevar product  await _repository.GetProductrequest.ProductIdAsi que nos deshicimos de ese metodo.Si una linea de codigo no se entiende de un primer vistazo tal vez haga falta simplificarlaPor la misma razon intentamos cambiar este otro metodoprivate CommandResult GenerateResponseProductDto product      CommandResult.Successnew ResponseproductPero esta vez sustituir esta linea de codigoreturn GenerateResponsedtoPor esta otrareturn CommandResult.Successnew ResponseproductNo nos dejaba tan claro que es lo que hacia.Analizando mas profundamente nos dimos cuenta de que aqui entraban en juego 3 artefactos y que para darle claridad antes de realizar el cambio tendriamos que refactorizar esta parte.Si necesitamos 3 artefactos para montar una respuesta tal vez podamos simplificar a unoLos artefactos a los que nos referimos sonCommandResultT una clase generica que usamos para recubrir respuestas estandares del sistema. Basicamente tiene un listado de errores y un payload.Response es el objeto de respuesta de la feature. Cada una tiene el suyo. Y aqui contiene un DTO Data Transfer Object de tipo ProductDto.ProductDto es el DTO que se encapsula dentro del objeto Response.Lo primero que nos llama la atencion es que en este caso no estamos gestionando errores. Solo hay dos posibilidades devolvemos un CommandResultResponse exitoso o lanzamos una excepcion que no hemos gestionado. Por lo que en realidad no estamos usando las caracteristicas de este objeto. Asi que lo desechamos.Por otro lado que un objeto Response contenga otro de tipo ProductDto tampoco nos aportaba valor. Asi que decidimos crear un objeto Response con las propiedades que necesitabamos del ProductDto. De esta forma nos quedamos con un solo objeto y pudimos cambiar nuestro codigovar product  await _repository.GetProductrequest.ProductIdvar response  Map.MapToResponseproductreturn responseAsi solucionabamos dos problemas de un tiro.Si necesitamos una clase con un solo metodo que se usa en un solo lugar tal vez esa clase no haga faltaResulta que los objetos IRepository y Repository solo se usaban dentro de esta feature. Ademas solo tenian un metodo GetProduct. Al analizarlo por dentro todo parecia ser una llamada simple a un metodo de busqueda de un artefacto de Mongo.Driver IMongoCollectionProduct.internal class Repository  IRepository  public RepositoryIMongoCollection mongoCollection      _mongoCollection  mongoCollection    public Task GetProductstring productId      return _mongoCollection.Findx  x.ProductId  request.ProductId                           .FirstOrDefaultAsync  Como el codigo parecia muy simple pensamos en cogerlo y aplicarlo directamente al Handler pero esto complicaria mucho el codigopublic class Handler  IRequestHandler  private readonly IMongoCollection _mongoCollection  public HandlerIMongoCollection mongoCollection      _mongoCollection  mongoCollection    public Task HandleRequest request CancellationToken cancellationToken      var product  _mongoCollection.Findx  x.ProductId  request.ProductId                                  .FirstOrDefaultAsynccancellationToken    var response  Map.MapToResponseproduct    return response  La verdad es que visto con perspectiva es bastante parecido a lo que teniamos al inicio pero nos hemos quitado de en medio dos dependencias la de la abstraccion IRepsository y la concrecion Repository.Si estamos usando un Mapper para convertir un objeto de base de datos a otra cosa tal vez podriamos usar una proyeccionComo ahora mismo no teniamos un repositorio que nos ocultara el comportamiento con respecto la base de datos nos encontramos con que el metodo Find de una IMongoCollectionT acepta proyecciones. Asi que quiza podriamos quitarnos tambien el objeto Mapper_mongoCollection.Findx  x.ProductId  request.ProductId                .Projectnew ClientSideDeserializationProjectionDefinition                .FirstOrDefaultAsynccancellationTokenResultado finalEl resultado final de esta sesion fue la simplificacion del codigo tanto en numero de artefactosGetProductByIdController.csHandler.csRequest.csResponse.csValidator.csComo en numero de lineas en el Handler. Ademas de en nuestra opinion ser un codigo mas legiblepublic class Handler  IRequestHandler  private readonly IMongoCollection _mongoCollection  public HandlerIMongoCollection mongoCollection      _mongoCollection  mongoCollection    public Task HandleRequest request CancellationToken cancellationToken      return _mongoCollection.Findx  x.ProductId  request.ProductId                           .Projectnew ClientSideDeserializationProjectionDefinition                           .FirstOrDefaultAsynccancellationToken  Todo este code review te hace pensar. Si todo lo que estaba hecho al principio era correcto... significa que este codigo esta mas acopladoSi pensamos en cuanto me implica realizar un cambio como por ejemplo anadir una propiedad nueva en la base de datos antiguamente hubiera implicado cambiar Product ProductDto y Mapper. Con el codigo que hemos desarrollado solo tendriamos que cambiar Product y Response Es esto mejor Pero si tenemos un cambio mas grande en el futuro quiza tenga que pasar por todas mis features y cambiarlas una a una. Entonces esto podria ser un mal cambio.Seria mejor tener mas clases y metodos pequenos como hemos tenido al principio O es mejor tener un codigo como el resultado finalEl equipo finalmente prefirio este resultado. La razon principal era porque resultaba mas facil de leer. Tambien anadia menos pasos para encontrar una linea de codigo concreta. Cumplia con que una feature encapsule una sola accion completa con el minimo codigo imprescindible. En definitiva se sentian mas comodos con esta implementacion."
    } ,
  
    {
      "title"    : "The Hitchhiker's Guide to Azure WebApps",
      "category" : "",
      "tags"     : "azure, webapp, appservice",
      "url"      : "/video/2020/09/10/advanced-azure-webapps",
      "date"     : "2020-09-10 00:00:00Z",
      "content"  : "La grabacion del evento de Twitch de Catzure junto con Robert Bermejo. En esta sesion os ensenamos algunos de esos truquillos que solemos utilizar para mantener nuestras aplicaciones con vida a pesar de que el mundo las quiera matar.DescripcionSi no nos anticipamos a los imprevistos si no esperamos lo inesperado en una nube con infinitas posibilidades podriamos hallarnos a merced de cualquiera y de cualquier cosa que no pueda ser programada etiquetada o clasificada.A fin de cuentas todos los servicios de Azure son parte de un todo mucho mas grande que los integra todos llevan dentro el caos y el orden la creacion y la destruccion. Todos son al mismo tiempo victimas y responsables de su propia vida.El problema es que no podemos ni sabemos como anticiparnos a todo. A veces los errores ocurren. Y a veces necesitamos lanzar un comando kill para matar un proceso o un reboot para reiniciar una maquina en una instancia de una Azure Web App.Aqui podras encontrar el video grabado de la sesion online"
    } ,
  
    {
      "title"    : "Azure WebApps Tips & Tricks",
      "category" : "",
      "tags"     : "azure, webapp, appservice",
      "url"      : "/2020/08/26/azure-webapps-tips-n-tricks",
      "date"     : "2020-08-26 01:03:24Z",
      "content"  : "Si no nos anticipamos a los imprevistos si no esperamos lo inesperado en una nube con infinitas posibilidades podriamos hallarnos a merced de cualquiera y de cualquier cosa que no pueda ser programada etiquetada o clasificada.A fin de cuentas todos los servicios de Azure son parte de un todo mucho mas grande que los integra todos llevan dentro el caos y el orden la creacion y la destruccion. Todos son al mismo tiempo victimas y responsables de su propia vida.El problema es que no podemos ni sabemos como anticiparnos a todo. A veces los errores ocurren. Y a veces necesitamos lanzar un comando kill para matar un proceso o un reboot para reiniciar una maquina en una instancia de una Azure Web App.Este es el caso que nos ocurrio hace unos dias. En una de nuestras Web Apps de produccion una instancia empezo a dar errores 500 Server Error. Eso significaba que una serie de usuarios estaban recibiendo paginas de error en lugar de respuestas correctas a sus peticiones. No todos pero si algunos.Antes de ponernos a analizar e intentar identificar el problema teniamos la obligacion de dar servicio a nuestros clientes. Y como algunas instancias si funcionaban y otras no decidimos que lo mejor era reiniciar el proceso de dotnet core de esa instancia.Pero a veces eso no es suficiente. Asi que hoy os mostraremos diferentes trucos operacionales relacionados con las Azure Web AppsAdvanced Application RestartAzure REST APIReiniciar los procesos especificos de una instancia de una Web AppGet Web Apps instancesGet Web Apps instance ProcessesRestart Web App Instance ProcessCambiar la instancia de una Web App por otraGet Web App Instance Process DetailsReplace WorkerAutoHealingAdvanced Application RestartLa forma mas sencilla de reiniciar los procesos que sirven nuestra pagina web en una Web App de Azure es ir al portal y seleccionar la opcion de Diagnose and solve problems dentro de la propia Web AppUna vez ahi se nos mostrara un buscador y varias opciones. Si en el buscador escribimos Advanced Application Restart nos aparecera la opcion que estamos buscando. Al hacer clic sobre ella navegaremos a un panel de controlAqui se nos mostraran 3 secciones y la forma de fijarnos en ellas es a la inversaPrimero marcaremos la instancia que esta generando errores. En este caso como podemos comprobar en las graficas anteriores es RD2818783C1B19.Si vamos a reiniciar mas de una instancia podemos decidir el Restart Sleep timer que es el tiempo que pasa entre que se reinicia una y otra.Cuando tenemos todo listo le damos al boton de Restart.Al cabo de un poco tiempo se nos mostrara un aviso de que las instancias han sido reiniciadas correctamente y podremos comprobar si esta dando o no errores.Azure REST APITodo el proceso  anteriormente descrito Advanced Application Restart se puede realizar usando las Azure REST APIs. Ademas estas APIs nos proveen de otro tipo de acciones mas complejas para cuando el problema parezca un poco mas dificil de resolver.Para consumir estos servicios tenemos que conseguir un access token mediante oauth en nuestro Azure AD httpsdocs.microsoft.comenusrestapiazure o como mostraremos a continuacion podremos usar el portal de documentacionGet Web Apps instancesLa jerarquia de recursos necesarios para tener disponible una Web App en Azure se podria definir de la siguiente formaApp Service Plan Hosting de Web AppsWeb App Aplicacion Web que se aloja en un App Service Plan. Pueden ser varias en dependencia del tamano y tipo de plan.Worker Cada App Service Plan tiene un conjunto de Workers donde se ejecutan los procesos de las Web Apps.Web App Instance cada Web App contiene una instancia por Worker. Esta instancia representa uno de los procesos en ejecucion.Processes cada instancia tiene una serie de procesos en ejecucion como por ejemplo el portal de SCM o el proceso de IIS que hospeda nuestra Web App.Por lo tanto antes de poder llegar a reiniciar un proceso tendremos que saber que procesos existen en las diferentes instancias de nuestra Web App. Y antes de poder conocer esos procesos tendremos primero que saber que instancias tenemos.Para eso sirve el primer metodo de la API. Para listar las instancias de nuestra Web App.Si navegamos aqui nos encontraremos una definicion del servicioAl pulsar en el boton verde de Try It podremos hacer identificarnos en Azure y conseguir autorizacion para usar este servicio desde el propio navegadorPara poder ejecutarlo tendremos que introducirname el nombre de nuestra Web App lo que viene antes del .azurewebsites.net.resourceGroupName el nombre de grupo de recursos al que pertenece la Web App.subscriptionId es un selector donde si tenemos mas de una suscripcion de Azure tendremos que elegir la que contiene la Web App.Al ejecutar el comando veremos que en la parte inferior de la pantalla nos aparece el resultado en formato jsonAqui lo mas importante es conocer el nombre de cada una de las instancias en el campo name.Get Web Apps instance ProcessesCada una de las instancias de una Web App tiene varios procesos en funcionamiento. La idea es coger un listado de los mismos.A tal fin navegaremos aquiRepetiremos la accion de darle al boton de Try It e identificarnos si hiciera faltaEn este caso tendremos que indicarinstanceId es la propiedad name que recogimos del listado de instancias como resultado del paso anterior.name el nombre de nuestra Web App el mismo que introdujimos en el paso anterior.resourceGroupName el nombre de grupo de recursos al que pertenece la Web App el mismo que introdujimos en el paso anterior.subscriptionId la suscripcion que tiene nuestra Web App la misma que introdujimos en el paso anterior.Al ejecutar obtendremos un listado de procesosCuando estamos hablando de IIS los procesos se llaman w3wp. Tendremos que buscar los procesos con ese nombre y guardar su id para poder realizar operaciones con el.Es posible que existan dos procesos con el name w3wp. Esto se debe a que Kudu el portal SCM de nuestra Web App es un proceso a parte de nuestra aplicacion dentro de IIS. Para poder diferenciarlos correctamente tendremos que llamar al servicio de Get Web App Instance Process Details y buscar el valor de la propiedad is_scm_site.Restart Web App Instance ProcessUna vez hemos llegado a este paso ya podemos matar el proceso que nos causa problemas. Despues de eliminarlo el propio IIS lo volvera a lanzar automaticamente en la proxima peticion o directamente en dependencia del estado del parametro AlwaysOn de la Web App. Por lo tanto aunque esta operacion sea borrar un proceso el efecto que se obtiene es que se reinicia.Nos dirigiremos a esta paginaHaremos clic en el boton de Try ItRellenaremos los siguientes parametrosinstanceId el nombre de la instancia de la Web App el mismo que introdujimos en los pasos anteriores.name el nombre de nuestra Web App el mismo que introdujimos en los pasos anteriores.processId es el campo id del proceso que recuperamos en el paso anterior.resourceGroupName el nombre de grupo de recursos al que pertenece la Web App el mismo que introdujimos en los pasos anteriores.subscriptionId la suscripcion que tiene nuestra Web App la misma que introdujimos en los pasos anteriores.Despues de ejecutar esperaremos dos tipos de resultado200 cuando se ha borrado el proceso correctamente.404 cuando ese proceso no existe o ya se ha borrado.Get Web App Instance Process DetailsSi queremos conocer mas detalles sobre los procesos como por ejemplo su relacion con los App Service Workers las maquinas virtuales donde se ejecutan nuestras instancias por asi decirlo tendremos que visitar esta paginaAqui encontraremos un servicio que nos devuelve los detalles de un proceso dentro de una instancia de una Web App de Azure.Para ejecutarla introduciremosinstanceId el nombre de la instancia de la Web App el mismo que introdujimos en los pasos anteriores.name el nombre de nuestra Web App el mismo que introdujimos en los pasos anteriores.processId es el campo id del proceso el mismo que introdujimos en el paso anterior.resourceGroupName el nombre de grupo de recursos al que pertenece la Web App el mismo que introdujimos en los pasos anteriores.subscriptionId la suscripcion que tiene nuestra Web App la misma que introdujimos en los pasos anteriores.Y en el resultado entre muchos parametros dentro de las variables de entorno encontraremos COMPUTERNAME que coincide con el nombre del App Service Worker y a su vez es el nombre que encontramos en la herramienta de monitoring cuando realizamos el split por instancia.Replace WorkerCuando los problemas parecen no solucionarse dentro de una instancia quiza es que se ha quedado corrupta por alguna razon. Asi que podriamos necesitar cambiarla.Si ya tenemos el nombre del Worker podemos realizar un cambio. Para ello usaremos el servicio de reboot workerEste servicio reinicia un App Service Plan Worker que es donde se hospedan nuestras instancias de Web App. Al reiniciarlo el sistema no espera a que vuelva a estar disponible lo sustituye automaticamente por otro Worker que si que esta funcionando. Asi que podriamos interpretar esta accion como un reemplazo de una instanciaIntroduciremosname el nombre del App Service Plan que hospeda nuestra Web App.resourceGroupName el nombre de grupo de recursos al que pertenece el App Service Plan.subscriptionId un selector donde tendremos que elegir la suscripcion que contiene nuestro App Service Plan.workerName el nombre del Worker que recogimos en el paso anterior.Despues de ejecutar podremos comprobar en la herramienta de monitoring como el worker que hemos reiniciado para de recibir peticiones y estas pasan a un Worker nuevo que entra en juego lineas rosa y morada respectivamenteY tambien podriamos apreciar como han dejado de suceder errores al cambiar de Worker.AutoHealingSi estamos experimentando problemas de forma periodica lo mas recomendable mientras se estudian y solucionan es usar una de las features ocultas que mas valor aportan a una Web App el AutoHealing.Esta herramienta nace sabiendo que algunos comportamientos inesperados pueden resolverse temporalmente con algunos simples pasos de mitigacion reiniciar el proceso iniciar otro ejecutable o requerir la recopilacion de datos adicionales. Esto es lo que nos va a permitir esta utilidad.Para acceder a ella tan solo tendremos que ir a la opcion de Diagnose and solve problems dentro de nuestra Web App en el portal de Azure y en el buscador introducir AutoHealing. Al hacer clic en la opcion que aparece encontraremos un nuevo panelAqui podremos configurar diferentes acciones reciclar procesos anadir trazas de log o acciones customizadas para diferentes eventos duracion de una request limite de memoria response status codes ....Dentro de esta configuracion hay ciertos detalles que los podemos especificar a nivel de codigo en el archivo web.config de nuestra Web App. Si bien es muy completa la configuracion hay algun caso que no termina de funcionar bien pero puedes leer mas sobre el tema aqui.ConclusionesEsta claro que como seres humanos no somos perfectos. Y por lo tanto nuestro software tampoco lo es. Nunca sabemos cuando vamos a tener un problema en produccion. Pero estos apuntes es posible que puedan salvarte en un momento de aprieto convirtiendo una larga caida en unos minutos de respuestas erroneas parciales.No obstante si estos trucos se convierten en una practica habitual es posible que estemos escondiendo otro tipo de problemas en nuestro software que deberiamos analizar mas a fondo."
    } ,
  
    {
      "title"    : "Novedades de c# 9",
      "category" : "",
      "tags"     : "csharp, dotnet, novedades, net5",
      "url"      : "/2020/08/19/csharp-9",
      "date"     : "2020-08-19 01:22:31Z",
      "content"  : "Este ano 2020 esta siendo como una montana rusa. Y el mundo de las tecnologias de desarrollo de Microsoft no iba a ser diferente. En noviembre se espera la presentacion de .Net 5  A unified platform. No no es el titulo de una pelicula de sabado por la tarde de antena 3. Es la nueva plataforma para gobernarlas a todas. Una plataforma para encontrarlas una plataforma para atraerlas a todas y atarlas en las tinieblas.Pero eso de que junten todos los frameworks en uno simplifiquen su nombre a .Net y le den una version con rima graciosa no es lo que importa en realidad. Lo que mas nos excita es que a .Net 5 le acompana una nueva version de su lenguaje insignia. Hoy os presentamos las novedades de C 9 y sus notas segun la escala sexiloca.Las normas son las de siempre eje vertical es lo util que nos resulta y el eje horizontal la locura de su implementacion. Y el objetivo de cada caracteristica es estar por encima de la diagonal Vicky Mendoza xy para poder considerarse una buena caracteristica.Y antes de que empeceis a juzgar las notas de cada una de las features comentar que el proceso de asignacion de este ano ha sido rigurosamente objetivo hemos desempolvado los dados de El Senor de los Anillos el juego de rol de la Tierra Media y los hemos lanzado 3 veces cada uno. Despues hemos calculado la mediana. Y para terminar se le ha aplicado un factor de correccion delta. A prueba de todo error.Al turronComo cada ano anadir que algunas de estas funcionalidades no llegaran a aparecer y otras podrian llegar a publicarse pero de una forma diferente a la que aqui exponemos.Initonly propertiesCuantas veces habremos tenido un error de compilacion por meter un objeto como readonly en el constructor y quererlo asignar en otro momento por primera y unica vez. La verdad es que no muchas pero si algunas. Y esto ya justifica esta funcionalidad.Cojamos como ejemplo este codigopublic class Place  public string Name  get set   public string Founder  get set var p  new Place  Name  Mordor  Founder  Sauronp.Name  MoriaAhora cambiemos el set de las propiedades por un init y tendremos un objeto inmutablepublic class Place  public string Name  get init   public string Founder  get init var p  new Place  Name  Mordor  Founder  Sauronp.Name  Moria  ErrorEs genial la inmutabilidad verdadY si os estais preguntando como se lleva el init con las propiedades readonly os dire que la mar de bienpublic class Place  private readonly string _name  private readonly string _founder  public string Name      get  _name    init  _name  value  throw new ArgumentNullExceptionnameofName    public string Founder      get  _founder    init  _founder  value  throw new ArgumentNullExceptionnameofFounder  Esta funcionalidad puede parecer trivial. Pero abrir la opcion a objetos inmutables de una forma sencilla y evitar ciertos errores con los readonly esta muy bien.ValoracionUseful  7Crazy   2RecordsUn record es un nuevo tipo especial cuyo origen podemos encontrar en el lenguaje F que esta basado en una class de C y hereda sus caracteristicas.public record Place  public string Name  get set   public string Founder  get set Podriamos crear un record inmutable con su constructor y su metodo de deconstruccionpublic record Place  public string Name  get init   public string Founder  get init   public Placestring name string founder     Name Founder  name founder  public void Deconstructout string name out int founder     name founder  Name FounderO definirlo como un record de tipo posicionalpublic record Placestring Name string FounderEsto dos ultimos ejemplos nos permitiran realizar operaciones de construccion y deconstruccionvar place  new PlaceMordor Sauronvar name founder  placeAdemas los tipos record admiten herenciapublic record NamedRecordstring Namepublic record Placestring Name string Founder  NamedRecordNameY anaden nuevos usos a la palabra clave withvar place  new PlaceMordor Sauronvar otherPlace  place with  Name  Baraddur Aqui se crearia un nuevo record como una copia de place y se cambiaria la propiedad Name. Una sintaxis muy comoda que seguro que nos trae mas opciones en el futuro.Si quereis profundizar mas en esta feature os recomiendo una lectura de un articulo sobre records que escribio Eduard Tomas en su blog.De cualquier forma consideramos que esto de los records es la funcionalidad estrella de esta version 9 de C. Y creemos que va a ser tan util como loco puede resultar el hacer tanta magia por detras usando una palabra clave nueva.ValoracionUseful  9Crazy   8Pattern matchingEn la version 8 de C ya pudimos ver un gran avance en el Pattern matching. En esta nueva entrega sigue mejorando.Lo que podriamos definir con las caracteristicas de C 8 comostring SayPopulationobject place   place switch      Place t when t.Population  1000000  Great orc horde    Place t when t.Population  100000  Orc horde    Place t when t.Population  100  A few orcs    Place _  Orcs    _  throw new ArgumentExceptionNot a known place type nameofplace  Ahora queda simplificado por el uso de nuevas palabras clave como and y orstring SayPopulationobject place   place switch      Place t  t.Population switch           1000000         Great orc horde       100000  Orc horde       100   A few orcs                   _  Orcs        not null  throw new ArgumentExceptionNot a known place type nameofplace    null    throw new ArgumentNullExceptionnameofplace  Ademas se ha anadido la comparacion a null y la not null. Esta ultima nos vendra muy bien la hora de realizar comparaciones de tipoif e is Customer  ...   oldif e is not Customer  ...   C 9Una evolucion que viene siguiendo la linea de simplificar la sintaxis y que ayudara a tener un codigo mas claro.ValoracionUseful  8Crazy   3Target typingSi ya tenemos el tipo no tendremos que volver a repetirlo cuando hagamos un newPlace p  new Mordor SauronY usando ternarios tipo  o  ahora ya no tendremos que hacer casting de objetos con una base comun o del nullPerson person  student  customer  Shared base typeint result  b  0  null  nullable value typeEsta ultima opcion me parece que ayudara muchisimo porque muchas veces terminas haciendo un casting en un ternario que lo ensucia bastante y hace que pierda claridad. La mayor parte de las veces que lo hago termino metiendo un bloque if para que no quede una linea tipo churro. Pero con esta nueva feature quedara perfecto.ValoracionUseful  7Crazy   1Covariant returnsLa contravarianza habilita la conversion de referencias implicita de tipos base. Esto implica que podamos crear una clase con un metodo y otra derivaba con la sobreescritura de ese metodo pero usando un objeto de retorno hijoabstract class Animal  public abstract Food GetFoodclass Tiger  Animal  public override Meat GetFood  ...Esto nos parece muy loco porque es un cambio innocuo que aportara poco valor a la hora de la verdad. Y lo que es mas importante ya podemos reproducir este comportamiento actualmenteabstract class Animal  public abstract Food GetFoodclass Tiger  Animal  public override Food GetFood  new Meat...Asi que las valoraciones en este caso van a ser un poco duras.ValoracionUseful  1Crazy   9Toplevel programsCuando queremos crear un programa simple de consola como un ola k ase en C tenemos que escribir un monton de movidas absurdas que no ayudan nada. Cosas como una clase o un metodo estatico llamado Mainusing Systemclass Program  static void Main      Console.WriteLineola k ase  Pues esto se acabo. Ahora podemos ponernos a escribir directamente nuestro programa de consola como si esto fuera node con javascriptusing SystemConsole.WriteLineola k aseY ademas nos han solucionado la vida. Esta feature nos permite aceptar casi cualquier sentencia como el using o un await. Y lo que es mejor si necesitas los parametros de entrada existe una variable llamada args donde por arte de magia los encontraras.Bajo nuestro punto de vista si esta caracteristica no va vinculada a una suerte de C Script no aporta demasiado a nuestro codigo.ValoracionUseful  2Crazy   8ConclusionesHemos podido ir jugando con la mayoria de estas features y la verdad es que el protagonista es el nuevo tipo record. Puede ser que debamos empezar a normalizar su uso desde ya mismo porque parece ser que ha venido a quedarse. Es una suerte de implementacion ya medio formada tipo syntax sugar muy estetica y que nos va a ayudar mucho en nuestro dia a dia. Y quiza no solo a nivel de tratamiento de datos... si no al tiempo.Sobre el resto de caracteristicas consideramos  que representan una buena evolucion sobre lo ya existente. Y como es habitual alguna ida de olla tambien nos podemos encontrar.Los graficos resumen son los siguientesY dividiendo por la diagonal Vicky MendozaParece ser que este ano tenemos mas aprobados que suspensos. O quiza es porque estoy escribiendo esto de vacaciones y me encuentro magnanimo.Pruebalo tu mismoPodeis probar todo esto siguiendo estos pasosDescargar e instalar LINQPad6httpswww.linqpad.netLINQPad6.aspxAbrir la aplicacion.En el menu dirigirse a Edit  Preferences.En la ventana que aparece hay que seleccionar el Tab llamado Query.Marcad la opcion de use Roslyn Daily build for experimental C 9 support."
    } ,
  
    {
      "title"    : "Azure Summer Talks Community",
      "category" : "",
      "tags"     : "azure, webapp, appservice, cosmos, functions",
      "url"      : "/video/2020/07/31/azure-summer-talks",
      "date"     : "2020-07-31 00:00:00Z",
      "content"  : "Como cada ano para estas fechas desde Catzure organizan una mesa redonda con los mejores expertos de Azure de Espana para conversar sobre el cloud desarrollo y todo aquello que les aptezca. Y este ano he tenido la suerte de ser uno de los invitados.La mesa redonda estuvo compuesta por unos cracksAna Maria BisbeCarlos LandeCarlos MendibleElena SalcedoJavier PalloNacho Fanjulademas de un servidorAqui podras encontrar el video grabado de la sesion online"
    } ,
  
    {
      "title"    : "Introducción a la programación cuántica v2",
      "category" : "",
      "tags"     : "quantum, programming",
      "url"      : "/video/2020/07/09/introduccion-programacion-cuantica",
      "date"     : "2020-07-09 00:00:00Z",
      "content"  : "Te explicare por que estas aqui. Estas aqui porque sabes algo aunque lo que sabes no lo puedes explicar pero lo percibes. Ha sido asi durante toda tu vida. Algo no funciona en la programacion no sabes lo que es pero ahi esta como una astilla clavada en tu mente. Y te esta enloqueciendo. Esa sensacion te ha traido hasta aqui Sabes de lo que estoy hablando Te gustaria saber que es la programacion cuanticaAqui podras encontrar el video grabado de la sesion online para CrossDvlup sobre programacion cuanticaMaterial adicionalCodigo fuenteArticulosIntroduccion a Quatum Development iIntroduccion a Quatum Development iiIntroduccion a Quatum Development iii"
    } ,
  
    {
      "title"    : "Asp.Net core data protection",
      "category" : "",
      "tags"     : "aspnetcore, data, protection, dotnet",
      "url"      : "/2020/05/13/aspnet-core-data-protection",
      "date"     : "2020-05-13 07:00:41Z",
      "content"  : "Cuando encontre en stackoverflow mi primer algoritmo para encriptar datos fue como entrar por primera vez en Hogwarts la escuela de magia y hechiceria. No entendia muy bien que era eso pero podia copiar el codigo y hacer lo mismo en mis aplicaciones.La criptografia a pesar de ser una ciencia que resulta muy divertida es tambien muy complicada. Cada algoritmo las claves publicas las privadas los saltos base64 los certificados y sus movidas.Menos mal que Microsoft ha sacado una libreria para hacer toda esta mierda sin tener que copiar codigo y entiendo mas o menos lo mismo acerca de que es lo que pasa por dentro.Quick startCuentan las leyendas que existe un paquete de nuget llamado Microsoft.AspNetCore.DataProtection que puedes instalar en tus aplicaciones.Ese paquete contiene el conjuro necesario para evitar que los mortifagos se hagan con el libro de hechizos de Dumbledore. Esto es importante si no la batalla del bien contra el mal se decantaria del lado de ElQueNoDebeSerNombrado.Para usar este conjuro solo necesitamos sacar nuestras varitaspublic void ConfigureServicesIServiceCollection services   ...  services.AddDataProtectionRealizar un leve gesto de izquierda a derechapublic class SecureUserRepository  private readonly IDataProtector _protector  public SecureUserRepositoryIDataProtectionProvider protectionProvider      _protector  protectionProvider.CreateProtectornameofSecureUserRepository  Y recitar los hechizos de Protecto Unprotect segun la necesidadpublic IEnumerable LoadUsers  var users  ...  foreachvar user in users        user.Id  _protector.Protectuser.Id      user.Email  _protector.Protectuser.Email      user.Phone  _protector.Protectuser.Phone      yield return user  public UserDetails LoadUserstring encryptedId    var id  _protector.UnprotectencryptedId    var user  ...    return userWingardium LeviosaMore in depthEl secreto de su eficacia se basa en la funcion CreateProtectorstring. Segun el nombre que especifiquemos podremos llegar a desproteger un dato o no.Vamos a preparar unas pociones para sacar lo mejor de estos hechizosprivate readonly IDataProtectionProvider _providerpublic DataProtectionTests  var services  new ServiceCollection  services.AddDataProtection  var serviceProvider  services.BuildServiceProvider  _provider  _serviceProvider.GetServiceComo ya hemos visto podriamos proteger y desproteger una cadena de textoFactpublic void ProtectsString  const string expected  this is a random string  var protector  _provider.CreateProtectornameofProtectsString  var unreadable  protector.Protectexpected  var unprotector  _provider.CreateProtectornameofProtectsString  var readable  unprotector.Unprotectunreadable  Assert.NotEqualexpected unreadable  Assert.Equalexpected readableY tambien podemos proteger arrays de bytesFactpublic void ProtectsByte  const string expected  this is a random string  var byteArray  Encoding.UTF8.GetBytesexpected  var protector  _provider.CreateProtectornameofProtectsByte  var protectedBytes  protector.ProtectbyteArray  var unprotector  _provider.CreateProtectornameofProtectsByte  var unprotectedBytes  protector.UnprotectprotectedBytes  var actual  Encoding.UTF8.GetStringunprotectedBytes  Assert.NotEqualprotectedBytes unprotectedBytes  Assert.Equalexpected actualPero si cambiamos el nombre entonces nos encontraremos con una excepcionFactpublic void CanNotUnprotectWithDiferentProtectors  const string expected  this is a random string  var protector  _provider.CreateProtectornameofCanNotUnprotectWithDiferentProtectors  var unreadable  protector.Protectexpected  var unprotector  _provider.CreateProtectornameofProtectsString  Assert.Throws  unprotector.UnprotectunreadableExpecto PatronumTime limitedSi anadimos el paquete de nuget llamado Microsoft.AspNetCore.DataProtection.Extensions anadimos una funcionalidad muy interesante crear datos encriptados con fecha de caducidad a partir de la cual ya no se pueden desencriptar.Para ello necesitamos anadir al leve gesto de varita de izquierda a derecha otro movimiento de arriba a abajopublic class SecureTokenProvider  private readonly ITimeLimitedDataProtector _protector  public SecureUserRepositoryIDataProtectionProvider protectionProvider      var parentProtector  protectionProvider.CreateProtectornameofSecureTokenProvider    _protector  parentProtector.ToTimeLimitedDataProtector  Y cuando recitemos el hechizo Protect podremos anadir el tiempo que pasara hasta que ya no podamos realizar el Unprotectpublic string CreateToken  var text  ...  var token  _protector.Protecttext TimeSpan.FromDays7  return tokenY podriamos probar con otra pocima como al caducar la informacion nos da una excepcionFactpublic void CanNotUnprotectExpiredInformation  const string expected  this is a random string  var protector  _provider.CreateProtectornameofCanNotUnprotectExpiredInformation  var timeLimitedProtector  protector.ToTimeLimitedDataProtector  var unreadable  timeLimitedProtector.Protectexpected TimeSpan.FromSeconds1  Thread.Sleep1001  var unprotector  _provider.CreateProtectornameofCanNotUnprotectExpiredInformation  var timeLimitedUnprotector  unprotector.ToTimeLimitedDataProtector  Assert.Throws  timeLimitedUnprotector.UnprotectunreadableExpelliarmusKey ringA la hora de la verdad toda esta movida es muy segura. Para proteger el libro de hechizos de Dumbledore no solo estamos usando un movimiento de varita especifico name en el metodo CreateProtector. Ademas esta libreria usa internamente un juego de varitas keys de cifrado. Cada varita esta habilitada para diferentes ocasiones y se gestionan internamente en un almacen al que llamamos key ring. Una suerte de tienda de Ollivander donde ir almacenando las varitas antiguas y las nuevas en local o de forma distribuida y de una forma segura.Este key ring nos proporciona un lugar donde a pesar de que pase el tiempo podremos ir a buscar las claves criptograficas. Nos protegera en caso de que se generen nuevas. Y hara que todo el sistema se comporte perfectamente incluso cuando usemos diferentes instancias o aplicaciones.Para ello se nos permitira definirAplicacionPor defecto el sistema buscara el nombre de la aplicacion segun las dlls y el contenido que estamos ejecutando. Pero para estar seguros de que usamos el mismo key ring incluso entre diferentes aplicaciones lo mejor es poner un nombre en la configuracion para nuestra aplicacionservices.AddDataProtection        .SetApplicationNameshared app namePersistenciaPodemos especificar un lugar donde almacenar las claves criptograficas. Podriamos llegar a usar una unidad local o compartidaservices.AddDataProtection        .PersistKeysToFileSystemnew DirectoryInfosharedprotectionO instalando diferentes paquetes adicionales como por ejemplo Microsoft.AspNetCore.DataProtection.AzureStorage almacenarlas en un Azure Storage en AWS Redis ...services.AddDataProtection        .PersistKeysToAzureBlobStoragenew UriProteccionOtro detalle que tenemos que tener en cuenta es la proteccion de las claves de cifrado. La libreria nos provee de una serie de facilidades con las que podremos anadir una proteccion de estas claves por certificadoservices.AddDataProtection        .PersistKeysToFileSystemnew DirectoryInfosharedprotection        .ProtectKeysWithCertificatethumbprintO por ejemplo usando un Azure Key Vaultservices.AddDataProtection        .PersistKeysToAzureBlobStoragenew Uri        .ProtectKeysWithAzureKeyVault  Rotacion de clavesSi no especificamos lo contrario se iran generando nuevas claves en el key ring cada 90 dias. Pero puede ser que esta periodicidad no nos venga bien. Pero podremos especificar cada cuantos dias queremos rotarlasservices.AddDataProtection        .SetDefaultKeyLifetimeTimeSpan.FromDays14No generar claves automaticamentePor ultimo se podria dar el caso de que no queramos que una aplicacion o una instancia esclava de nuestro sistema creara claves para ello podriamos usar la siguiente configuracionservices.AddDataProtection        .DisableAutomaticKeyGenerationBasicamente nos encontraremos con un poder absoluto sobre el comportamiento de este key ring. Aunque personalmente lo que recomendaria por mi experiencia seria usarNombre de aplicacionAzure Storage como almacenamientoAzure Key Vault como proteccionCon estas 3 configuraciones tendriamos un sistema de proteccion escalable seguro y distribuido.Avada KedavraOtras configuracionesOtras posibilidades de Asp.Net Data Protection es la personalizacion de el algoritmo a usar para encriptar la informacionservices.AddDataProtection        .UseCryptographicAlgorithms        new AuthenticatedEncryptorConfiguration                    EncryptionAlgorithm  EncryptionAlgorithm.AES_256_CBC            ValidationAlgorithm  ValidationAlgorithm.HMACSHA256        Pero si esta configuracion nos parece insuficiente siempre podemos implementar un algoritmo propioservices.AddDataProtection        .UseCustomCryptographicAlgorithms...AlohomoraConclusionesEs genial tener este libro de magia. Nos aporta un nuevo punto de vista de la proteccion de la informacion dentro de nuestras aplicaciones en Asp.Net Core. Un estandar. Ademas tienen otras aplicaciones como podria ser la generacion de tokens de validacion y otras cosas relacionadas con la seguridad.En mis proyectos lo estamos usando ya asi que...Obliviate"
    } ,
  
    {
      "title"    : "Azure DevOps: El panel de expertos",
      "category" : "",
      "tags"     : "azure, devops, alm, github, git, iac, monitoring, cac",
      "url"      : "/video/2020/04/24/gab-azure-devops",
      "date"     : "2020-04-24 17:10:00Z",
      "content"  : "En esta sesion del Global Azure Bootcamp 2020 nuestros expertos van a repasar la actualidad del universo Azure DevOps y su ecosistema. Se abordan temas tales como Github Ephimeral Agents IaC monitorizacion MLOps y otras muchas cosas mas.Conversacion distendida junto aAlejandro Almeida como moderador.Luis Fraile como contertulio.Alberto Picazo como contertulio.Sergio Navarro Pino como contertulio.y un servidor tambien como contertulio.Aqui podras encontrar el video grabado para la sesion de la Global Azure Bootcamp Virtual de 2020 sobre Azure DevOps"
    } ,
  
    {
      "title"    : "Unit testing tips",
      "category" : "",
      "tags"     : "best-practices, unit, test, csharp",
      "url"      : "/2020/04/22/unit-testing-tips",
      "date"     : "2020-04-22 09:15:41Z",
      "content"  : "Cuando preguntas a la gente por unit tests en mi experiencia te puedes encontrar con tan solo un punado de posibilidades los que no saben que son los que dicen saber que son y los que saben que son. Y si miramos mas profundamente este ultimo grupo nos encontraremos con los que dicen usarlos cuando deben los que dicen usarlos cuando pueden los que dicen usarlos siempre y los que los usan cuando pueden.Lo que esta claro es que no todos hacemos todos los tests que deberiamos. Y la culpa es de nuestra naturaleza developer...Si me dices que disene una arquitectura distribuida de trescientas capas voy a perder el culo haciendo diagramas de todas las movidas que se me ocurran. Voy a empezar un proyecto de prueba de concepto de esta nueva arquitectura. Voy a hacer una charla sobre las chorradas que me pasan por la cabeza. Le voy a poner un nombre molon como Ntexture chocolate cake architecture. Igual hasta monto un twitch para monetizar las locuras que brotan a borbotones de mi mente.Pero si me dices que consiga un 80 de cobertura de unit tests me voy a meter en LinkedIn a ver si hay un curro chulo de hacer arquitecturas nuevas. Porque no nos enganemos probar codigo es un penazo. Aunque por otro lado si hay que hacerlo por que no vamos a dar lo mejor de nosotros mismos y programar los mejores tests que podamosCon esta premisa en mente me he propuesto escribir unos pocos consejos sobre como intento escribir unit tests cuando puedo hacerloArt of Unit TestingTests triple ATest FIRSTTest doublesUsa nombres descriptivosUsar TraitAttributeNo usar expected como paramDescribe el contextoAssumePrueba un Bug luego lo corrigesArt of Unit TestingSegun The art of unit testing de Roy Osherove y editado por Manning Publications co un unit test es una parte de un codigo generalmente un metodo que invoca otra parte del codigo y luego verifica la exactitud de algunas suposiciones. Si las suposiciones resultan ser incorrectas el unit test ha fallado.Y parecera una tonteria exponerlo porque si estas leyendo esto es muy posible que ya sepas lo que es un unit test. Pero en serio hay que recordarlo cada cierto tiempo. Es muy facil que como programadores se nos olvide que es exactamente un test unitario y hagamos mas cosas de las que debemos dentro de una prueba.Factpublic void IsPrime_returns_false_When_number_is_2    var primeService  new PrimeNumbersService    var result  primeService.IsPrime2    Assert.Falseresult 2 should not be primeTests triple ALos unit tests son como los buenos videojuegos. Los de la triple A son los mejores. Los GotY.Cuando hablamos de triple A en unit testing nos referimos a los diferentes pasos que debe dar un testArrange en esta fase se prepara el entorno para poder ejecutar la prueba.Act realizamos la prueba en cuestion.Assert comprobamos que el estado del entorno es el esperado.public void Sum_returns_5_When_input_numbers_are_2_and_3     Arrange    const int expected  5    const int inputA  2    const int inputB  3    var target  new Calculator     Act    var actual  target.SuminputA inputB     Assert    Assert.Equalexpected actualUna tecnica que nos puede ayudar a acostumbrarnos a esto es a la hora de escribir un test siempre empezar usando este code snippetFactpublic void MyTestMethod     Arrange     Act     Assert    Assert.FalsetrueTest FIRSTComo en cualquier otra tecnica del desarrollo los unit tests tienen un acronimo para definir los principios por los que se rige la excelencia en la materia. En este caso es FIRSTFast una prueba unitaria tiene que ser muy muy rapida. La unidad para medirlo debe ser milisegundos. Y mejor 1ms que 200ms. Vamos a hacer muchas pruebas no querras estar 2 horas mirando la pantalla noIsolated tu test tiene que estar aislado de los demas tests. Deben ser independientes totalmente. No empieces a crear un objeto en un test y que otro lo borre. Eso no son tests unitarios. Tienes que poder ejecutar un unit test en cualquier momento y en cualquier orden.Repeatable las pruebas deben ser repetibles todas las veces que se necesite. Y si no hemos modificado el codigo fuente debemos obtener siempre el mismo resultado de cualquier ejecucion de un test.SelfValidating una prueba unitaria tiene que ser validada por ella misma. De esta forma evitaremos cualquier interaccion manual que decida si el test se ha finalizado con exito o no.Timely el unit test debe ser escrito en el momento oportuno. Antes del codigo que lo resuelve. Si no corremos el riesgo de que nuestro test no tenga efecto. Asi que si no practicamos TDD lo mejor es al menos comprobar que el test puede fallar.Test doublesEl termino generico de Test Double acunado por Gerard Meszaros se usa para referirse a cualquier caso en el que con fines de prueba se reemplaza un objeto de produccion por otro no productivo. Hay varios tipos de doblesDummy son objetos que se intercambian como parametro pero que no se usan en realidad. El ejemplo mas claro de un dummy es un DTO con unos valores concretos.Fake son objetos que tienen implementacion funcional. Pero generalmente toman un atajo que los hace no adecuados para la produccion p.e. UseInMemoryDatabase de EF Core.Stubs proporcionan respuestas enlatadas a las llamadas realizadas durante la prueba.Spies Los espias son stubs que ademas registran informacion en funcion de las llamadas que se realizan. Por ejemplo un servicio de correo electronico que registra cuantos emails se han enviado.Mocks son objetos preprogramados. Tienen expectativas sobre las llamadas que se espera que reciban. Pueden lanzar una excepcion si reciben una llamada que no esperan y se verifican durante la fase de Assert.Los Test Doubles son fundamentales en nuestros unit tests para poder desencapsular procesos complejos y asi poder probar un solo caso unitario.En las librerias de Test Doubles de .net lo mas comun es encontrar objetos Mock que tienen implementados Spies y Stubs p.e. MoqTheoryInlineDataPaw0rdpublic void Password_is_valid_When_value_isstring password     Arrange    var dummy  new PasswordOptions            RequireDigit  true        RequireLetter  true        RequiredLength  6        var stub  new Mock    stub.Setupx  x.GetPasswordOptions.Returnsdummy    var target  new PasswordServicestub.Object     Act    var actual  target.Validatepassword     Assert    Assert.Trueactual.SuccededUsa nombres descriptivosQue un producto como Azure DevOps haya tenido una multitud de nombres tales como Team Foundation Server Online Team Foundation Services o Visual Studio Online no es mas que un ejemplo de que poner nombres es muy dificil. Pero por favor no hagas estoFactpublic void Test1_Passstring password     ...Existen muchisimas formas de nombrar los unit tests. Elije una que te sirva y con la que estes a gusto. Y usala pero pensando en los inputs y outputs.En mi caso personal me gusta ignorar el convenido de nombres original de dotnet y usar un formato tipo snake_case semejante al siguientepublic class PasswordService_Validate_Should  public void Return_failed_When_password_has_less_than_6_characters   ...public class PasswordService_Hash_Should  public void Throw_argument_exception_When_password_is_null  public void Throw_argument_exception_When_password_is_empty  public void Return_unreadable_hash_When_password_is_valid  public void Return_the_same_unreadable_hash_When_password_is_the_samePuedes usar otra notacion que te parezca mejor. Aunque al final todas son bastante parecidas. Lo que hay que tener en cuenta es no tener miedo a escribir nombres largos. Cuanto mas descriptivos sean muchisimo mejor.Tambien en las variablesNo olvidemos que la forma de nombrar no es solo para los nombres de clases y metodos. Las variables y constantes tambien deben estar bien descritas. Podemos nombrarlas segun su mision o segun el estado que van a generar.const ValidationResult succeded_result  ValidationResult.Succededvar target  new PasswordServiceoptionsServiceStub.Objectvar actual  target.Validatevalid_passwordAssert.Equalsucceded_result actualDondetarget es el objeto que vamos a testear.expected es el estado objetivo del test.actual es el estado resultado del test.optionsServiceStub es un stub del servicio OptionsService.valid_password un password valido.No usemos magic numbers ni magic string usemos constantes con nombres referentes a su uso para elloprivate const string valid_password  PssW0rdprivate const string invalid_password  Pssprivate const string unsecure_url  httpdummy.comprivate const string secure_url  httpsdummy.comprivate static readonly User some_user  new User  Id  1 DisplayName  Test Usar TraitAttributeLo mejor a la hora de tratar nuestros unit tests es pensar en hacer codigo legible que nos ayude a formar parte de la documentacion.Existe un atributo en xunit que nos ayudara a clasificar los tests Trait.TraitCategory UnitTraitClass nameofUserServiceTraitMethod nameofUserService.DeleteUserpublic class DeleteUserService_DeleteUser_ShouldEsta forma de clasificar las pruebas nos va a permitir luego realizar filtrosdotnet test filter CategoryUnitClassUserServiceNo usar expected como paramEn ocasiones podemos encontrarnos un caso en el que podamos usar parametros para realizar todas las pruebas que deseamos con un solo metodo de test. Esto es una buena practica desde el punto de vista de la programacion ya que evitamos repetir codigoTheoryInlineDatatrue Paw0rdInlineDatatrue Passw0rd...InlineDatafalse Pa0InlineDatafalse public void Return_expected_When_passwor_isbool expected string password    var target  new PasswordService    var actual  target.Validatepassword    Assert.Equalexpected actual.SuccededPero esta forma de programar unit tests nos va a ocultar un poco que es lo que esta sucediendo realmente dentro. Por eso deberiamos intentar de omitir que el resultado de una prueba sea uno de los parametrosTheoryInlineDataPaw0rdInlineDataPassw0rd...public void Return_succeded_When_password_is_validstring valid_password    var target  new PasswordService    var actual  target.Validatevalid_password    Assert.Trueactual.SuccededInlineDataPa0InlineData...public void Return_failed_When_password_is_invalidstring invalid_password    var target  new PasswordService    var actual  target.Validateinvalid_password    Assert.Falseactual.SuccededDe esta manera dejamos explicito que estamos probando password validos que se validan con exito en un metodo y en el caso contrario en el otro.Describe el contextoLa tecnica de refactoring en los unit tests nos va a llevar a generar funciones que realizan casi todo el trabajoFactpublic void Return_succeded_When_length_is_equal_than_expected    var target  CreateTargetToSucceded    var actual  target.Validatevalid_password    IsSuccededactualPero a veces escribir mas y detallar parametros que quiza no eran necesarios nos puede dejar mas claro que hace nuestro testFactpublic void Return_succeded_When_length_is_equal_than_expected    var target  CreateTargetWithmin_length 6    var actual  target.Validatelength_6_password    AssertIsSuccededactualAssumePara ayudar con la documentacion del codigo usando unit tests tenemos el paquete de xunit.Assume.El uso de la cuarta A deja implicito en nuestro test cuando y por que lo estamos saltandoAssumeFactpublic void Return_somthing_When_any_thing     Arrange     Assume    Assume.TrueIsWindows This OS is not supported     Act     Assertprivate static bool IsWindows    return RuntimeInformation.IsOSPlatformOSPlatform.WindowsAqui dejariamos patente que este test solo corre en sistemas Windows. Pero no generariamos un error. Solo un aviso y skip cuando se ejecute en otro tipo de maquina.Prueba un Bug luego lo corrigesCuando te encuentres un bug nuevo en el sistema es evidente que no tenias un test que cubriera ese caso previamente. Si no no hubiera sido un bug. Asi que antes de nada crea un unit test que haga evidente este error.Si por ejemplo nuestro servicio lanza una excepcion cuando el password es nulo y nos gustaria que en su lugar devuelva un estado de errorFactpublic void Return_failed_when_password_is_null    var target  new PasswordService    var actual  target.Validatenull_password    Assert.Falseactual.SuccededAl pasar nuestro test fallara asi que ahora ya podemos corregirlo y ver como se pone en verde.ConclusionesEscribir buenos unit test requiere practica aunque en realidad esto es cierto para casi cualquier actividad en la vida. Al seguir algunas de las reglas de esta lista podemos mantener las pruebas limpias faciles de mantener y comprender y con el potencial de generar... blah blah blah... que programes bien cono ya"
    } ,
  
    {
      "title"    : "No comments...",
      "category" : "",
      "tags"     : "",
      "url"      : "/2020/04/11/no-comments",
      "date"     : "2020-04-11 11:02:29Z",
      "content"  : "Muchas gracias a todos por los comentarios que habeis ido dejando en este blog. Siempre me habeis animado a seguir escribiendo y de alguna manera le habeis dado sentido a todo esto. Hoy admito mi derrota. Las redes sociales han ganado. Hoy comienza la era de sin comentarios.No me gustan los cambios. Hasta que no me veo abofeteado por la cruda realidad no me doy cuenta. Lo disimulo diciendo que es vintage pero ya me conoceis. Hoy en dia el lugar donde todo el mundo comenta todo son las redes sociales. Basta de resistirnos. Critiquemos duramente este blog en general y mas concretamente a mi en las plataformas mejor habilitadas para ello.Me rindo.Tan solo queda retirarse dignamente y dar las gracias de nuevo a aquellos que alguna vez comentasteis.Nos leemos en Twitter.AtentamenteFernando Escolar aka el Developerro.Pd. Los comentarios seguiran en disqus."
    } ,
  
    {
      "title"    : "N-Texture Chocolate Cake Architecture",
      "category" : "",
      "tags"     : "best-practices, csharp, architecture",
      "url"      : "/video/2020/04/09/n-texture-chocolate-cake-architecture",
      "date"     : "2020-04-09 00:00:00Z",
      "content"  : "La arquitectura de la tarta de chocolate de ntexturas tambien conocida como arquitectura tarta sacher o por su nombre mas informal Vertical Slice es un modelo de desarrollo que ha venido a revolucionar el mundo de la arquitectura de software tal y como lo conocemos.Aqui podras encontrar el video grabado de la sesion de la NetCoreConf Valencia 2020 sobre Vertical Slice Architecture"
    } ,
  
    {
      "title"    : "Introducción a la programación cuántica",
      "category" : "",
      "tags"     : "quantum, programming",
      "url"      : "/video/2020/04/04/introduccion-programacion-cuantica",
      "date"     : "2020-04-04 00:00:00Z",
      "content"  : "Te explicare por que estas aqui. Estas aqui porque sabes algo aunque lo que sabes no lo puedes explicar pero lo percibes. Ha sido asi durante toda tu vida. Algo no funciona en la programacion no sabes lo que es pero ahi esta como una astilla clavada en tu mente. Y te esta enloqueciendo. Esa sensacion te ha traido hasta aqui Sabes de lo que estoy hablando Te gustaria saber que es la programacion cuanticaAqui podras encontrar el video grabado de la sesion de la NetCoreConf Virtual 2020 sobre programacion cuanticaMaterial adicionalCodigo fuenteArticulosIntroduccion a Quatum Development iIntroduccion a Quatum Development iiIntroduccion a Quatum Development iii"
    } ,
  
    {
      "title"    : "Github Actions vs. Azure Pipelines",
      "category" : "",
      "tags"     : "azure, devops, github, pipelines, actions",
      "url"      : "/video/2020/01/25/github-actions-vs-azure-pipelines",
      "date"     : "2020-01-25 00:00:00Z",
      "content"  : "Analizaremos a fondo Github Actions y Azure Pipelines en busca de sus ventajas y sus puntos debiles. Sera un combate a muerte entre estas dos tecnologias mellizas para gestionar nuestro CICD.Aqui podras encontrar el video grabado de la corta pero intensa sesion de la NetCoreConf Barcelona 2020 scon una comparativa entre Github Actions y Azure Pipelines"
    } ,
  
    {
      "title"    : "Azure hex-char-counter tutorial",
      "category" : "",
      "tags"     : "azure, webapp, appservice, node",
      "url"      : "/2019/10/01/azure-hex-char-counter",
      "date"     : "2019-10-01 13:51:23Z",
      "content"  : "El otro dia en el trabajo surgio la necesidad de crear un sistema online complejo y escalable para la resolucion de un problema general que teniamos. El objetivo era conseguir el tamano de una cadena de texto en formato hexadecimal. Lo primero que nos vino a la cabeza fue Azure como podria ayudarnosLa respuesta a esta pregunta es que de una forma muy eficiente. Si no no hubieramos escrito este articulo... Ya teniamos proyecto. Y su codename HCC HexadecimalCharactersCounter. Y he aqui como lo ejecutamosImplementando HCCLo primero que tenemos que hacer es ir al portal de Microsoft Azure y crear un servicio PaaS donde hospedar nuestra aplicacion. Asi que buscaremos en el marketplace el recurso de tipo Web AppEntonces tendremos que configurar los datos sobre la suscripcion el grupo de recursos el tipo de aplicacion usaremos la de por defecto .Net 4.7 en una maquina Windows la region y el tamano del servicioEl tema de monitoring lo dejaremos configurado por defecto. Esto implica que creara un servicio de App Insights con el mismo nombre que nuestra aplicacion.Una vez hemos revisado todo creamos el servicio dentro de Azure y tras unos pocos segundos se notificara de que ya esta disponiblePor cuestiones de facilidad hemos decidido utilizar nodejs como lenguaje de programacion de servidor. Asi no tendremos que crear complejos procesos de compilacion. Para ello tendremos que anadir un en la configuracion del App Service un AppSettting con el nombre WEBSITE_NODE_DEFAULT_VERSION y con el valor de una version estable como por ejemplo la 6.2.2Despues de guardar todos los cambios tendremos que subir la aplicacion en cuestion que ayudara a los servicios de Azure a realizar la cuenta de la longitud de una cadena de texto en hexadecimal. Asi que podemos abrir el editor de codigo online seleccionando la opcion de App Service Editor Preview del menuUna vez en este editor tendremos que crear un nuevo archivo llamado index.js con el siguiente contenidoconst http    requirehttpconst port    process.env.PORT  3000const server  http.createServermyServerfunction myServerrequest response response.writeHead200  ContentType textplain     response.endrequest.url.substring1    returnserver.listenportconsole.logServer running at port    portComo podemos observar nuestro servicio va a devolver en forma de texto el valor que pasemos a la url de nuestro explorador. Si por ejemplo ponemos en la barra de direccion httpdomain.comsomething devolvera something.Despues tendremos que crear otro archivo llamado Web.config donde le diremos al servicio de Azure como puede ejecutar nuestra aplicacionEl contenido de este archivo deberia ser parecido a este                                                                                                    Con esto ya podemos probar nuestra aplicacion. Pero entonces os dareis cuenta de que solo devuelve texto y que en ningun momento estamos contando los caracteres de la cadenaAqui es donde una de las mejores features de Microsoft Azure nos pueden ayudar. Con 3 sencillos pasos vamos a conseguir aplicar la magiaDentro del App Service ir al menu de Application InsightsCambiar el valor de Collection level a RecommendedPulsar el boton de ApplyUna vez termine la actualizacion si entramos de nuevo en nuestra aplicacion web podremos ver que tendremos una respuesta diferenteDonde en la primera linea de la respuesta podremos ver el tamano de la cadena de texto que hemos enviado en hexadecimal.HCC ya esta funcionando en un entorno escalable y al servicio de todos. Mision cumplidaConclusionesSon solo dosQue facil es hacer cosas con Azure. Los chicos de Microsoft estan haciendo un gran trabajo.Me parece genial lo de estas features ocultas que meten sin avisar. Aportan muchisimo valor. Aunque vendria muy bien que estuvieran documentadas."
    } ,
  
    {
      "title"    : "Terraform con Azure",
      "category" : "",
      "tags"     : "azure, terraform",
      "url"      : "/2019/07/19/terraform-azure",
      "date"     : "2019-07-19 07:51:23Z",
      "content"  : "La terraformacion es una serie de tecnicas que aplicadas en conjunto conseguirian dotar a un planeta o asteroide inerte de una serie de caracteristicas semejantes a las de la tierra. De esta forma se conseguiria un planeta habitable. Y aunque pueda parecer pomposo que lo es Terraform va de eso mismo pero con el cloud. No es que queramos que la nube sea habitable para un ser humano pero si para nuestras aplicaciones.Terraform es una herramienta que nos ayuda a gestionar infraestructura como codigo. Hereda las caracteristicas de esta practica la capacidad de versionar construir actualizar o borrar infraestructura sin tener que interactuar fisicamente con el hardware o con herramientas interactivas. Asi conseguimos una forma de administracion de sistemas informaticos mas potente y sencilla. Al menos para un programador.Y como se usa esto con AzureInstalandoInstalar Terraform en Windows como dice un colega es un poco tricky. Basicamente consiste enDescargar el archivo .exe de su web oficial.Almacenar ese archivo en un directorio local.Anadir a la variable de entorno PATH la ruta de esa carpeta.Para el resto de sistemas operativos recomendaria usar los repositorios de paquetes habituales. Por ejemplo en macOS puedes usar brew brew install terraformProyecto para AzureCreando un service principalPara empezar a usar Terraform con Microsoft Azure lo primero que tendremos que hacer es tener unas credenciales de una cuenta de service principal para nuestra suscripcion de Azure. Para obtenerlas podemos usar la herramienta Azure CLI az loginDespues de introducir nuestra credenciales se listaran las suscripciones a las que pertenecemos. De ahi hay que leer el parametro id de la que nos interese. Despues seleccionamos esa suscripcion para trabajar con ella az account set subscriptionSUBSCRIPTION_IDY finalmente creamos la cuenta de service principal az ad sp createforrbac roleContributor scopessubscriptionsSUBSCRIPTION_IDEsta peticion nos devolvera algo como esto  appId ...  displayName ...  name ...  password ...  tenant ...DondeappId equivale a client_idpassword es el client_secrettenant es en realidad el tenant_idInicializando TerraformAntes de inicializar Terraform tendremos que crear un archivo .tf pe. main.tf. En este archivo introduciremos el proveedor que queremos utilizar y las credenciales de service principal necesarias para su usoprovider azurerm   subscription_id  ...  client_id        ...  client_secret    ...  tenant_id        ...Ahora ejecutaremos en la consola o el terminal el comando de inicializacion terraform initEsto descargara todo lo necesario para poder conectar con Azure e interactuar. Los archivos se almacenaran en una carpeta con el nombre .terraform que podra ser excluida del repositorio de codigo fuente.CodificandoComo habremos podido observar ya el formato para crear infraestructura en Terraform es el HCL HashiCorp configuration language. La idea es utilizar la siguiente estructuraresource tipo_de_recurso nombre_interno_recurso     propiedad1   valor1    propiedad2   valor2    propiedad3         sub_propiedad1  sub_valor1        sub_propiedad2  sub_valor2    De esta forma si quisiera crear un Resource Group de Azure podria anadir a mi archivo .tf algo como estoresource azurerm_resource_group mi_resource_group   name      pruebaterraform  location  West EuropeSi ahora quisiera anadir a este grupo un App Service Plan donde alojar mi pagina web anadiriaresource azurerm_app_service_plan mi_app_service_plan   name                 pruebaterraformserviceplan  location             azurerm_resource_group.mi_resource_group.location  resource_group_name  azurerm_resource_group.mi_resource_group.name  kind                 Windows  sku     tier       Standard    size       S1  Como podemos observar en este caso para definir los parametros location y resource_group_name estoy llamando directamente a las variables de salida de la creacion del grupo de recursos. Cada recurso por tanto tienes unos parametros de entrada que son los que escribimos en su definicion y unos de salida que podemos utilizar llamando a tipo_de_recurso.nombre_interno_recurso.parametro_salida.Finalmente para este ejemplo creariamos una Web App de App Servicesresource azurerm_app_service mi_app_service   name                 pruebaterraformweb  location             azurerm_resource_group.mi_resource_group.location  resource_group_name  azurerm_resource_group.mi_resource_group.name  app_service_plan_id  azurerm_app_service_plan.mi_app_service_plan.id  site_config     always_on          true    default_documents  default.aspxdefault.htmlindex.htmlhostingstart.html  Todos estos bloques que codigo se pueden anadir a un mismo archivo con extension .tf o se pueden almacenar en varios.OperandoTerraform es una herramienta de consola por lo que para realizar operaciones es necesario usar los diferentes comandos que tiene en un terminal. Los que mas veces vamos a utilizar sonplanCuando consideremos que ya tenemos listo nuestro codigo es el momento de probar que sintacticamente es correcto. Para ello ejecutaremos el comando plan terraform planLa salida de este comando si es correcto el contenido de los .tf sera un json descriptivo con la infraestructura que se va a crear modificar yo borrar. Y en caso de errores nos senalara donde se encuentran y nos informara de la causa.El comando plan al igual que todos los demas comando de Terraform buscara en el directorio donde lo ejecutemos todos los archivos con extension .tf y los tratara como uno solo. Ademas tampoco tenemos que preocuparnos por el orden en el que declaramos los recursos. Terraform buscara cual es el orden correcto que tareas puede paralelizar y como realizar la creacion de la infraestructura lo mas eficientemente que pueda.applyUna vez que estamos satisfechos con la propuesta que hemos visto en el plan es el momento de llevar esos recursos a la nube. Para ello usaremos el comando apply. Este comando realiza un incremental de actualizacion sobre nuestra infraestructuraSi no existe lo crea todoSi ya existe planifica la creacion modificacion y borrado de los recursos ya existentes con respecto los propuestos en nuestro codigo. terraform apply autoapproveHaber realizado un plan previamente no nos garantiza que no pueda fallar el apply. El primero calcula que la sintaxis sea correcta y apply se enfrenta directamente con Microsoft Azure. En esta plataforma existen mas normas como por ejemplo que el nombre de nuestro app service no exista previamente. Si estas normas de la plataforma no se ven satisfechas nos encontraremos ante errores en este punto.Hay que tener en cuenta que el comando apply se basa en la existencia de un estado almacenado. Si en un momento determinado el estado de nuestros recursos en Azure ha evolucionado de forma diferente a la ultima vez que ejecutamos el comando apply lo mas recomendable es sincronizar el estado usando el comando import.destroySi en un momento determinado queremos borrar todos los recursos que creamos anteriormente el comando que tendremos que utilizar es destroy. Este comando realizara la operacion contraria al apply dejando nuestra cuenta de Azure limpia de infraestructura. Es un comando muy util para crear y borrar entorno de desarrollo o prueba. La sintaxis es semejante a los anteriores comandos terraform destroyUso algo mas avanzadoHasta aqui hemos visto un quick start del uso de Terraform con Microsoft Azure. Pero los archivos de codigo .tf tienen mucha mas miga de lo que puede parecer en un principioVariables de entradaLas variables que mas vamos a utilizar son las de entrada Input Variables. Estas funciones se declaran comovariable nombre_variable   description  una descripcion de para que es esta variable  default  un valor por defectoPodemos prescindir de escribir un valor por defecto si es que no es necesario. Pero por favor no prescindas de poner una descripcion.Para usar este tipo de variables en codigo es tan facil como escribir var.nombre_variablevariable resource_group_name   description  The resource group name  default  testterraformresource azurerm_resource_group my_resource_group   name      var.resource_group_name  location  West EuropeSi queremos modificar el valor de una variable podemos hacerlo utilizandoEl argumento varnombre_variablevalor en nuestro comandoterraform apply varnombre_variablevalorUsando un archivo .tfvars que podremos referenciar con el argumento varfilemi_archivo.tfvarsterraform apply varfilemi_archivo.tfvarsTambien podemos hacer que nuestro archivo sea cargado automaticamente nombrandolo terraform.tfvars o terminando en auto.tfvars.Este archivo de tipo .tfvars contendra claves y valores en este formatonombre_variable_texto    valor variablenombre_variable_bool     falsenombre_variable_numero   1nombre_variable_lista    uno dosnombre_variable_objeto    parametro  valor Variables localesLas variables locales se declaran dentro de un bloque llamado localslocals   nombre_variable1  valor 1  nombre_variable2  valor 2Y para su uso se referencian con el formato local.nombre_variable1.Estas variables se pueden usar para componer otros valores diferentes a partir de variables de entrada o de salida. El ejemplo mas comun seria el de concatenar cadenas de textovariable environment variable application locals     name  var.environmentvar.applicationresourcegroupFuncionesTambien tenemos funciones de Terraform que nos permiten hacer operaciones mas complejasDesde buscar mascaras de un rango de IPs en formato CIDRvariable cidr  default  10.12.127.020 locals     ip    cidrhostvar.cidr 16  10.12.112.16    mask  cidrnetmaskvar.cidr   255.255.240.0Hasta abrir archivos como base64variable filepath  default  .certificate.pfx locals     certificate_base64  filebase64var.filepathBuclesTerraform nos permite realizar la creacion de el mismo recurso varias veces usando el parametro countresource azurerm_resource_group mi_resource_group   count     2  name      pruebaterraformcount.index  location  West EuropeEste codigo crearia dos grupos de recursos en mi cuenta de Azure uno con el nombre de pruebaterraform0 y otro pruebaterraform1. Recordad generar diferentes nombres para los recursos que se generan con el count si quereis evitar errores.Si quisiera referenciar el nombre de los recursos que acabo de crear tengo varias formas diferentesazurerm_resource_group.mi_resource_group0.name             pruebaterraform0elementazurerm_resource_group.mi_resource_group 1.name    pruebaterraform1elementazurerm_resource_group.mi_resource_group..name 0  pruebaterraform0azurerm_resource_group.mi_resource_group..name1           pruebaterraform1El parametro count tambien se puede utilizar como condicional asignandole los valores 1 o 0 en dependencia de un ternariovariable create_resource_group   default  falseresource azurerm_resource_group mi_resource_group   count     var.create_resource_group  1  0  name      pruebaterraform  location  West EuropeY tambien tenemos bucles for para la creacion de variables. Su comportamiento es semejante a un for each y nos da mucha versatilidadvariable ip_cidr   default   10.0.1.024 10.0.2.024 10.0.3.024 locals   subnets  for x in var.ip_cidr     ip    elementsplit x 0    mask  cidrnetmaskx  En este codigo convertiriamos una lista de rangos de IP en formato CIDR en un listado de objetos con las propiedades ip y mask.ModulosYa sabemos que podemos crear todos los archivos .tf que queramos. No es de extranar entonces que muchos desarrolladores piensen en dividir la creacion de una infraestructura completa en varios archivos clasificados por tipo de recurso. Algo como estomain.tfgroup.tfdatabase.tfcache.tfwebapp.tfwaf.tftraffic.tfTampoco creo que fuera descabellado pensar que si tengo ya archivos .tf especializados en una sola tarea por que no iba a reusar estos archivos en otros proyectos o infraestructuras.Los modulos de terraform vienen a solucionar las problematicas derivadas de este uso tan impio.Un modulo de terraform se define con una serie compuesta por variables de entrada definicion de recursos y variables de salida. Por lo que es muy comun encontrarnos y recomendable usar una estructura de ficheros como la siguientemi_modulovars.tfmi_modulomain.tfmi_modulooutput.tfSi quisieramos crear un modulo para la creacion de una Web App creariamos una nueva carpeta llamada webapp dentro de la carpeta modules moduleswebappvars.tfvariable resource_group_name variable location variable tier variable size  moduleswebappmain.tfresource azurerm_app_service_plan mi_app_service_plan   name                 pruebaterraformserviceplan  location             var.location  resource_group_name  var.resource_group_name  kind                 Windows  sku     tier       var.tier    size       var.size  resource azurerm_app_service mi_app_service   name                 pruebaterraformweb  location             var.location  resource_group_name  var.resource_group_name  app_service_plan_id  azurerm_app_service_plan.mi_app_service_plan.id moduleswebappoutput.tfoutput id   value  azurerm_app_service_plan.mi_app_service_plan.idoutput name   value  azurerm_app_service.mi_app_service.nameAqui podemos observar una novedad las variables de salida.Como se puede ver es muy simple declararlas basta con poner output el nombre de la variable y una propiedad donde encontraremos el valor.Si ahora queremos utilizar nuestro modulo lo haremos usando el bloque module. Este bloque tiene una propiedad llamada source en la que indicamos el path de la carpeta que contiene nuestro modulo. Y despues podemos anadir como parametros el resto de variables de entrada que creamos en el propio moduloresource azurerm_resource_group mi_resource_group   name      pruebaterraform  location  West Europemodule webapp   source               .moduleswebapp  resource_group_name  azurerm_resource_group.mi_resource_group.name  location             azurerm_resource_group.mi_resource_group.location  tier                 Standard  size                 S1Si quisieramos ahora utilizar los valores de salida de este modulo los tendremos disponibles enmodule.nombre_modulo.nombre_variable_salidamodule.webapp.name  pruebaterraformwebComo bola extra un modulo de Terraform no tiene por que encontrarse en el sistema de ficheros de la maquina. En dependencia de como formemos el path de la propiedad source podremos ir aEl registro publico de Terraformmodule un_modulo   source   hashicorpmi_moduloazurerm  version  1.0.0Una URLmodule un_modulo   source  httpswww.ejemplo.commimodulo.zipO incluso un repositorio de gitmodule un_modulo   source  githttpswww.ejemplo.comrepositorio.gitmodulesmimodulomodule otro_modulo   source  gitsshusernameejemplo.comrepositorio.gitWorkspacesLa ultima caracteristica que vamos a comentar son los workspaces de Terraform. Generalmente suelo utilizar esta caracteristica como punto de division y gestion de diferentes entornos.Terraform crea un archivo con el estado actual de la infraestructura. Por defecto este archivo se llama terraform.tfstate. Como deciamos anteriormente es gracias a este archivo que podemos hacer incrementales al ejecutar un apply y en caso de tener una desincronizacion con el entorno real existente deberiamos actualizarlo usando un comando import.El problema viene cuando tenemos mas de un entorno que se distribuye con el mismo proyecto de Terraform. En este caso nuestro estado almacenado quedaria sobre escrito continuamente por los diferentes entornos terminando en errores sin solucion. Y aqui es donde un workspace nos va a resultar muy util.Un workspace es un nuevo espacio donde almacenamos un estado propio. El estado de un workspace esta aislado del estado del resto de workspaces. Y podemos tener todos los workspaces que necesitemosqueramos.Para crear un nuevo workspace bastara con ejecutar el siguiente comando terraform workspace new devAsi crearemos un nuevo espacio de trabajo llamado dev. A su vez veremos que se ha creado una carpeta llamada terraform.tfstate.d donde encontraremos una carpeta con el nombre dev. Aqui se almacenara el archivo de terraform.state de nuestro workspace.Si queremos saber cuales son los workspaces que tenemos ejecutaremos terraform workspace list  default devCon un asterisco se marcara el workspace que tenemos selecciona.Si queremos cambiar el seleccionado terraform workspace select devY por ultimo si queremos borrar un workspace ejecutaremos terraform workspace delete devSi quisieramos saber dentro del codigo de un .tf cual es el workspace actual usariamos la variable terraform.workspaceresource azurerm_resource_group mi_resource_group   name      pruebaterraformterraform.workspace  location  West EuropeConclusionesTerraform como plataforma de infraestructura como codigo es una herramienta muy competente. Hereda las ventajas de IaC y les saca un mayor partidoAutomatizacion de proceso de creado de infraestructura la nube nos permite prescindir de la interaccion fisica con el hardware. Herramientas como Terraform anaden la automatizacion a la nube.Poder crear la infraestructura facilmente nos permite generar infraestructura y actualizarla de una forma simple y rapida.La capacidad de replicar entornos de una forma sencilla si por ejemplo tenemos entornos de DEV PRE y PRO podemos generarlos como replicas y mantener las mismas caracteristicas en unos y otros.Versionar la infraestructura a la vez que se versiona el codigo cuando se anade una nueva caracteristica como por ejemplo una cache distribuida tendremos el codigo fuente asociado a la modificacion de la infraestructura necesaria.Independencia de un desarrollador para generar sus propios entornos de prueba.Descripcion clara de la infraestructura el formato hcl de Terraform hace sencillo leer y hacerse a la idea de que hay montado.Pero tambien hereda sus retos y anade otra serie de problemasLos limites de la plataforma los sigues teniendo. Y no se avisa de los mismos hasta la ejecucion de apply.Una desincronizacion de la infraestructura o un error en la ejecucion de un apply puede resultar en un estado de Terraform erroneo e irrecuperable.Las dependencias circulares un limite que hoy en dia que no tiene una solucion ni simple y ni mucho menos elegante.No tiene soporte para todas las caracteristicas y tipos de recursos de Azure.Es un verdadero reto mantener infraestructuras complejas que se creen y actualicen a partir de la misma version de archivos de Terraform.Terraform vs ARM vs Azure CLIPero vamos a lo que a todos interesa la pelea de gallos.Terraform vs ARM  El hcl de Terraform es mas legible sencillo y por tanto mantenible que el json de ARM  Los modulos de Terraform y sus diferentes fuentes dan mucha mas versatilidad que los nested templates de ARM  La validacion sintactica previa con el comando plan de Terraform  Soporta ejecucion de ciertas partes de infraestructura via plantillas ARM  Terraform vale para otras nubes diferentes a Azure Herramienta de comandos multi plataforma Mejores herramientas para edicion de ARM hoy en dia Puedes sacar plantillas ARM de recursos ya creados en Azure ARM da mas detalles en los mensajes de error de Azure Con ARM tienes todos los recursos que existen en AzureARM y Terraform son herramientas semejantes. Vienen a solucionar el problema de IaC. Uno de forma general y otro solo para Azure. Pero las ventajas de Terraform las encontramos en que todo es mucho mas sencillo y mantenible que en ARM. Y a una mala siempre puedes encapsular una plantilla ARM en un recurso de Terraform.Terraform vs Azure CLI  El hcl de Terraform esta muy bien preparado para los problemas de IaC mientras que los ShellScript o Batch no tienen ese objetivo en un principio  Cuando se crea o se esta modificando infraestructura con Terraform podemos verlo en el portal de Azure  Los modulos de Terraform no tienen nada que ver en comparacion con el uso de archivos .sh o .bat  La validacion sintactica previa con el comando plan de Terraform  El hcl es multi plataforma pero un .sh o un .bat no  Terraform vale para otras nubes diferentes a Azure  Si usas Makefiles para Azure CLI necesitas instalar otra herramienta mas La sintaxis es sencilla de entender tanto en comandos como en hcl Herramientas multi plataforma Ninguna de las dos plataformas soporta todos los recursos de Azure En ambos se pueden usar plantillas de ARM Las dependencias circulares son un verdadero problema de Terraform y muy sencillo de solventar usando Azure CLI A base de usar Makefiles Azure CLI se puede convertir en una herramienta super potente Se pude usar un modo Complete en el deploy de Azure CLI de forma que volveriamos a replicar un mismo entornoAzure CLI no es una herramienta de IaC aunque se puede usar de esta forma. En contra de lo que se pueda pensar por la sintaxis los comandos create que usa son idempotentes por lo tanto se comportan de una forma semejante a la incremental. Pero no deja de ser una herramienta de consola para la gestion general de Azure. Terraform es una herramienta pensada solo para gestionar infraestructura como codigo.OpinionTerraform es una herramienta muy potente que sirve para diferentes entornos con una sintaxis mas o menos sencilla y que funciona muy bien. Aunque no es la herramienta perfecta en mi opinion es hoy por hoy de lo mejorcito que tenemos disponible.Si no consideras que te pueda ser util es mejor que no uses esta herramienta. Pero si por el contrario crees que te puede ayudar preparate para enfrentarte a algunos retos que te pondran a prueba. Eso si al final te aseguro que no te arrepentiras."
    } ,
  
    {
      "title"    : "Introducción a Quatum Development (iii)",
      "category" : "",
      "tags"     : "quantum, programming",
      "url"      : "/2019/05/29/introduccion-quatum-development-iii",
      "date"     : "2019-05-29 08:01:23Z",
      "content"  : "Se que teneis miedo. Temeis los qubits. Temeis el cambio. Yo no conozco el futuro. No he venido para deciros como acabara todo esto. Al contrario he venido a deciros como va a comenzar. Voy a terminar de escribir este articulo. Voy a ensenarles a todos lo que vosotros no quereis que vean. Les ensenare un mundo sin vosotros. Un mundo sin bits y sin problemas de calor en los transistores sin limites ni fronteras. Un mundo donde todos los estados sean posibles. Lo que hagais despues es una decision que dejo en vuestras manos.En articulos anteriores i y ii ya introdujimos la computacion cuantica el Microsoft Quantum Development Kit los qubits de forma basica y algo mas avanzada ademas de varias operaciones y puertas logicas de este nuevo paradigma.Y como lo prometido es deuda hoy vamos a hablar de entrelazamiento y teleportacion cuanticosEntrelazamientoEl entrelazamiento cuantico es algo mas abstracto si cabe ya de explicar. Quiza lo mas sencillo es verlo con un ejemploImaginad que tenemos dos qubits representados de la siguiente formaa00   b01   c10   d11Donde a y d valen uno partido de la raiz cuadrada de dos y b y c valen 01sqrt200   001   010   1sqrt211  1sqrt200   11Esto quiere decir que solo tenemos dos estados posibles con un 50 de posibilidades de que ocurra 00 o 50 de 11 mientras que no es posible que sea 01 ni 10.Como ya vimos en otros articulos en el momento en el que realizo una medida de un qubit este colapsa a ese estado. Una consecuencia del principio de incertidumbre.Entonces si midieramos el valor del primer qubit y este diera como resultado 0 esto significaria que el segundo qubit colapsaria tambien pasando a valer 0.Podriamos decir que un estado de entrelazamiento es un estado en el que tener un valor en un qubit implica otro valor en otro qubit diferente.A estos estados de entrelazamiento completo se les conoce como estados de Bell. Y para este ejemplo en concreto se puede conseguir aplicando una formula con operaciones que ya hemos vistoEsta misma formula escrita en Q podria seroperation Entanglementq1 Qubit q2 Qubit Unit    Hq1    CNOTq1 q2Donde aplicamos una puerta H al primer qubit y acto seguido una puerta CNOT usando como objetivo el segundo qubit. El resultado de estas dos operaciones es un estado de entrelazamiento semejante al usado en el ejemplo anterior 50 00 y 50 11.Ahora podriamos escribir el codigo de ejemplo de ejecucion de la operacion de entrelazaroperation Setq Qubit value Result Unit    let current  Mq    if current  value            Xq    operation MakeEntanglementinitial Result Result Result    using qubits  Qubit2            Setqubits0 initial        Setqubits1 Zero        Entanglementqubits0 qubits1        let res0  Mqubits0        let res1  Mqubits1        ResetAllqubits        return res0 res1    Aqui simplemente asignamos un valor inicial a un par de qubits realizamos el entrelazado y los medimos.En C podriamos realizar una prueba con este codigousing var qsim  new QuantumSimulator    var initials  new Result  Result.Zero Result.One     foreach var initial in initials            var res  MakeEntanglement.Runqsim initial.Result        var res1 res2  res        Console.WriteLineres1 res2    Que al ejecutar podria tener una salida semejante a dotnet runZero ZeroOne OneAunque podria tener diferentes combinaciones por eso del 50 de posibilidades de uno y otro siempre que se repitan el valor primero y segundo por eso del entrelazamiento cuantico.TeleportacionLa teleportacion cuantica es algo parecido a lo que piensas pero no es exactamente como piensas.La idea es conseguir enviar el estado de un qubit a a otro b que no se encuentra en el mismo lugar.Para ello podemos usar el entrelazamiento cuantico como canal de comunicacionImaginemos que tenemos dos particulas entrelazadas como en el ejemplo anterior. Una la coges tu y la otra la cojo yo. Cada uno nos vamos a nuestra casa. En la comodidad del hogar decides mirar el valor de tu particula y en ese momento colapsas el valor de ambas. De forma que si la tuya vale 1 ya sabes que la mia vale 1.Ahora si tu copias el valor de otra particula no entrelazada con la mia en la que si que esta entrelazada entonces podriamos decir que has pasado el estado de una tercera particula hasta mi.La realidad detras de este experimento es que no se ha transmitido ninguna informacion. Esta informacion ya la teniamos tanto tu como yo. Y si yo llego a mirar primero el valor de mi particula hubiera obtenido el mismo resultado. Es lo que se conoce como la paradoja EPR algo que escapa totalmente al ambito de este articulo.Podriamos aclarar o complicar no lo tengo claro todo un poco mas usando un ejemplo con punteros de C   esta es mi particulaint mine  1 esta es la tuyaint yours  NULL y este el valor que quieres teleportarint other  0 nos entrelazamosyours  mine ahora cada uno nos iriamos a nuestra casa desde tu casa transformas el valor de tu particulayours  other el resultado final es que tanto tu particula como la mia son 0cout Realizar esta operacion en Q es algo mas complejo pero no mucho. Si echamos un vistazo a la pagina de la wikipedia sobre el tema nos propone unos pasosPuerta CNOTOperador de HadamardMedida y transmisionFinalmente se deberia aplicar un operador Z o X en el valor del qubit b en dependencia de los valores medidos para tener el valor originaloperation Teleport source  Qubit target  Qubit  Unit     creamos un canal de transmision    using channel  Qubit             lo entrelazamos con el qubit objetivo        Entanglementchannel target         puerta CNOT        CNOTsource channel         operador de Hadamard        Hsource         medida        let data1  Msource        let data2  Mchannel         transformacion        if data1  One  Ztarget         if data2  One  Xtarget          dejar los qubits a Zero        Resetchannel    Para llamar a la teleportacion usariamos la siguiente operacion de Qoperation MakeTeleportmessage  Result  Result    using source target  Qubit Qubit            Setsource message        Teleportsource target        let measurement  Mtarget        ResetAllsource target        return measurement    Donde creamos dos qubits. En el primero guardamos el valor del mensaje que queremos enviar. Realizamos la teleportacion y devolvemos el valor del destino.El codigo en C para probarlo seria algo como estousing var qsim  new QuantumSimulator    var initials  new Result  Result.Zero Result.One     foreach var initial in initials            var res  MakeTeleport.Runqsim initial.Result        Console.WriteLineinitial  res    Y la salida que obtendriamos al ejecutar vendria a confirmar que se ha transmitido la informacion dotnet runZero  ZeroOne  OneComo curiosidad unos cientificos chinos tienen el record de distancia de teleportacion cuantica llegando a 1200km de separacion entre dos particulas entrelazadas Ahi es nadaConclusionesCreo que con esta tercera parte he conseguido resarcirme del fiasco de la anterior. Aqui se aplican practicamente todos los conceptos que hemos ido explicando en los articulos anteriores y es cuando empezamos a ver dos capacidades muy potentes de la programacion cuantica.Por ejemplo el uso de estas tecnicas es fundamental en la criptografia cuantica un tipo de encriptacion de informacion mucho mas seguro de los usados hoy en dia. Y por extension seria muy interesante el uso de ambas tecnicas para conseguir una comunicacion muy rapida y segura.Eso si todo esto que comentamos hoy en dia seria muy dificil de aplicar en el mundo real...No te digo nada y te lo digo todo. Asi a la vez."
    } ,
  
    {
      "title"    : "Introducción a Quatum Development (ii)",
      "category" : "",
      "tags"     : "quantum, programming",
      "url"      : "/2019/05/23/introduccion-quatum-development-ii",
      "date"     : "2019-05-23 07:37:32Z",
      "content"  : "Te explicare por que estas aqui. Estas aqui porque sabes algo aunque lo que sabes no lo puedes explicar pero lo percibes. Ha sido asi durante toda tu vida. Algo no funciona en la programacion no sabes lo que es pero ahi esta como una astilla clavada en tu mente. Y te esta enloqueciendo. Esa sensacion te ha traido hasta aqui Sabes de lo que estoy hablando Te gustaria saber que es la programacion cuanticaQue me dirias si todo lo que estuvimos comentando en el anterior articulo fuera tan solo la superficie Y si fuera el equivalente a la tabla de multiplicar del 1 en el mundo de la resolucion de derivadasHoy vamos a profundizar mas en el tema de la programacion cuantica y para ello realizaremos ejemplos usando el Microsoft Quantum Development Kit. Asi que te recomendamos que lo instales si no lo has hecho ya.QubitEl otro dia vimos que un qubit puede ser representado como una superposicion de 0 y 10   1Pero lo que no dijimos es que tanto  como  podrian ser numeros complejos que en su forma exponencial estarian representados porrei0   rei1Al usar numeros complejos esto nos aporta una nueva dimension la parte real y otra imaginaria. Esto unido con la superposicion de 0 y 1 nos llevaria a poder representar un qubit en forma de una esfera de BlochDondeEn el eje Z tendriamos los valores 1 y 0.En el eje X marcariamos los estados positivo o negativo   o .En el eje Y encontrariamos la parte imaginaria i o i.Operacion MeasureSi quisieramos realizar una medida en cualquiera de estos ejes deberiamos recurrir a las matrices de PauliQue en Q seria algo como estooperation MeasureAll Result Result Result    using qubits  Qubit1            Hqubits0        let resZ  MeasurePauliZ qubits        let resY  MeasurePauliY qubits        let resX  MeasurePauliX qubits        ResetAllqubits        return resZ resY resX    Donde la operacion de H se usa para que no siempre tenga el valor Zero en la Z. Y la operacion Measure es igual que la M con la diferencia de que la operacion M realiza la medida con la matriz Z y Measure nos deja seleccionar la matriz a utilizar. Recordad dejar a Zero el estado de todos los qubits que usemos antes de dejar de utilizarlos. Para ello si es una matriz podemos usar la operacion ResetAll.De esta manera el codigo de driver.cs quedaria comostatic void Mainstring args    using var qsim  new QuantumSimulator            forvar i  0 i   1            var y  resY  Result.Zero  i  i            var x  resX  Result.Zero                 Console.WriteLineZ z 4 Y y 4 X x 4            Y al ejecutar este codigo nos deberia dar como resultado algo parecido a esto dotnet runZ 0  Y i  X  Z 1  Y i  X  Z 1  Y i  X Z 1  Y i  X  Z 1  Y i X Z 0  Y i  X  Z 0  Y i  X Z 0  Y i X Z 0  Y i  X Z 0  Y i  X  Sistema de varios QubitsEn la computacion tradicional usamos mas de un bit para definir estados mas complejos que 0 y 1. Por ejemplo si queremos definir el numero 3 usariamos dos bits con valor 111 binario  3 decimalDe la misma forma podemos actuar con los qubits. Podriamos tener dos qubits que nos definieran un solo estado de manera que esto formaria un nuevo sistema mas complejoa00   b01   c10   d11Dondea   b  c   d  1Si por ejemplo tuvieramos 3 qubits entonces tendriamos definidos los estados 000 001 010 011 100 101 110 y 111. Y asi sucesivamente.Cuantos mas qubits usemos en nuestro sistema mas estados contemplamos. Si por ejemplo tuvieramos un sistema de 10 qubits estariamos hablando de 1024 estados con 100 qubits nos encontrariamos en un orden de 1030 de combinaciones. Y se opera con todas ellas a la vez. Esto nos da una idea de la gran potencia que tiene la computacion cuantica.Mas Operaciones CuanticasEn el articulo anterior pudimos estudiar algunas operaciones o puertas logicas. En este vamos a extender ese listado con dos operaciones que solemos usar usando dos qubits en lugar de solo unoPuerta CNOTCuando hablamos de un sistema de dos qubits la operacion mas utilizada es un CNOT o Controlled NOT. Tiene dos qubits por parametro uno de control y otro el objetivo de la operacion NOT. Cuando el control es 1 realiza un NOT en el objetivo. Se representa con el simbolo CNOT ct  cctY los posibles resultados serianOrigenControlObjetivoResultado CNOT000000010101101011111110Si quisieramos poner en practica la operacion con Q tendriamos el siguiente codigooperation Setq Qubit value Result Unit    let current  Mq    if current  value            Xq    operation MakeCNOTcontrol Result target Result Result Result    using qubits  Qubit2            Setqubits0 control        Setqubits1 target        CNOTqubits0 qubits1        let resControl  Mqubits0        let resTarget  Mqubits1        ResetAllqubits        return resControl resTarget    Aqui primero tenemos una operacion que nos ayuda a asignar un valor a un qubit llamada Set. Despues tenemos la operacion que realiza el CNOT. Primero reservamos dos qubits despues le asignamos los valores que hemos decidido para control y para target. Pasamos la puerta logica CNOT y leemos los resultados.En C tendriamos la llamadausing var qsim  new QuantumSimulator    var initials  new Result  Result.Zero Result.One     foreachvar control in initials            foreachvar target in initials                    var res  MakeCNOT.Runqsim control target.Result            var resControl resTarget  res            var source  intcontrolinttarget            var result  intresControlintresTarget            Console.WriteLinesource  result            Y la salida de la ejecucion seria como la tabla que hemos descrito antes dotnet run00  0001  0110  1111  10Puerta SWAPEsta puerta intercambia los valores de dos qubits. Su funcionamiento es semejante a la realizacion deCNOTqubit1 qubit2CNOTqubit2 qubit1CNOTqubit1 qubit2OrigenFin0000011010011111Si en el codigo anterior sustituyeramos el CNOT por SWAPoperation MakeSWAPcontrol Result target Result Result Result    using qubits  Qubit2            Setqubits0 control        Setqubits1 target        SWAPqubits0 qubits1        let resControl  Mqubits0        let resTarget  Mqubits1        ResetAllqubits        return resControl resTarget    Entonces la salida seria dotnet run00  0001  1010  0111  11ConclusionesLas segundas partes nunca fueron buenas. Es verdad que este articulo puede parecer un poco de relleno. Es que lo es. Pero si no queria alargarlo demasiado tenia que parar de escribir en algun momento.Hemos profundizado mucho mas en que es un qubit y como operar con uno o varios de ellos. Tambien nos hemos empezado a dar cuenta de la potencia que tendria un ordenador de proposito general que manejara muchos qubits.Hasta aqui hemos tratado dos puntos claves de la programacion cuanticaEl principio de incertidumbre de Heisenberg es imposible realizar una medida sin que el sistema se vea afectado.La superposicion cuantica o de estados un sistema existe en todos sus posibles estados a la vez.Para el proximo post prometo algo mucho mas divertido como el entrelazado cuantico y su relacion con la teleportacion...No te digo nada y te lo digo todo. Asi a la vez."
    } ,
  
    {
      "title"    : "Introducción a Quatum Development (i)",
      "category" : "",
      "tags"     : "quantum, programming",
      "url"      : "/2019/05/10/introduccion-quatum-development",
      "date"     : "2019-05-10 07:51:23Z",
      "content"  : "El mundo de la programacion tal y como lo conocemos tiene fecha de caducidad. Por muchos materiales nuevos aleaciones especiales o mejoras tecnologicas que tengamos un microprocesador tiene un limite de tamano a partir del cual deja de ser eficiente. Cada dia estamos mas cerca de ese limite y es aqui donde aparece la computacion cuantica. Preparate para cambiar de paradigma para cambiar bits por qubits.O en realidad no solo queria ponerme apocaliptico y algo dramatico para hacer mas llamativa la entradilla...Lo que si que es cierto es que en un microprocesador muy muy pequeno infimo los electrones que transporta al ser particulas cuanticas podrian saltarse la barrera fisica que supone un transistor y pasar a otro por eso del efecto tunel. Acontecimiento que desencadenaria un malfuncionamiento del micro.Esto que para muchos podria ser un problema para el senor Paul Benioff fue una oportunidad. Y es donde encontramos el origen de la computacion cuantica. Un paradigma que viene a intentar aprovechar las leyes cuanticas en beneficio de la computacion.La primera premisa de este paradigma es que en lugar de usar como unidad de proceso un bit que puede tener valores 0 o 1 usar un qubit cubit o bit cuantico que podria tener el valor 0 el 1 o el 0 y 1 a la vez.No os suena un poco a lo del gato de SchrodingerA la hora de la verdad lo que significa es que en una sola ejecucion de un programa tendriamos ambos estados.Un ejemplo simplista que podria ayudarnos a entender la potencia de la computacion cuantica si tuvieramos una operacion en la que se realiza una suma o una resta en dependencia de un valor bool tendriamos que ejecutar el programa dos veces una para obtener la suma y otra para la resta. Sin embargo si usaramos un Qubit en una sola ejecucion tendriamos ambos resultados.QubitUn qubit puede representarse matematicamente en notacion braket como una superposicion de un ket unitario 1 y un ket cero 00   1donde     1Podriamos determinar que este estado superpuesto indica que tiene  de probabilidad de ser 0 y   de ser 1.Ademas un ket puede representarse como una matriz de dos filas y una columna0  1 0      1  0 10   0    1  0 Operaciones CuanticasLejos quedan AND OR XOR y demas puertas logicas que tan buen resultado dan trabajando con bits. Para tratar con qubits existen otra serie de operaciones. Y aunque existen mas de las que vamos a citar para este articulo lo mejor sera comenzar con las basicas.MLa Medida o Measure es la operacion que se encarga de medir un qubit y darnos su valor 0 o 1.Si por ejemplo tenemos un qubit con una distribucion al 50 del estado 0 o 11sqrt2  0   1sqrt2  1Como hemos visto antes la probabilidad de que sea 0 o 1 vendria determinada por el cuadrado de 1sqrt2 que es 12 o 0.5.Al realizar una medida inmediatamente el qubit colapsa su estado quedando fijado en 0 o 1. Ademas no se puede deshacer esta operacion para recuperar los factores de probabilidad. De esta forma quedaria como10   01     o       00   11Se pueden realizar medidas para las diferentes bases de Pauli acerca de las matrices de Pauli. Aunque esos detalles quedarian fuera de esta introduccion...XLa puerta logica X es el equivalente a un NOT de programacion convencional 0 lo convierte en 1 y viceversa.Se representa como la matrizX  0 1 1  0Y al operar seria de la siguiente formaX1  0 1 1  0  0 1  00   11 10   01  1 0  0ZEsta operacion deja el valor 0 como 0 pero convierte 1 en 1. Se puede representar como la siguiente matrizZ  1 0 0  1HLa puerta Hadamard o puerta H superpone los valores 1 y 0. Se representa comoH  1sqrt2  1 1 1  1Que al operar daria como resultado0  0   1sqrt2  1  0  1sqrt2Microsoft Quantum Development KitPara poder aplicar lo que hemos aprendido sobre programacion cuantica Microsoft nos brinda Quantum Development Kit que liberara en breve como open source.Este conjunto de herramienta se basa en el lenguaje Q como base para programar algoritmos cuanticos.Estamos a un solo paso de empezar a programar. Para ello solo necesitamos tener instalado el dotnet core SDK e introducir dos comandos en el terminaldotnet tool install g Microsoft.Quantum.IQSharpdotnet new i Microsoft.Quantum.ProjectTemplatesUna vez hecho esto para crear un nuevo proyecto en Q introduciremos el siguiente comandodotnet new console lang QEntonces para comprobar que todo funciona correctamente introduciremos el comando run y veremos el HelloWorld dotnet runHello quantum worldEl proyecto esta formado por 3 archivos.csproj es el archivo del proyecto.Driver.cs es el programa en C que lanza el simulador de Q.Operations.qs es el programa cuantico en Q que ejecuta nuestro proyecto.QPara aprender mejor este nuevo lenguaje os recomiendo empezar a leer la documentacion oficial y por supuesto la referencia de sus librerias.No obstante es un lenguaje bastante facil de entender a simple vista. Si echamos un vistazo a Operations.qs veremosnamespace QSharp    open Microsoft.Quantum.Canon    open Microsoft.Quantum.Intrinsic    operation HelloQ Unit         MessageHello quantum world    Es una notacion parecida a C o TypeScript donde podemos ver que los imports son open y una static function se declara como operation.Si quisieramos aplicar lo que hemos aprendido hasta ahora podria crear un pequeno interruptornamespace qsharp    open Microsoft.Quantum.Intrinsic    operation Switchdesired Result q1 Qubit Unit            let current  Mq1        if desired  current                    Xq1            En este codigo vamos a hacer que nuestro Qubit cambie al valor de la variable desired que puede ser Zero o One.Anadimos otra operacion para saber que valor tenemos actualmente despues de hacer el Switchoperation SwitchAndReturndesired Result q1 Qubit Result    Switchdesired q1    let res  Mq1    return resAqui lo que hacemos es el cambio de estado y devolvemos en que estado ha quedado. Y esta operacion la llamaremos dentro de nuestra pruebaoperation TestSwitchcount Int initial Result Int Int    mutable numOnes  0    using qubits  Qubit1            for test in 1..count                    let res  SwitchAndReturninitial qubits0            if res  One                            set numOnes  numOnes   1                            SwitchZero qubits0        return count  numOnes numOnesPartiremos de un numero de iteraciones llamado count y de un estado de inicio llamado initial. Crearemos un Qubit y en cada iteracion modificaremos su valor al initial y comprobaremos si tiene el valor One para contar sus apariciones.Cuando terminemos las operaciones y antes de dejar de usar los qubits que necesitemos hay que dejarlos en estado Zero si no nos aparecera el error deMicrosoft.Quantum.Simulation.Simulators.Exceptions.ReleasedQubitsAreNotInZeroState Released qubits are not in zero state.Una vez terminadas las operaciones devolvemos los Zeros y Ones que encontramos.Con el fin de poder ejecutar este nuevo codigo de Q tendremos que modificar el archivo Driver.csstatic void Mainstring args    using var qsim  new QuantumSimulator            var initials  new Result  Result.Zero Result.One         foreach var initial in initials                    var res  TestSwitch.Runqsim 1000 initial.Result            var numZeros numOnes  res            Console.WriteLineinitial 4 Zeros numZeros 4 Ones numOnes 4            Que escribira en consola el numero total de Zeros y Ones encontrados para cada valor inicial que le pasemos.Al hacer un run deberiamos obtener la siguiente salida dotnet runZero Zeros 1000 Ones 0One  Zeros 0    Ones 1000Ahora vamos a jugar con la negacion vamos a realizar una modificacion en la operacion de SwitchAndReturn anadiendo la negacion X del qubit actualoperation SwitchAndReturndesired Result q1 Qubit Result    Switchdesired q1    Xq1    let res  Mq1    return resDe forma obtendremos el resultado contrario a la anterior ejecucion. Volvemos a ejecutar para comprobarlo dotnet runZero Zeros 0    Ones 1000One  Zeros 1000 Ones 0Por ultimo podriamos aplicar la puerta logica H en lugar de la Xoperation SwitchAndReturndesired Result q1 Qubit Result    Switchdesired q1    Hq1    let res  Mq1    return resY comprobar unos resultados diferentes en cada ejecucion y mas o menos distribuidos cerca de al 50 dotnet runZero Zeros 502  Ones 498One  Zeros 496  Ones 504ConclusionesLa computacion cuantica esta en un estado poco maduro. Las grandes empresas de hardware estan dedicando mucho dinero en crear ordenadores fisicos que no solo simulen si no que sean verdaderamente sistemas cuanticos.No esta llamada a sustituir la computacion actual pero si a extenderla. Hoy en dia podria resolver cierta clase de algoritmos concretos de una forma mas eficiente y rapida.Lo que si que podemos intuir es que sera importante en el futuro y aunque no le veas sentido creo que es divertido e interesante el irnos acostumbrando a este nuevo paradigma.No te digo nada y te lo digo todo. Asi a la vez."
    } ,
  
    {
      "title"    : "Azure Functions: custom triggers",
      "category" : "",
      "tags"     : "azure-functions, azure, functions, csharp, triggers",
      "url"      : "/2019/05/02/azure-functions-custom-triggers",
      "date"     : "2019-05-02 07:59:43Z",
      "content"  : "Si ya se que soy un vago. Es el tercer articulo mas o menos igual que escribo. Este es sobre como crear triggers personalizados de Azure Functions. Y antes ya tuvimos los de los bindings de out y de in. Teneis que entender que tengo otra vida mas alla de este blog. Ademas creo que al final queda mucho mejor separado. De cualquier forma si no os gusta asi hago un cuarto articulo juntando el contenido de todos .Hoy vamos a tratar los custom triggers para Azure Functions. Y como ejemplo tomaremos el sistema PubSub que viene incluido en redis. Este sistema nos permite crear suscripciones a un canal donde recibiremos los mensajes que sean publicados en el.El funcionamiento es bastante simple si tenemos dos consolas de redis abiertas bastara con que en la primera nos suscribamos al canal test y quedemos a espera SUBSCRIBE testReading messages... press ENTER to quitY en la otra tendremos que publicar un mensaje PUBLISH test ola k aseinteger 1Entonces veremos que en la primera consola escribe la informacion que le ha llegado sobre dicho mensaje1 subscribe2 test3 integer 11 message2 test3 ola k asePara programar esto en dotnet core podriamos recurrir a una libreria llamada StackExchange.Redis y tener un codigo semejante a estevar connection  ConnectionMultiplexer.ConnectredisConnectionStringvar subscriber  connection.GetSubscriberawait subscriber.SubscribeAsyncchannel channel value   Console.WriteLineMessage arrive value A partir de ahora y para variar vamos a convertir esto en un custom trigger que desencadene la ejecucion de una Azure FunctionProyecto del triggerCon el fin de desarrollar nuestro custom trigger vamos a crear un nuevo proyecto de tipo libreria de .net standard 2.0. A este proyecto le anadiremos referencias a los siguientes paquetes de NuGetMicrosoft.Azure.WebJobs.Extensions es donde encontramos los artefactos necesarios para crear nuestra extension.StackExchange.Redis este paquete contiene una implementacion cliente de redis.Despues definiremos nuestro atributo de trigger. Ese que podemos ver en los parametros de la Azure Function entre corchetes. Asi que buscaremos un nombre que defina correctamente lo que queremos hacer y le anadiremos AttributeAttributeUsageAttributeTargets.ParameterBindingpublic class RedisSubTriggerAttribute  Attribute    public string Connection  get set     public string Channel  get set Los parametros que pediremos seranConnection donde almacenaremos la cadena de conexion a redis.Channel el nombre del canal al que nos vamos a suscribir.Para facilitar la captura del valor de la cadena de conexion desde los AppSettings o las variables de entorno anadiremos una funcion que nos ayude con la tareainternal string GetConnectionString    return Environment.GetEnvironmentVariableConnectionAhora que tenemos definido nuestro trigger si queremos que desencadene una Azure Function deberiamos definir un artefacto que implemente la interfaz generica IListenerpublic class RedisSubListener  IListener    public Task StartAsyncCancellationToken cancellationToken            public Task StopAsyncCancellationToken cancellationToken            public void Cancel            public void Dispose        Esta implementacion nos permitira definir la operacion para empezar a escuchar mensajes la de parar de escuchar mensajes la de cancelar la escucha de mensajes y por ultimo Dispose donde limpiaremos los recursos que hemos usado. Para escuchar mensajes de redis podriamos tener algo parecido a estopublic class RedisSubListener  IListener    private readonly ITriggeredFunctionExecutor _executor    private readonly RedisSubTriggerAttribute _attribute    private readonly ConnectionMultiplexer _connection    private ISubscriber _subscriber  null    public RedisSubListenerITriggeredFunctionExecutor executor RedisSubTriggerAttribute attribute            _executor  executor        _attribute  attribute        _connection  ConnectionMultiplexer.Connectattribute.GetConnectionString        public Task StartAsyncCancellationToken cancellationToken            if _subscriber  null throw new InvalidOperationExceptionRedis PubSub listener has alread been started        _subscriber  _connection.GetSubscriber        return _subscriber.SubscribeAsync_attribute.Channel OnMessageArrived        public async Task StopAsyncCancellationToken cancellationToken            if _subscriber  null throw new InvalidOperationExceptionRedis PubSub listener has already been stopped        await _subscriber.UnsubscribeAllAsync        _subscriber  null        public void Cancel            if _subscriber  null return        _subscriber.UnsubscribeAll        _subscriber  null        public void Dispose            _connection.Dispose        private void OnMessageArrivedRedisChannel channel RedisValue value            var triggerData  new TriggeredFunctionData                    TriggerValue  value.ToString                var task  _executor.TryExecuteAsynctriggerData CancellationToken.None        task.Wait    Si observamos el codigo lo que hacemos es crear y gestionar una suscripcion a un canal de redis y cada vez que llegue un mensaje a esta suscripcion le indicamos a un objeto ITriggeredFunctionExecutor que ejecute el mensaje.Con el objetivo de crear nuestro IListener y a la vez tratar la informacion que hemos lanzado tendremos que crear un artefacto que implemente la interfaz ITriggerBindingpublic class RedisSubTriggerBinding  ITriggerBinding    public Type TriggerValueType  throw new NotImplementedException    public IReadOnlyDictionary BindingDataContract  throw new NotImplementedException    public Task BindAsyncobject value ValueBindingContext context            public Task CreateListenerAsyncListenerFactoryContext context            public ParameterDescriptor ToParameterDescriptor        Lo primero que haremos sera definir la creacion del escuchador de eventos para lo que necesitaremos el atributo del parametro de la funcion y el objeto ITriggeredFunctionExecutorprivate readonly ParameterInfo _parameterpublic RedisSubTriggerBindingParameterInfo parameter    _parameter  parameterpublic Task CreateListenerAsyncListenerFactoryContext context    var executor  context.Executor    var attribute  _parameter.GetCustomAttribute    var listener  new RedisSubListenerexecutor attribute    return Task.FromResultlistenerY con el fin de bindar el trigger al parametro que le pasamos implementaremos el resto de propiedades y metodospublic Type TriggerValueType  typeofstringpublic IReadOnlyDictionary BindingDataContract  new Dictionarypublic ParameterDescriptor ToParameterDescriptor    return new TriggerParameterDescriptor            Name  _parameter.Name        DisplayHints  new ParameterDisplayHints                    Prompt  RedisSub            Description  RedisSub message trigger            public Task BindAsyncobject value ValueBindingContext context    var valueProvider  new RedisSubValueBindervalue    var bindingData  new Dictionary    var triggerData  new TriggerDatavalueProvider bindingData    return Task.FromResulttriggerDataComo podemos observar hemos creado un nuevo artefacto llamado RedisSubValueBinder. Este objeto implementa la interfaz IValueBinder que es necesaria para definir el TriggerData que se usara para bindar los datos al parametro. En nuestro caso estamos gestionando mensajes como cadenas de texto por lo que una implementacion simple nos bastarapublic class RedisSubValueBinder  IValueBinder    private object _value    public RedisSubValueBinderobject value            _value  value        public Type Type  typeofstring    public Task GetValueAsync            return Task.FromResult_value        public Task SetValueAsyncobject value CancellationToken cancellationToken            _value  value        return Task.CompletedTask        public string ToInvokeString            return _value.ToString    Con estos dos artefactos ya tendriamos definido todo el comportamiento de nuestro custom trigger que ejecutara una Azure Function al recibir mensajes de un canal de PubSub de redis. Lo que nos falta es hacerle saber al sistema que vamos a extender el comportamiento de las Azure Functions.A este fin vamos a crear un nuevo artefacto que implemente ITriggerBindingProvider. Este objeto buscara los parametros de nuestras Azure Functions que tienen el atributo de tipo trigger que creamos al principio. Y cuando lo encuentre instanciara un nuevo RedisSubTriggerBinding para ese parametropublic class RedisSubTriggerBindingProvider  ITriggerBindingProvider    public Task TryCreateAsyncTriggerBindingProviderContext context            var parameter  context.Parameter        var attribute  parameter.GetCustomAttributefalse        if attribute  null return Task.FromResultnull        if parameter.ParameterType  typeofstring throw new InvalidOperationExceptionInvalid parameter type        var triggerBinding  new RedisSubTriggerBindingparameter        return Task.FromResulttriggerBinding    Y por ultimo implementaremos la interfaz IExtensionConfigProvider para crear una nueva regla que utilice el objeto RedisSubTriggerBindingProvider para resolver los atributos de tipo RedisSubTriggerAttributepublic class RedisSubExtensionConfigProvider  IExtensionConfigProvider    public void InitializeExtensionConfigContext context            var rule  context.AddBindingRule        rule.BindToTriggernew RedisSubTriggerBindingProvider    Para terminar y que nuestro ensamblado registre por defecto el proveedor de configuracion que hemos creado tendremos que declarar una clase tipo Startup donde al arrancar el host de Azure Functions anadiremos nuestra extensionusing Microsoft.Azure.WebJobsusing Microsoft.Azure.WebJobs.Hostingassembly WebJobsStartuptypeofCustomBindings.Startupnamespace CustomBindings    public class Startup  IWebJobsStartup            public void ConfigureIWebJobsBuilder builder                    builder.AddExtension            Probando nuestro triggerSi queremos ver que todo ha funcionado correctamente podemos crear un nuevo proyecto de Azure Functions. Alli crearemos una nueva funcion que no devuelva nada pero que sea lanzada por el trigger que hemos creadoFunctionNameTestRedisSubpublic static void TestRedisSub    RedisSubTriggerConnection  RedisConnectionString Channel  test string message    ILogger log    log.LogInformationC Redis pubsub trigger function processed a request.    log.LogInformationMessage arrived messagePara finalizar dentro del archivo local.settings.json y en la propiedad Values anadiremos la cadena de conexion del servidor de redisValues     AzureWebJobsStorage     RedisConnectionString xxxxx.redis.cache.windows.net6380passwordxxxxxsslTrue    FUNCTIONS_WORKER_RUNTIME dotnet  Ahora podemos ejecutar el proyecto. Despues abrimos una consola de redis y publicamos un mensaje PUBLISH test ola k aseinteger 1En el terminal donde se ven los logs de las Azure Functions encontraremos que el mensaje ha sido procesado correctamenteC Redis pubsub trigger function processed a request.Message arrived ola k aseConclusionesCuidado con los ciclos de vida de los triggers.Y lo demas pues lo mismo de los otros dias con los bindings in y out. Pero en este caso es un poco mas complejo.Puedes ver el proyecto completo que hemos utilizado en este articulo en este repositorio de Github."
    } ,
  
    {
      "title"    : "Azure Functions: custom in bindings",
      "category" : "",
      "tags"     : "azure-functions, azure, functions, csharp, bindings",
      "url"      : "/2019/04/23/azure-functions-custom-in-bindings",
      "date"     : "2019-04-23 07:44:11Z",
      "content"  : "El otro dia veiamos como crear custom out bindings para Azure Functions asi que hoy me he visto obligado a tratar los custom in bindings o de entrada. Para ello utilizaremos de ejemplo la creacion de un binding que lea un valor secret del servicio Azure Key Vault.Una de las formas que tenemos de acceder a una cuenta de Key Vault desde dotnet core es utilizando una cuenta de acceso tipo Service Principal o App Registration en el portal de azure de Azure Active Directory. De esta forma tendremos dos parametros necesarios para recoger un token de acceso un client_id y client_secret. Entonces solo tendriamos que utilizar la clase AuthenticationContext que encontraremos en el paquete de NuGet Microsoft.IdentityModel.Clients.ActiveDirectory para realizar la peticionvar keyVaultClient  new KeyVaultClientasync authority resource scope             var authContext  new AuthenticationContextauthority        var clientCredential  new ClientCredentialclient_id client_secret        var result  await authContext.AcquireTokenAsyncresource clientCredential.ConfigureAwaitfalse        return result.AccessToken    Una vez tenemos instanciado nuestro objeto de tipo KeyVaultClient solo tendremos que realizar una llama a una funcion que realizara toda la magia por nosotrosvar secret  await keyVaultClient.GetSecretAsyncinput.SecretIdentifier cancellationTokenConsole.WrilteLinesecret.ValueProyecto de bindingCon el fin de desarrollar nuestro custom binding vamos crear un nuevo proyecto de tipo libreria de .net standard 2.0. A este proyecto le anadiremos referencias a los siguientes paquetes de NuGetMicrosoft.Azure.WebJobs.Extensions es donde encontramos los artefactos necesarios para crear nuestra extension.Microsoft.Azure.KeyVault es el paquete que contiene el cliente de Azure Key Vault.Microsoft.IdentityModel.Clients.ActiveDirectory aqui encontraremos las clases necesarias para obtener un token de Azure Active Directory.Despues definiremos nuestro atributo de binding. Ese que podemos ver en los parametros de la Azure Function entre corchetes. Asi que buscaremos un nombre que defina correctamente lo que queremos hacer y le anadiremos AttributeAttributeUsageAttributeTargets.ParameterBindingpublic class KeyVaultSecretAttribute  Attribute    public KeyVaultSecretAttribute            public KeyVaultSecretAttributestring key            SecretIdentifier  key        public string ClientId  get set     public string ClientSecret  get set     public string SecretIdentifier  get set Con estos parametros tendriamos todo lo necesario para realizar una peticion de un secreto. Ademas hemos delimitado el uso de este atributo a parametros de una funcion. Y con el decorador Binding indicamos que es un atributo que sirve para bindar valores usando el SDK de Azure WebJobs.Una forma de anadir facilidades a esta clase seria crear una propiedad Connection decorada con el atributo AppSetting que indica que es un parametro que se recoge de las variables de entorno o del archivo local.settings.json. La idea es que esta propiedad contenga una cadena de conexion con el formato client_idxxxxclient_secretyyyyyAppSettingpublic string Connection  get set Y podriamos anadirle una serie de metodos para convertir la cadena de conxion en los valores que esperamos y para validarlosinternal void Validate    Autofill    if string.IsNullOrEmptyClientId  string.IsNullOrEmptyClientSecret            throw new ArgumentExceptionYou should specify client_id and client_secret KeyVaultSecret binding parameters.        if string.IsNullOrEmptySecretIdentifier            throw new ArgumentExceptionYou should specify secret identifier KeyVaultSecret binding parameters.    private void Autofill    if string.IsNullOrEmptyConnection return    var values  Connection.Split.ToDictionaryx  x.Split0.Trim.ToLowerInvariant x  x.Split1.Trim    if values.ContainsKeyclient_id            ClientId  valuesclient_id        if values.ContainsKeyclient_secret            ClientSecret  valuesclient_secret    Ahora que tenemos definido nuestro binding si queremos usarlo como parametro de entrada deberiamos definir un artefacto que implmente la interfaz generica IConverterTS o para metodos asincronos IAsyncConverterTS. DondeT es el tipo del atributo que usamos para marcar el binding. En este caso KeyVaultSecretAttribute.Ses el tipo de objeto que va a devolder. En Azure Key Vault un secreto se almacena en formato de cadena de texto por lo que sera un string.public class KeyVaultSecretAsyncConverter  IAsyncConverter    public async Task ConvertAsyncKeyVaultSecretAttribute input        La implementacion basada en el codigo que indicamos al inicio de este documento seriapublic async Task ConvertAsyncKeyVaultSecretAttribute input CancellationToken cancellationToken    input.Validate    using var client  CreateClientinput.ClientId input.ClientSecret            var secret  await client.GetSecretAsyncinput.SecretIdentifier cancellationToken        return secret.Value    private static KeyVaultClient CreateClientstring clientId string clientSecret    return new KeyVaultClientasync authority resource scope             var authContext  new AuthenticationContextauthority        var clientCredential  new ClientCredentialclientId clientSecret        var result  await authContext.AcquireTokenAsyncresource clientCredential.ConfigureAwaitfalse        if result  null                    throw new InvalidOperationExceptionFailed to obtain the JWT token                return result.AccessToken    Donde validariamos la informacion que almacena nuestro binding y acto seguido realizariamos la peticion al cliente obteniendo asi el secreto.Con estos dos artefactos ya tendriamos definido todo el comportamiento de nuestro custom binding que leera valores de una cuenta de Azure Key Vault. Lo que nos falta es hacerle saber al sistema que vamos a extender el comportamiento de las Azure Functions para lo que tendremos que crear un nuevo artefacto que implemente IExtensionConfigProviderpublic class KeyVaultExtensionConfigProvider  IExtensionConfigProvider    public void InitializeExtensionConfigContext context            var rule  context.AddBindingRule        rule.AddValidatorValidateKeyVaultSecretAttribute        rule.BindToInputnew KeyVaultSecretAsyncConverter        private static void ValidateKeyVaultSecretAttributeKeyVaultSecretAttribute attribute Type parameterType            attribute.Validate    En este proveedor de configuracion indicaremos que vamos a crear un nuevo binding usando el atributo KeyVaultSecretAttribute. A esta nueva regla le anadiremos una validacion que comprobara que los datos del atributo son correctos. Y finalmente le indicaremos que es un binding de tipo entrada con la funcion BindToInput.Cuando indicamos que es un binding de entrada de datos hemos visto que tenemos la variante de devolver una instancia de un objeto de tipo IConvertero IAsyncConverter pero en realidad tambien admite una funcion. A mi personalmente me parece mas elegante definir un converter pero cuando estamos hablando de poca logica es posible que con una simple lambda quede resuelto.Para terminar y que nuestro ensamblado registre por defecto el proveedor de configuracion que hemos creado tendremos que declarar una clase tipo Startup donde al arrancar el host de Azure Functions anadiremos nuestra extensionusing Microsoft.Azure.WebJobsusing Microsoft.Azure.WebJobs.Hostingassembly WebJobsStartuptypeofCustomBindings.Startupnamespace CustomBindings    public class Startup  IWebJobsStartup            public void ConfigureIWebJobsBuilder builder                    builder.AddExtension            Probando nuestro bindingSi queremos ver que todo ha funcionado correctamente podemos crear un nuevo proyecto de Azure Functions. Crearemos una funcion que responda a una peticion HTTP y cuyo nivel de acceso sea AnonymousFunctionNameTestKeyVaultSecretpublic static IActionResult TestKeyVaultSecret    HttpTriggerAuthorizationLevel.Anonymous get Route  null HttpRequest req    ILogger logAhora anadiremos un parametro de tipo string y lo decoraremos con el atributo KeyVaultSecret. Para que esto funcione especificaremos la URL que identifica nuestro secreto e indicaremos que la conexion que va a usar esta en el AppSetting con nombre KeyVaultConnectionStringFunctionNameTestKeyVaultSecretpublic static IActionResult TestKeyVaultSecret    HttpTriggerAuthorizationLevel.Anonymous get Route  null HttpRequest req    KeyVaultSecrethttpsmy.vault.azure.netsecretsMyKey Connection  KeyVaultConnectionString string secret    ILogger log    log.LogInformationC HTTP trigger function processed a request.    return new OkObjectResultYour secret secretPara finalizar dentro del archivo local.settings.json y en la propiedad Values anadiremos la cadena de conexion con los datos del Service Principal al que dimos accesoValues     AzureWebJobsStorage     KeyVaultConnectionString client_idxxxxclient_secretyyyyy    FUNCTIONS_WORKER_RUNTIME dotnet  Ahora podemos ejecutar el proyecto llamar a la funcion con nuestro browser preferido y ver como imprime por pantalla el valor almacenado en un secreto de una cuenta de Azure Key Vault.ConclusionesPues lo mismo del otro dia pero con in en lugar de out.Puedes ver el proyecto completo que hemos utilizado en este articulo en este repositorio de Github."
    } ,
  
    {
      "title"    : "Azure Functions: custom out bindings",
      "category" : "",
      "tags"     : "azure-functions, azure, functions, csharp, bindings",
      "url"      : "/2019/04/15/azure-functions-custom-out-bindings",
      "date"     : "2019-04-15 07:59:45Z",
      "content"  : "La gran ventaja en de Azure Functions frente a otra tecnologia es que escribes muy poco codigo ya que solo debes gestionar como fluyen los datos usando unos bindings de entrada y salida. El propio SDK nos aporta un buen conjunto por defecto de bindings que nos ayudaran a tratar con peticiones HTTP Azure Storage Account Blob Queue y Table Service Bus o Cosmos DB. Pero y si quiero integrarme con otro sistema no soportadoAfortunadamente en la v2 de Azure Functions basadas en dotnet core podemos encontrar un sistema de extension en el que podremos crearnos todo tipo de bindings y triggers personalizados.En este articulo vamos a crear un out binding que nos ayudara a realizar un simple envio de emails basandonos en una conexion SMTP.Generalmente cuando queramos enviar un email usando el protocolo SMTP desde dotnet core usaremos un codigo muy semejante a estevar smtp  new SmtpClienthost portsmtp.EnableSsl  useSslsmtp.Credentials  new NetworkCredentialuser passwordvar fromAddress  new MailAddressfromvar toAddress  new MailAddresstovar message  new MMessagefromAddress toAddressmessage.Subject  subjectmessage.Body  bodyawait smtp.SendMailAsyncmessageLo que vamos a hacer es crear un nuevo proyecto con un binding que nos facilitara esta tarea. Para ello crearemos una nueva solucion de tipo libreria de .net standard 2.0 y anadiremos una referencia al paquete de NuGet Microsoft.Azure.WebJobs.Extensions que es donde encontramos todo lo necesario para crear nuestra extension.Despues definiremos nuestro atributo de binding. Ese que podemos ver en los parametros de la Azure Function entre corchetes. Asi que buscaremos un nombre que defina correctamente lo que queremos hacer y le anadiremos AttributeAttributeUsageAttributeTargets.ReturnValue  AttributeTargets.ParameterBindingpublic class MailSendAttribute  Attribute    public string Host  get set     public int Port  get set     public string User  get set     public string Password  get set     public bool UseSsl  get set Le hemos anadido ciertas caracteristicas para delimitar el uso de este atributo solo como valor devuelto por una funcion o como parametro de esta. Y hemos anadido una serie de propiedades que nos ayudan a definir una conexion SMTP.Pero quiza encontremos demasiado tedioso ir definiendo propiedad por propiedad y nos puede resultar mas sencillo usar un AppSetting que contenga una especie de cadena de conexion a nuestro servidor. Para ello vamos a anadir una propiedad llamada Connection y la vamos a decorar con el atributo AppSetting para que el sistema interprete que este valor lo debe leer de las variables de entorno o del archivo local.settings.jsonAppSettingpublic string Connection  get set Y para rellenar todas las propiedades de MailSendAttribute vamos a anadir una funcion que nos ayude a transformar esa cadena de conexioninternal void Autofill    if string.IsNullOrEmptyConnection return    var values  Connection.Split.ToDictionaryx  x.Split0.Trim.ToLowerInvariant x  x.Split1.Trim    if values.ContainsKeyhost            Host  valueshost        if values.ContainsKeyport  int.TryParsevaluesport out int port            Port  port        if values.ContainsKeyuser            User  valuesuser        if values.ContainsKeypassword            Password  valuespassword        if values.ContainsKeyusessl  bool.TryParsevaluesusessl out bool useSsl            UseSsl  useSsl        if string.IsNullOrEmptyHost  Port Para definir una cadena de conexion valida para tiempo de desarrollo iremos al archivo local.settings.json y en la propiedad Values anadiremosValues     SmtpConnectionString Hostsmtp.server.comPort587UsermyUserPasswordmyPasswordUseSsltrue  Ya tenemos definido nuestro atributo de binding ahora definiremos el tipo de objeto que vamos a manejar. En este caso sera un email para lo que crearemos un nuevo objeto serializable a JSON con los datos necesariospublic class MailMessage    JsonPropertyfrom    public string From  get set     JsonPropertyto    public string To  get set     JsonPropertysubject    public string Subject  get set     JsonPropertybody    public string Body  get set Ya tenemos un atributo y el objeto que va a utilizar nuestro binding ahora tendremos que definir el recolector el manejador o como queramos llamar al artefacto que va a gestionar la direccion out del binding. Esta clase debe implementar la interfaz generica IAsyncCollectorT donde T es el objeto manejado. Para este caso MailMessagepublic class MailAsyncCollector  IAsyncCollector    public Task AddAsyncMailMessage item CancellationToken cancellationToken  default            public Task FlushAsyncCancellationToken cancellationToken  default        Por cuestiones de simplicidad en lugar de usar nuestro artefacto IAsyncCollector como un acumulador vamos a hacer que envie directamente los emails con forme los tenga. Aunque esta no es la implementacion recomendada si que la podremos probar y comprobar que funciona correctamente.para ello anadiremos un constructor en el que le pasaremos el atributo de binding. De este objeto de tipo MailSendAttribute vamos a recoger los parametros de conexion con el servidor SMTP y del objeto MailMessage sacaremos los datos necesarios para enviar el email por el servidor ya definidoprivate readonly MailSendAttribute _bindingpublic MailAsyncCollectorMailSendAttribute binding    _binding  binding    _binding.Autofillpublic async Task AddAsyncMailMessage item CancellationToken cancellationToken  default    using var smtp  CreateSmtpClient_binding            var message  CreateMailMessageitem        await smtp.SendMailAsyncmessage    public Task FlushAsyncCancellationToken cancellationToken  default    return Task.CompletedTaskprivate static SmtpClient CreateSmtpClientMailSendAttribute binding    var smtp  new SmtpClientbinding.Host binding.Port    smtp.EnableSsl  binding.UseSsl    if string.IsNullOrEmptybinding.User  string.IsNullOrEmptybinding.Password            smtp.Credentials  new NetworkCredentialbinding.User binding.Password        return smtpprivate static System.Net.Mail.MailMessage CreateMailMessageMailMessage mail    var from  new MailAddressmail.From    var to  new MailAddressmail.To    var message  new System.Net.Mail.MailMessagefrom to    message.Subject  mail.Subject    message.Body  mail.Body    return messageCon estos tres artefactos tendremos definido todo el comportamiento de nuestro custom binding que nos ayuda en el envio de emails. Lo que nos falta es hacerle saber al sistema que vamos a extender el comportamiento de las Azure Functions para lo que tendremos que crear un nuevo artefacto que implemente IExtensionConfigProviderpublic class MailExtensionConfigProvider  IExtensionConfigProvider    public void InitializeExtensionConfigContext context             add json to MailMessage mapper        context.AddConverterinput  input.ToObject         add output custom binding        context            .AddBindingRule            .BindToCollectorattr  new MailAsyncCollectorattr    En este proveedor de configuracion indicaremos como vamos a mapear un MailMessagepor si lo quisieramos usar con otro lenguaje de programacion como JavaScript y definiremos el out binding indicando el atributo y el IAsyncCollector que hemos creado.Finalmente para que nuestro ensamblado registre por defecto el proveedor de configuracion que hemos creado tendremos que declarar una clase tipo Startup donde al arrancar el host de Azure Functions anadiremos nuestra extensionusing Microsoft.Azure.WebJobsusing Microsoft.Azure.WebJobs.Hostingassembly WebJobsStartuptypeofCustomBindings.Startupnamespace CustomBindings    public class Startup  IWebJobsStartup            public void ConfigureIWebJobsBuilder builder                    builder.AddExtension            Probando nuestro bindingSi queremos ver que todo ha funcionado correctamente podemos crear un nuevo proyecto de Azure Functions. Crearemos una funcion que responda a una peticion HTTP y cuyo nivel de acceso sea AnonymousFunctionNameHelloWorldpublic static IActionResult Run    HttpTriggerAuthorizationLevel.Anonymous get Route  null HttpRequest req    ILogger log    log.LogInformationC HTTP trigger function processed a request.    string name  req.Queryname    return name  null         ActionResultnew OkObjectResultHello name         new BadRequestObjectResultPlease pass a name on the query string or in the request bodyAhora anadiremos un parametro de salida con el binding indicando el nombre del AppSetting de la cadena de conexion. En este caso SmtpConnectionString. Y en el cuerpo de la funcion anadiremos los datos necesarios para enviar el emailFunctionNameHelloWorldpublic static IActionResult Run    HttpTriggerAuthorizationLevel.Anonymous get Route  null HttpRequest req    MailSendConnection  SmtpConnectionString out MailMessage message    ILogger log    log.LogInformationC HTTP trigger function processed a request.    string name  req.Queryname    message  new MailMessage    message.From  noreplydeveloperro.com    message.To  fernando.escolardeveloperro.com    message.Subject  Binding Demo    message.Body  Deal with it name    return name  null         ActionResultnew OkObjectResultHello name         new BadRequestObjectResultPlease pass a name on the query string or in the request bodyPara finalizar dentro del archivo local.settings.json y en la propiedad Values anadiremos la cadena de conexion con nuestro servidor SMTPValues     AzureWebJobsStorage     SmtpConnectionString Hostsmtp.server.comPort587UsermyUserPasswordmyPasswordUseSsltrue    FUNCTIONS_WORKER_RUNTIME dotnet  Ahora podemos ejecutar el proyecto llamar a la funcion con nuestro browser preferido y ver como nos llega un email a la cuenta que hayamos configurado.ConclusionesLas Azure Functions son una herramienta muy potente que nos permite escribir muy poco codigo. Cada dia que las utilizo me gustan mas. Y sabiendo como crear bindings personalizados podemos conseguir centrarnos aun mas tan solo en la parte mas importante de nuestro codigo.Una excelente manera de convertir 10 lineas de codigo que podrian ser una funcion compartida en cinco clases. Pero a la vez molar que te cagas por lo guapo que queda meter un atributo decorando una propiedad marcada con out y que automagicamente realice una accion mas o menos compleja.Puedes ver el proyecto completo que hemos utilizado en este articulo en este repositorio de Github."
    } ,
  
    {
      "title"    : " Microsoft Graph: Webhooks",
      "category" : "",
      "tags"     : "azure, microsoft, graph, webhooks",
      "url"      : "/2019/04/09/microsoft-graph-webhooks",
      "date"     : "2019-04-09 07:58:12Z",
      "content"  : "Es el dia en el que salen a la venta las entradas de la ComicCon o cualquier evento grande de semejante calado como el Global Azure Bootcamp en Madrid Barcelona o incluso en Seattle. Cuando publicas un nuevo post en tu blog y todavia no se ha actualizado. Si estas usando autenticacion de doble factor y aun no ha llegado el email de verificacion. El caso es que toda situacion semejante acaba igual aporreando la tecla F5.Asi nacieron los ataques DDoS.Porque estamos en una sociedad que esta acostumbrada a tener todo de forma inmediata. Internet nos ha abierto a un mundo de servicios creados para en apariencia hacernos mas felices. Hemos olvidado los modales en un mundo cada vez mas egocentrista en el que quiero y ahora han sustituido palabras tan elegantes como podrias y por favor.Esta forma de pensar tambien ha cambiado nuestra forma de programar. Ya no dudamos en realizar llamadas continuas a un servicio hasta obtener una respuesta. No nos preocupa machacar una API desde un bucle infinito en codigo cliente. Y eso de los WebSockets es que no los implementa casi nadie.En este grosero mundo de los Thin Clients de las paginas Web pesadas de APIs Rest y de los servicios distribuidos que interactuan unos con otros los Webhooks son la cortesia.Es lo mas parecido a decirle a un cliente que te alegra mucho su interes pero que aun no tienes lo que esta buscando. No obstante no tiene por que preocuparse debido a que le avisaras tan pronto este disponible. Y tan pronto tengas lo que te pedian acercarte por su casa para avisarle.WebhookEl termino de Webhook fue originariamente propuesto por Jeff Lindsay en 2007 en un articulo de su blog como una propuesta para un prototipo en el que estaba trabajando.La idea era que basandose en las tecnologias que existian en esa epoca no se habian definido los WebSockets crear un sistema de callbacks web. De alguna forma un cliente enviaria una peticion indicando que tipo de eventos quiere escuchar y una URL. Despues el sistema enviaria peticiones POST indicando detalles de los eventos con forme estos ocurrieran.De esta forma surgieron las implementaciones que podemos encontrar hoy en dia de Webhooks donde no hay un estandar definido pero mas o menos todos los proveedores estan haciendo implementaciones parecidasPrimero se crea una suscripcion en la que indicaremos la URL que va a recibir los Webhooks los tipos de evento a los que se suscribe y una contrasena.Antes de crearse la suscripcion se valida que esa URL responde a una llamada tipo echo Se envia una cadena de texto y la URL deberia devolver esa misma cadena.Cuando suceden eventos el sistema original envia peticiones POST a la URL indicada con los detalles de estos.A esa peticion se le anade la contrasena que se especifico en la creacion de la suscripcion. De esta forma podemos validar que es una llamada valida.Webhooks en Microsoft GraphMicrosoft Graph como toda buena API contemporanea nos propone una relacion basada en la amabilidad y la educacion. Asi que tiene implementado un sistema de Webhooks que podremos encontrar en su documentacion con el nombre de notificaciones.Para poder usar los Webhooks antes tendremos que tener acceso al Microsoft Graph con un token valido puedes leer acerca de esto en el articulo anterior. Y antes de poder recibir notificaciones tendremos que crear una suscripcionSubscriptionsPara crear una suscripcion tendremos que tener permiso de lectura sobre el recurso acerca del que deseamos recibir notificaciones. Por ejemplo  si es un email necesitaremos el permiso de Mail.Read o si es un usuario el User.Read.Despues tendremos que realizar una peticion POST a la URL de las suscripciones con unos datos en formato JSONPOST httpsgraph.microsoft.comv1.0subscriptionsContenttype applicationjsonAuthorization Bearer ...   changeType updated   notificationUrl httpsmydomain.comwebhookhandler   resource groups   expirationDateTime20190410T182345.9356913Z   clientState mysecrettextDondechangeType puede tener valores created updated or deleted. No obstante un resource raiz como por ejemplo groups o users no pueden tener el tipo created.notificationUrl es la URL donde se aloja nuestro capturador de eventos.resource es el tipo de recurso del que se quieren recibir eventos de cambio. Se corresponde con el path del recurso en el propio Graph. Por ejemplo si quiero recibir cambios de usuarios la URL es httpsgraph.microsoft.comv1.0users y el path sera users. O si deseo recibir cambios en mi inbox cuya URL es httpsgraph.microsoft.comv1.0memailFoldersInboxmessages entonces pondre como valor memailFoldersInboxmessages.expirationDateTime es la fecha y la hora en la que la suscripcion caducara. Para Mail Calendar Contacts y elementos raiz no podra ser mayor de 4230 minutos menos de 3 dias. Y para alertas de seguridad 43200 menos de 30 dias.clientState es la contrasena o secreto que sera enviado a nuestra URL para validar la autenticidad del mensaje.Como hemos visto los tiempos de expiracion de una suscripcion son bastante cortos por lo que podria ser interesante realizar una Azure Function que cada cierto tiempo comprobara que existe la suscripcion y en caso negativo la creara. De esta forma siempre estariamos suscritos.Un ejemplo seria el siguienter Newtonsoft.Jsonusing Systemusing System.Netusing System.Net.Http.Headersusing Newtonsoft.Jsonconst string tenantId  ....const string clientId  ....const string clientSecret  ....const string notificationUrl  httpsmywebhooks.comwebhookhandlerconst string notificationSecret  MySuperSecretValueYouNeverWillKnowpublic static async Task RunTimerInfo myTimer ILogger log    log.LogInformationChecking if subscription exists    var auth  await GetTokenAsynctenantId clientId clientSecret    await CreateSubscriptionIfNotExitslog auth notificationUrl notificationSecret    log.LogInformationDoneAqui declarariamos las variables que necesitamos para funcionar. Lo primer seria definir el tenant el client_id y el client_secret para solicitar el token. Y despues definiriamos la URL y la contrasena de el capturador de eventos.Para obtener un tokenclass AuthResponse    JsonPropertyaccess_token    public string AccessToken  get set     JsonPropertytoken_type    public string Type  get set     JsonPropertyexpires_in    public int ExpiresIn  get set     JsonPropertyext_expires_in    public int ExtExpiresIn  get set     public AuthenticationHeaderValue AsHeader            return new AuthenticationHeaderValueType AccessToken    static async Task GetTokenAsyncstring tenantId string clientId string clientSecret    var url  httpslogin.microsoftonline.comtenantIdoauth2v2.0token    var data  new Dictionary    data.Addgrant_type client_credentials    data.Addclient_id clientId    data.Addclient_secret clientSecret    data.Addscope httpsgraph.microsoft.com.default    using var client  new HttpClient            client.DefaultRequestHeaders.Accept.Addnew MediaTypeWithQualityHeaderValueapplicationjson        var response   await client.PostAsyncurl new FormUrlEncodedContentdata        var json  await response.Content.ReadAsStringAsync        var res  JsonConvert.DeserializeObjectjson        return res    Y finalmente validariamos si existe o no la suscripcion. En caso negativo la crearemosclass Response    JsonPropertyvalue    public IEnumerable Value  get set class Subscription    JsonPropertyresource    public string Resource  get set     JsonPropertychangeType    public string ChangeType  get set     JsonPropertyclientState    public string ClientState  get set     JsonPropertynotificationUrl    public string NotificationUrl  get set     JsonPropertyexpirationDateTime    public string ExpirationDateTime  get set static async Task CreateSubscriptionIfNotExitsILogger log AuthResponse auth string url string secret    var subscriptionsUrl  httpsgraph.microsoft.comv1.0subscriptions    using var client  new HttpClient            client.DefaultRequestHeaders.Authorization  auth.AsHeader        var response  await client.GetAsyncsubscriptionsUrl        var json  await response.Content.ReadAsStringAsync        var res  JsonConvert.DeserializeObjectjson        var exists  res.Value.Anyx  x.NotificationUrl  url        log.LogInformationexists    exists        if exists                    var subscription  new Subscription                            ChangeType  updateddeleted                NotificationUrl  url                Resource  groups                ExpirationDateTime  DateTime.UtcNow.AddMinutes4200.ToStringo                ClientState  secret                        var content  new StringContentJsonConvert.SerializeObjectsubscription.ToString System.Text.Encoding.UTF8 applicationjson            var resp  await client.PostAsyncsubscriptionsUrl content            var text  await resp.Content.ReadAsStringAsync            log.LogInformationtext            En este caso nos hemos suscrito a modificaciones y borrados que se observen en el tipo de recursos groups.Webhook handlerAntes de poder crear una suscripcion tendremos que tener creado nuestro Webhook. No hace falta una implementacion completa simplemente una implementacion que responda a la validacion tipo echo. Esta validacion consiste en enviar de vuelta el valor del parametro validationToken que podremos encontrar en la URL de solicitud con codigo de estado 200public static async Task RunHttpRequest req ILogger log    log.LogInformationWebHook handled    var validationToken  req.QueryvalidationToken.FirstOrDefault    if string.IsNullOrWhiteSpacevalidationToken            log.LogInformation  validationToken    validationToken        return new OkObjectResultvalidationToken        return new AcceptedResultGetRawUrlreq Notification receivedstatic string GetRawUrlHttpRequest request    var httpContext  request.HttpContext    return httpContext.Request.SchemehttpContext.Request.HosthttpContext.Request.PathhttpContext.Request.QueryStringEn caso de ser una notificacion el parametro validationToken no existira o lo encontraremos vacio. Entonces para indicar que hemos recibido el evento correctamente responderemos con el codigo de estado 202.En cada notificacion se nos pueden enviar uno o varios eventos que se almacenan en una propiedad llamada value del JSON que nos llegapublicclassWebhook where T  ResourceData    JsonPropertyvalue    publicIEnumerable Events getsetCada evento tiene una serie de propiedades que nos hablan de la suscripcion y del evento que se ha producido. Incluyendo una propiedad llamada resourceData donde encontraremos detalles especificos del eventopublicclassEvent where T ResourceData    JsonPropertychangeType    publicstringChangeTypegetset    JsonPropertyclientState    publicstringClientStategetset    JsonPropertyresource    publicstringResourcegetset    JsonPropertyresourceData    publicT ResourceDatagetset    JsonPropertysubscriptionExpirationDateTime    publicDateTimeSubscriptionExpirationDateTimegetset    JsonPropertysubscriptionId    publicstringSubscriptionIdgetset    JsonPropertytenantId    publicstringTenantIdgetsetUn ResourceData tiene una serie de propiedades por defecto pero el resto son especificas de cada sistemapublic class ResourceData    JsonPropertyodata.type    public string ODataType  get set     JsonPropertyodata.id    public string ODataId  get set Por ejemplo si estamos hablando de un usuario que se anade a un grupo existentepublic class GroupMemberData  ResourceData    JsonPropertyid    public string Id  get set     JsonPropertyorganizationId    public string OrganizationId  get set     JsonPropertysequenceNumber    public object SequenceNumber  get set     JsonPropertymembersdelta    public IEnumerable MembersDelta  get set public class MembersDelta    JsonPropertyid    public string Id  get set Una vez tenemos definidos estos tipos seremos capaces de deserializar la peticion y comprobar que se envia en ClientState el secreto que mandamos en la suscripcionpublic static async Task RunHttpRequest req ILogger log    var validationToken  req.QueryvalidationToken.FirstOrDefault    if string.IsNullOrWhiteSpacevalidationToken            log.LogInformationValidating Webhook handler    validationToken        return new OkObjectResultvalidationToken        log.LogInformationWebHook handled    var json  await new StreamReaderreq.Body.ReadToEndAsync    var webhook  JsonConvert.DeserializeObjectjson    if webhook.Events.Allx  x.ClientState  notificationSecret            log.LogInformationBad Client State        return new BadRequestResult         Do something    return new AcceptedResultGetRawUrlreq Notification receivedY a partir de aqui tan solo quedaria tratar la respuesta para realizar algun tipo de actividad u otro.Un detalle muy importante para la primera vez que esteis esperando un Webhook de Microsoft Graph estos pueden tardar en llegar hasta un minuto despues de haber realizado una accion. Asi que lo mejor es tranquilizarse tener paciencia y ser educados.ConclusionLos Webhooks se estan extendiendo cada dia mas por Internet. Su uso es complementario al de las APIs Rest mas conocidas y aportan un valor anadido muy importante como es el poder recibir notificaciones sin necesidad de tener que hacer constantes peticiones a un servicio. Ademas usan protocolos ya conocidos y muy comunes que nos facilitan su programacion en practicamente cualquier entorno.Lo mejor es que se usa en los servicios mas conocidos y se esta adoptando como un estandar de facto. Asi que ya solo queda que la IETF tomen nota de ello y anadan un RFC ex profeso a su libreria.Y en el contexto de Microsoft Graph nos aporta la funcionalidad necesaria para poder realizar actividades automatizadas a partir de eventos que ocurren en nuestros servicios en la nube. Algo casi indispensable a la hora de realizar integraciones hoy en dia.Una solucion simple elegante y facil de implementar que nacio para cubrir un problema bastante mas complejo."
    } ,
  
    {
      "title"    : "Microsoft Graph",
      "category" : "",
      "tags"     : "azure, microsoft, graph",
      "url"      : "/2019/04/02/microsoft-graph",
      "date"     : "2019-04-02 07:40:40Z",
      "content"  : "Toda buena pelicula adolescente de los 80 comienza con un pelele al que todo el mundo margina. Este personaje esconde algo en su interior de lo que al principio solo se notan pequenos y casuales atisbos. Despues de una gran aventura de autoconocimiento y superacion personal el protagonista consigue dominar este poder oculto y convertirse en un verdadero heroe. Esto es lo que le pasa a Naruto Microsoft Graph que es como Daniel Larusso.Todos llegamos a esta API por lo que parece que promete por esos atisbos de poder que nos dejan vislumbrar las demos que Microsoft nos presenta. Luego cuando llega la hora de utilizarla a fondo nos damos cuenta de sus problemas de lo crazy de algunas decisiones de su modelado. Entonces la marginamos. Nos metemos con ella. Ya no la invitamos a nuestros meetups. Una vez superado este duro camino plagado de frustracion la aceptamos y nos damos cuenta de todas las posibilidades que nos aporta llegando finalmente a un idilio llano y sincero.Microsoft Graph es una API con un modelo de datos unificado para poder explotar todas las aplicaciones que forman parte de Office 365 Azure Active Directory Enterprise Mobility Windows 10 y Education. Esta basada en OData v4 aunque no todos los recursos tienen hoy en dia las mismas capacidades. Y podemos encontrar toda la documentacion de la version 1.0 y beta aqui.Existe un SDK para practicamente todos los lenguajes y plataformas de programacion conocidas que nos ayudara a autenticarnos y solicitar datos de esta API. No obstante al ser OData una especificacion por encima de REST resulta simple utilizarla sin necesidad de usar el SDK.App registrationLo primero que tenemos que saber para explotar Microsoft Graph es como conseguir acceso. Para ello tendremos que solicitar un token a la API v2 del protocolo de autenticacion de Azure Active Directory. Afortunadamente esta API usa la especificacion OIDC que se basa en OAuth 2.0. Asi que este paso lo tenemos practicamente superado. Lo unico que tendremos que saber es que hay dos tipos de permisos dentro de Graph que podemos solicitarDelegated es un tipo de permiso que se obtiene al identificarse usando un usuario y una contrasena. De esta forma diremos que se delega un permiso sobre Graph a este usuario. Algunos privilegios especificos pueden exigir que este usuario sea Administrador.Application este es el tipo de permiso que tenemos que usar en una aplicacion que no requiere de un usuario para que realice ninguna accion. Para estas aplicaciones se requiere que un administrador apruebe previamente el uso de estos permisos.Para configurar estos permisos tendremos que dirigirnos al portal de Azure con una cuenta de administrador de nuestro tenant. Ahi nos dirigiremos a la opcion de menu Azure Active Directory dentro de esta opcion a App registrations Preview y al boton de New registrationDespues le daremos un nombre al App registration comprobaremos que esta marcada la opcion de permitir solo accesos con cuentas de nuestro directorio y presionaremos el boton RegisterAl crearla podremos observar en el panel de OverviewApplication client ID que corresponde con el client_idDirectory tenant ID que se corresponde con el valor de tenant_idUna vez hemos creado el nuevo registro podremos generar un nuevo secreto navegando a Certificates  secrets y presionando el boton de New client secretNo olvides guardar en un lugar seguro el secreto que se acaba de generar ya que coincidira con el valor de client_secret.Ahora vamos a asignar los permisos del registro. Para ello iremos a API permissions y dentro de esta opcion pulsaremos Add a permissionEn el nuevo modal que se abrira lo primero que tendremos que elegir es Microsoft Graph para poder elegir los permisos para esta APIDespues nos dara a elegir entre Delegated permissions o Application permissions. Deberemos elegir una en dependencia de como tenemos pensado obtener el token. Despues tendremos un listado de recursos y al desplegar alguno de ellos encontraremos los permisos especificos que podemos asignarSi queremos saber que permisos deberiamos anadir todo depende de la operacion dentro de Microsoft Graph que queremos realizar. Afortunadamente en la documentacion en la referencia a la API podremos encontrar antes de cada accion que permisos son necesarios para ejecutarlaUna vez hayamos anadido los que necesitemos en el portal de Azure podemos presionar en Add Permissions. Y si queremos que ya cuenten con la validacion del administrador en la pantalla de API permissions presionaremos el boton de Grant admin consent for your directory.Ahora ya podremos conectar con Microsoft Graph y jugar con todas las posibilidades que nos ofrece. Para ello proponemos 4 formasA mano esta es la que siempre me gusta mas para ensenar. Aqui podreis ver como funciona internamente.Usando el SDK esta es la mas recomendada a dia de hoy.Usando el futuro SDK esta es la que mas me gusta pero tienen que pulir aun ciertos aspectos.Microsoft Graph Explorer para que podamos probar nuestras llamadas en un contexto mas directo y simple.A manoAccess tokenEn un articulo anterior vimos las diferentes formas que podriamos usar para pedir este token a un servicio que usara OAuth 2.0. En este caso vamos a usar el mas simple usando tan solo el Grant Type de client_credentials.Para ello nos crearemos una clase donde almacenar la respuesta de una peticion de tokenclass AuthResponse    JsonPropertyaccess_token    public string AccessToken  get set     JsonPropertytoken_type    public string Type  get set     JsonPropertyexpires_in    public int ExpiresIn  get set     JsonPropertyext_expires_in    public int ExtExpiresIn  get set     public AuthenticationHeaderValue AsHeader            return new AuthenticationHeaderValueType AccessToken    Y realizaremos la peticion al servidor de la forma que indicabamosasync Task GetAuthAsyncstring tenantId string clientId string clientSecret    var url  httpslogin.microsoftonline.comtenantIdoauth2v2.0token    var data  new Dictionary    data.Addgrant_type client_credentials    data.Addclient_id clientId    data.Addclient_secret clientSecret    data.Addscope httpsgraph.microsoft.com.default    using var client  new HttpClient            client.DefaultRequestHeaders.Accept.Addnew MediaTypeWithQualityHeaderValueapplicationjson        var response   await client.PostAsyncurl new FormUrlEncodedContentdata        var json  await response.Content.ReadAsStringAsync        return JsonConvert.DeserializeObjectjson    Donde los valores que le pasamos a la funcion coincidiran que aquellos que recogimos en el proceso de creacion del App registration en el portal de Azure. Y el valor de scope seran los permisos que queremos solicitar para llamar a la API que tenemos que haber seleccionado y consentido en el App registration o el valor httpsgraph.microsoft.com.default que indicara que estamos solicitando todos los permisos que hemos seleccionado en el App registration y asi evitar estar constantemente indicandolos.Llamando a Microsoft GraphVamos a usar como ejemplo una llamada al listado de usuario basandonos en la propia referencia de Microsoft Graph. Para ello tendremos que tener al menos el permiso de aplicacion de User.Read.All consentido en nuestro App registration.Una vez tenemos esto tendremos que realizar una llamada al endpoint httpsgraph.microsoft.comv1.0users usando un token valido y tendremos ese listado.La respuesta de listados desde Microsoft Graph esta envuelta en un objeto que nos indica cual es la siguiente pagina asi pues crearemos una estructura compleja para mapear los resultadosclass GraphListResponse    JsonPropertyodata.context    public string Context  get set     JsonPropertyodata.nextLink    public string NextLink  get set     JsonPropertyvalue    public IEnumerable Value  get set De un usuario nos interesaria en un principio el id el displayNamey el mail por lo que no vamos a crear mas propiedadesclass User    JsonPropertyid    public string Id  get set     JsonPropertydisplayName    public string DisplayName  get set     JsonPropertymail    public string Email  get set Por lo que la llamada para listar todos los usuarios podremos filtrar las propiedades a devolver anadiendo la propiedad select con el valor iddisplayNamemail quedando el endpoint como httpsgraph.microsoft.comv1.0usersselectiddisplayNamemailasync Task GetUsersAsyncAuthResponse auth string nextLink  null    var result  new List    var url  httpsgraph.microsoft.comv1.0usersselectiddisplayNamemail    using var client  new HttpClient            client.DefaultRequestHeaders.Accept.Addnew MediaTypeWithQualityHeaderValueapplicationjson        client.DefaultRequestHeaders.Authorization  auth.AsHeader        GraphListResponse res  null        do             var response  await client.GetAsyncurl            var json  await response.Content.ReadAsStringAsync            res  JsonConvert.DeserializeObjectjson            if res  null                            result.AddRangeres.Value                url  res.NextLink                     while res  null  string.IsNullOrEmptyres.NextLink        return resultAsi pues podriamos recoger e imprimir todos los usuarios por pantalla realizando una llamada para recoger el token de acceso y otra para recoger los usuariosvar auth  await Functions.GetAuthAsynctenant_id client_id client_secretvar users  await Functions.GetUsersAsyncauthConsole.WriteLineUser Listforeachvar user in users    Console.WriteLine  user.DisplayName user.EmailUsando el SDKSi queremos usar el SDK para .Net que nos provee Microsoft primero tendremos que instalar los paquetes de Microsoft.Identity.Client y Microsoft.Graph.En un proyecto de dotnet core se puede realizar mediante consola dotnet add MyProject.csproj package Microsoft.Identity.Client... dotnet add MyProject.csproj package Microsoft.Graph...Y si estamos en Visual Studio siempre podemos abrir el panel de Package Manager Console InstallPackage Microsoft.Identity.Client... InstallPackage Microsoft.Graph...El codigo es bastante parecido. En un principio usamos los artefactos de Microsoft.Identity.Client para poder recoger un token para acceder a Microsoft Graph. Despues declaramos un cliente de Graph que use el proveedor de tokens que hemos generado. Finalmente realizamos la llamada a la API y escribimos los resultados por pantallavar clientCredential  new ClientCredentialclientSecretvar authClient  new ConfidentialClientApplication        clientId        httpslogin.microsoftonline.comtenantIdv2.0        urnietfwgoauth2.0oob        clientCredential        new TokenCache        new TokenCachevar authProvider  new DelegateAuthenticationProviderasync request             var authResult  await authClient.AcquireTokenForClientAsyncnew   httpsgraph.microsoft.com.default         request.Headers.Authorization  new AuthenticationHeaderValueBearer authResult.AccessToken    var graphServiceClient  new GraphServiceClienthttpsgraph.microsoft.comv1.0 authProviderIGraphServiceUsersCollectionPage page  nullConsole.WriteLineUser Listdo     var task  page  null  page.NextPageRequest  graphServiceClient.Users.Request.Selectu  new  u.Id u.DisplayName u.Mail     page  await task.GetAsync    foreachvar user in page            Console.WriteLine  user.DisplayName user.Mail     whilepage  null  page.NextPageRequest  nullEl futuro del SDKComo el codigo actual para obtener tokens puede resultar un poco confuso se esta desarrollando un paquete que esta actualmente en fase de pruebas y falla como una escopeta de feria donde la idea es simplificar esa parte. El paquete en cuestion se llama Microsoft.Graph.Auth dotnet add MyProject.csproj package Microsoft.Graph... dotnet add MyProject.csproj package Microsoft.Graph.Auth...o en Visual Studio InstallPackage Microsoft.Graph... InstallPackage Microsoft.Graph.Auth...El codigo que hoy en dia no funciona seria el siguientevar clientCredential  new ClientCredentialclientSecretvar clientApplication  ClientCredentialProvider.CreateClientApplicationclientId clientCredential tenant tenantIdvar authenticationProvider  new ClientCredentialProviderclientApplicationvar graphServiceClient  new GraphServiceClientauthenticationProviderIGraphServiceUsersCollectionPage page  nullConsole.WriteLineUser Listdo     var task  page  null  page.NextPageRequest  graphServiceClient.Users.Request.Selectu  new  u.Id u.DisplayName u.Mail     page  await task.GetAsync    foreachvar user in page            Console.WriteLine  user.DisplayName user.Mail     whilepage  null  page.NextPageRequest  nullComo podemos observar quedaria mucho mejor expuesta la obtencion del token simplificando mucho el codigo anterior.Microsoft Graph ExplorerSi quereis conocer bien Microsoft Graph y asi entender un poco mejor lo que significa la palabra frustracion siempre podeis usar una herramienta que Microsoft nos ofrece para que junto con la documentacion podamos explorar y entender la potencia interior oculta en esta API.Es muy sencillo de utilizarVamos a la pagina web de Microsoft Graph Explorer httpsdeveloper.microsoft.comenusgraphgraphexplorerHacemos login y nos asignamos permisos usando el panel de la izquierda.Es un funcionamiento de peticionrespuesta de HTTP pero si abrimos unas cuantas ventanas podremos realizar operaciones mas complejas y navegar en los oscuros y desconocidos fondos que ahi se ocultan.ConclusionesHoy hemos mostrado como utilizar Microsoft Graph a partir de aqui si seguis profundizando en el tema ya no es mi culpa."
    } ,
  
    {
      "title"    : "Azure Functions con docker",
      "category" : "",
      "tags"     : "azure-functions, azure, functions, docker, node",
      "url"      : "/2019/03/26/azure-functions-docker",
      "date"     : "2019-03-26 07:35:02Z",
      "content"  : "Docker y los containers son a la programacion lo que el rebozado y empanado a la cocina tradicional. Si bien anade un paso extra que ensucia y a veces se hace un poco dificil despues puedes transportar la comida con mucha facilidad sin romperla y ademas te resulta muy facil de comer. Y no nos enganemos esta claro que a todo el mundo le gustan las croquetas.Si has llegado hasta aqui es posible que quieras hacer pechugas a la villaroy o una imagen de docker con tus Azure Functions dentro nunca se sabe lo que puede pasar con las inteligencias artificiales hoy en dia. De cualquier forma estas en el lugar correcto para lo primero necesitaremos pan rallado huevo bechamel y pechugas de pollo y para lo segundo docker un proyecto de Azure Functions y 5 minutos de tu vida.Aqui os vamos a exponer dos recetas pero lo mas recomendable es que mezcleis cosas de una y de otraReceta 1La primera receta es mas a mano. Vamos a empezar desde hacer la bechamel hasta acabar friendo las pechugas. Eso no quiere decir que vayan a ser unos pasos dificilesPreparando la bechamelCogemos un cazo y calentamos leche. Metemos harina mantequilla mantenemos a fuego bajo revolvemos y rezamos para que no nos salgan grumos.En el caso de las Azure Functions necesitaremos tener instalado inicialmente npm dotnet sdk y docker.Despues instalaremos una tool de Azure Functions que nos permitira crear nuestro proyectonpm i g azurefunctionscoretoolsAhora podremos crear nuestro primer proyecto directamente con soporte para docker func init . dockerSelect a worker runtime1. dotnet2. node3. python previewChoose option 1dotnetWriting Usersfernandoescolarprojectsfunctionsdocker.vscodeextensions.jsonWriting DockerfileUna vez tenemos el proyecto anadimos nuestra primera Function func newSelect a template1. QueueTrigger2. HttpTrigger3. BlobTrigger4. TimerTrigger5. DurableFunctionsOrchestration6. SendGrid7. EventHubTrigger8. ServiceBusQueueTrigger9. ServiceBusTopicTrigger10. EventGridTrigger11. CosmosDBTrigger12. IotHubTriggerChoose option 2Function name HelloHelloThe function Hello was created successfully from the HttpTrigger template.Empanando las pechugasRebozar es una tarea simple sucia y tediosa. Las pechugas ya forman una unidad con la bechamel. Ahora batimos unos huevos en un plato mientras que en otro ponemos pan rallado. Entonces cogemos una a una las pechugas con bechamel y las banamos en el plato de huevo para acto seguido pasarlas por el de pan rallado. Si lo hacemos bien tendremos un preparado perfecto listo para cocinar.Por otro lado si queremos hacer una imagen de docker con nuestras Functions lo primero que haremos para poder probarlas es ponerles un acceso publico. Para ello abrimos el archivo Hello.cs y sustituimos el AuthorizationLevel.Function por AuthorizationLevel.Anonymous.FriendoPodriamos decir que solo tenemos que echar aceite a una sarten esperar a que se caliente e ir pasando las pechugas hasta que consideremos que estan bien hechas. Pero freir es un arte. Es como pintar un bello retrato o componer la mas deliciosa de las sinfonias. Solo el talento y la practica nos llevara a tener unos resultados excelentes.Mientras que generar una imagen de docker cuando ya tienes el Dockerfile hecho es bastante facil. Basta con ejecutar el comando build docker build t azurefunctionsdemo .Step 16  FROM microsoftdotnet2.1sdk AS installerenvStep 26  COPY . srcdotnetfunctionappStep 36  RUN cd srcdotnetfunctionapp      mkdir p homesitewwwroot      dotnet publish .csproj output homesitewwwrootStep 46  FROM mcr.microsoft.comazurefunctionsdotnet2.0Step 56  ENV AzureWebJobsScriptRoothomesitewwwrootStep 66  COPY frominstallerenv homesitewwwroot homesitewwwrootSuccessfully tagged azurefunctionsdemolatestComiendoLa mejor parte de hacer unas pechugas a la villaroy es comerselas. Con cuchillo y tenedor con la mano... da igual. Si sobra que no creo lo suyo es guardarlas en un tupperware para comerlas manana o pasado.En cuanto una imagen de docker si queremos ejecutarla localmente hay que lanzar el comando run exponiendo el puerto 80 en el 8080 local docker run p 808080 azurefunctionsdemoHosting environment ProductionContent root path Now listening on http80Application started. Press Ctrl C to shut down.Y podremos comprobar que ha funcionado visitando la url httplocalhost8080apihellonameChimo Bayo.Si lo que queremos es coger esa imagen y realizar un deploy a kubernetes las tools que instalamos al inicio nos ayudaranfunc deploy platform kubernetes name azurefunctionsdemo registry  pullsecret  min 3 max 10Un poquito de perejil y para el clusterReceta 2Esta es la receta mas sencilla con unos resultados excelentes con poco esfuerzoIr al corte ingles y comprarlas hechasHace un tiempo paseando por el supermercado del corte ingles descubri que hay un puesto fuera con comida ya cocinada. Si vas antes de las 12 despues de esa hora no puedo garantizaros nada tienen unas pechugas a la villaroy que estan riquisimas. Casi como las de mi madre. Pero en esta vez no habremos tenido que ensuciar nada solo meterlas al microondas.En el caso de docker y las Azure Functions tambien tenemos una tienda donde podremos adquirir todo lo que necesitemos.Si echamos un vistazo a Azure Functions Base en el Hub de docker encontraremos diferentes imagenes base en dependencia de la plataforma y lenguajes de programacion que usemos.Desde ahi encontraremos enlaces al repositorio de GitHub de Azure Functions Docker.En este repositorio podremos encontrar los ejemplos de Dockerfile para muchos casos de uso y para lenguajes de programacion como node c python o powershell. Ademas de una serie de archivos .yml que nos ayudaran a automatizar la integracion continua de nuestros contenedores en Hubs privados de Azure.Una mina de oro.ConclusionesA mi me gustan mucho las pechugas a la villaroy y tambien la propuesta de containers de docker con Azure Functions. Ambas cosas son sencillas de conseguir aunque tener maestria requiere practica y tiempo."
    } ,
  
    {
      "title"    : "OAuth 2.0 Grant Types",
      "category" : "",
      "tags"     : "oauth, csharp, jwt",
      "url"      : "/2019/03/19/oauth-authentication-grant-types",
      "date"     : "2019-03-19 08:26:33Z",
      "content"  : "Muy buenos dias y gracias por acompanarnos un martes mas. La pregunta de hoy y por 25 pesetas la respuesta acertada diganos tipos de concesion Grant Types permitidos por OAuth 2.0 como por ejemplo password. Un dos tres responda otra vez.PasswordClient CredentialsImplicitAuthorization CodeAuthorization Code con PKCERefresh TokenJWTsonido ensordecedor de bocinaEscuchemos a los supertacanonesAunque un servicio OAuth es capaz de devolver JSON Web Tokens JWT no es un tipo de concesion valido.OAuth 2.0 es una especificacion que describe diferentes formas Grant Types o tipos de concesion de solicitar un token de acceso access_token para un servicio HTTP. Se usa como base de la especificacion Open Id Connect OIDC y tambien de los protocolos de autenticacion autorizacion implementados por las grandes empresas de internet Twitter Facebook Microsoft... Google de hecho usa OIDC.Si bien es verdad que el RFC de OAuth no habla explicitamente de JWT como formato para los access_token es la forma mas comun en la que lo podremos encontrar hoy en dia.Dentro de este contexto OAuth propone varias formas de solicitar un access_token Grant Types e incluso una forma de crear nuestros propios formatos pero los mas utilizados hoy por hoy son los que exponiamos anteriormente Authorization Code Authorization Code con PKCE Client Credentials Implicit Password y Refresh Token.Access TokenLa respuesta de todos los metodos de autorizacion con o sin autenticacion al final tiene que ser un formato semejante un JSON con los siguientes valoresaccess_token requerido Suele ser un JWT que despues usaremos para autorizar las peticiones a una API.token_type requerido El tipo de token que vamos a usar. Generalmente al usar JWT se usa el valor Bearer.expires_in recomendado Indica la duracion en segundos del access_token. Una vez caduca puede ser renovado usando el Grant Type de Refresh Token.refresh_token opcional Si el access_token va a expirar y nos permiten volver a generar el token necesitaremos este valor en el proceso de Refresh Token.scope opcional Es un parametro que se utiliza para autorizar un token en un contexto concreto. Generalmente nuestra API buscara un scope concreto para el que se ha autorizado el token que le envian.id_token opcional Si se utiliza un scope con valor openid puede significar que queremos utilizar OpenId Connect OIDC para solicitar una autorizacion a partir de una autenticacion. En ese supuesto puede aparecer un JWT extra donde encontraremos la informacion sobre el perfil del usuario.Authorization CodeEl Authorization Code es uno de los flujos de autorizacion que mas beneficios ofrece. Se utiliza por lo general en paginas web. La idea es que inicialmente se solicita una autorizacion con el siguiente formatoGET oauthauthorize   client_idexample_client_id   response_typecode   redirect_urihttpexampledomain.com   statestring_as_status   scopeopenid HTTP1.1Host authorizationserver.comDondeclient_id es el identificador publico de la aplicacion. Una aplicacion es el resultado de registrar un nuevo cliente en nuestro servidor OAuth.response_type debe ser code.redirect_uri es la URI que esta preparada para recoger la respuesta de esta peticion.state es un valor que se usa para evitar ataques CSRF Cross Site Request Forgery. Una cadena unica aleatoria que debe ser devuelta por el servidor para poderlos comparar y ver que son iguales.scope podria ser openid para OIDC o cualquier otro para autorizar diferentes aplicaciones.Entonces el servidor de autorizacion solicita un usuario y un password via un formulario web autenticacion. Al introducir datos correctos el servidor nos redireccionara a la pagina que le pasamos en el parametro redirect_urihttpexampledomain.com    codeexamplecode    statestring_as_statusEn esta respuesta el valor de state debe ser el mismo que pasamos en la peticion. Y como parametro code encontraremos un codigo que normalmente es valido durante unos 60 segundos a partir del que podremos realizar la peticion del tokenPOST oauthtoken HTTP1.1Host authorizationserver.comAccept applicationjsonAuthorization Basic user_password_formulaContentType applicationxwwwformurlencodedContentLength ...grant_typeauthorization_coderedirect_urihttpexampledomain.comcodeexample_codeDondegrant_type es authorization_code.redirect_uri debe ser la URI que se uso para solicitar el code.code es el codigo que nos permitira recoger el token.Como peculiaridad la peticion vendra autorizada usando el esquema Basic cuyo contenido responde a la siguiente formulauser_password_formula  base64client_id      client_secretDondeclient_id es el identificador publico de la aplicacion. Una aplicacion es el resultado de registrar un nuevo cliente en nuestro servidor OAuth. El mismo que usamos en la primera peticion.client_secret es una contrasena o secreto que generaremos en el servidor de OAuth en relacion con el cliente la aplicacion.Como respuesta tendremos el formato anteriormente descrito de Access TokenHTTP1.1 200 OKCacheControl nostorePragma nocacheContentType applicationjsonContentLength ...    access_token a_lot_of_characters_in_base_64    token_type Bearer    expires_in 3600    scope openid    id_token a_lot_of_characters_in_base_64Authorization Code con PKCESe usa PKCE Proof Key for Code Exchange con el fin de tener una comunicacion segura sin tener que usar valores de client_secret. Es la solucion recomendada para aplicaciones moviles y Single Page Application SPA.El flujo es exactamente igual al anterior salvo porque vamos a anadir dos parametros nuevoscode_verifier es una cadena de texto aleatoria de al menos 43 caracteres.code_challenge es un hash sha256 en base64 de code_verifier.De esta forma la peticion inicial seriaGET oauthauthorize   client_idexample_client_id   response_typecode   redirect_urihttpexampledomain.com   statestring_as_status   scopeopenid   code_challenge_methodS256   code_challengeexample_code_challenge HTTP1.1Host authorizationserver.comDondeclient_id es el identificador publico de la aplicacion. Una aplicacion es el resultado de registrar un nuevo cliente en nuestro servidor OAuth.response_type debe ser code.redirect_uri es la URI que esta preparada para recoger la respuesta de esta peticion.state es un valor que se usa para evitar ataques CSRF Cross Site Request Forgery. Una cadena unica aleatoria que debe ser devuelta por el servidor para poderlos comparar y ver que son iguales.scope podria ser openid para OIDC o cualquier otro para autorizar diferentes aplicaciones.code_challenge_method como usamos SHA256 para crear el hash sera S256.code_challenge es el hash sha256 en base64 del code_verifier que creamos anteriormente.Entonces el servidor de autorizacion solicita un usuario y un password via un formulario web. Al introducir datos correctos el servidor nos redireccionara a la pagina que le pasamos en el parametro redirect_urihttpexampledomain.com    codeexamplecode    statestring_as_statusEn esta respuesta el valor de state debe ser el mismo que pasamos en la peticion. Y como parametro code encontraremos un codigo que normalmente es valido durante unos 60 segundos a partir del que podremos realizar la peticion del tokenPOST oauthtoken HTTP1.1Host authorizationserver.comAccept applicationjsonContentType applicationxwwwformurlencodedContentLength ...grant_typeauthorization_coderedirect_urihttpexampledomain.comcodeexample_codecode_verifierexample_code_verifierDondegrant_type es authorization_code.redirect_uri debe ser la URI que se uso para solicitar el code.code es el codigo que nos permitira recoger el token.code_verifier es la cadena de texto que generamos al principio. En este paso se validara con el valor de code_challenge que enviamos en la anterior peticion.Al contrario que el Authorization Code simple en este caso no se requiere la cabera de Authorization.Como respuesta tendremos el Access TokenHTTP1.1 200 OKCacheControl nostorePragma nocacheContentType applicationjsonContentLength ...    access_token a_lot_of_characters_in_base_64    token_type Bearer    expires_in 3600    scope openid    id_token a_lot_of_characters_in_base_64Client CredentialsEl modelo mas sencillo de solicitar una autorizacion de OAuth 2.0 es Client Credentials. Se usa para la comunicaciones de maquina a maquina donde no se requiere el permiso de un usuario especifico para acceder a los datos.Su funcionamiento consiste en realizar una peticion al servidor en el endpoint del generador de tokensPOST oauthtoken HTTP1.1Host authorizationserver.comAccept applicationjsonContentType applicationxwwwformurlencodedContentLength ...grant_typeclient_credentialsclient_idexample_client_idclient_secretexample_client_secretscopeuser.readDondegrant_type es client_credentials.client_id es el identificador publico de la aplicacion. Una aplicacion es el resultado de registrar un nuevo cliente en nuestro servidor OAuth.client_secret es una contrasena o secreto que generaremos en el servidor de OAuth en relacion con el cliente la aplicacion.scope podria ser cualquier valor que ayude a autorizar el uso de nuestras aplicaciones.La respuesta directamente sera el Access TokenHTTP1.1 200 OKCacheControl nostorePragma nocacheContentType applicationjsonContentLength ...  access_token a_lot_of_characters_in_base_64  token_type Bearer  expires_in 3600  refresh_token a_lot_of_characters_in_base_64  scope user.readImplicitCuando hablamos de un flujo de autorizacion Implicit lo mas probable es que estemos trabajando con paginas web SPA Single Page Application. Generalmente NO se recomienda usar este flujo e incluso algunos servidores prohiben su uso. Hoy en dia se recomienda usar en su lugar el flujo de Authorization Code con PKCE.De cualquier forma podria ser que tengamos que usarlo asi que nunca sobra describirlo. Todo consiste en una peticion simple al servidorGET oauthauthorize    client_idexample_client_id    response_typetoken    redirect_urihttpexampledomain.com    statestring_as_statusscopeopenid    scopeopenid HTTP1.1Host authorizationserver.comDonderesponse_type es token.client_id es el identificador publico de la aplicacion. Una aplicacion es el resultado de registrar un nuevo cliente en nuestro servidor OAuth.redirect_uri es la URI que esta preparada para recoger la respuesta de esta peticion.state es un valor que se usa para evitar ataques CSRF Cross Site Request Forgery. Una cadena unica aleatoria que debe ser devuelta por el servidor para poderlos comparar y ver que son iguales.scope podria ser openid para OIDC o cualquier otro para autorizar diferentes aplicaciones.La respuesta de esta peticion sera una llamada a la URI que le pasamos en redirect_uri con el siguiente formatohttpexampledomain.com    access_tokena_lot_of_characters_in_base_64    token_typeBearer    expires_in3600    statestring_as_statusDe tal forma que podremos comparar el valor de state y sacar la informacion del Access Token del resto de parametros.PasswordTambien conocido como Resource Owner Password Credentials este flujo de autorizacion es solo recomendable en entornos seguros donde existe una relacion de confianza entre el cliente y el servidor. Algo asi como un servicio del sistema operativo o una aplicacion que requiera permisos elevados. En resumen este deberia ser el ultimo flujo que deberiamos usar tan solo reservado para cuando no tenemos otra posibilidad.Se parece mucho a Client Credentials pero con la diferencia de que aqui vamos a solicitar la autorizacion autenticandonos como un usuario del sistema. De esta manera crearemos una peticion muy parecida al de ese modelo pero anadiendo ciertos campos adicionalesPOST oauthtoken HTTP1.1Host authorizationserver.comAccept applicationjsonContentType applicationxwwwformurlencodedContentLength ...grant_typepasswordusernameexampleuserpasswordexamplepasswordclient_idexample_client_idclient_secretexample_client_secretscopeuser.readgrant_type es password.username es el nombre del usuario que usariamos para identificarnos en el servidor.password es la contrasena del usuario que usariamos para identificarnos en el servidor.client_id es el identificador publico de la aplicacion. Una aplicacion es el resultado de registrar un nuevo cliente en nuestro servidor OAuth.client_secret es una contrasena o secreto que generaremos en el servidor de OAuth en relacion con el cliente la aplicacion.scope podria ser cualquier valor que ayude a autorizar el uso de nuestras aplicaciones.Y la respuesta sera el Access TokenHTTP1.1 200 OKCacheControl nostorePragma nocacheContentType applicationjsonContentLength ...  access_token a_lot_of_characters_in_base_64  token_type Bearer  expires_in 3600  scope user.readRefresh TokenCon el fin de que no siempre se esten transmitiendo los mismos datos algunos de ellos sensibles otro de los flujos que se nos proponen es el de Refresh Token. Para ello necesitaremos haber obtenido un Access Token y que este a parte de expiracion tenga el campo refresh_token indicado.Asi pues usando este campo despues de que el token anterior haya caducado podremos obtener otro token nuevo. Eso si un refresh_token tambien tiene una caducidad y a su vez es de un solo uso. De esta forma no podremos generar todos los tokens nuevos que queramos tan solo uno.Este metodo es muy sencillo realizaremos una peticion simple al servidor OAuth como la siguientePOST oauthtoken HTTP1.1Host authorizationserver.comAccept applicationjsonContentType applicationxwwwformurlencodedContentLength ...grant_typerefresh_tokenclient_idexample_client_idclient_secretexample_client_secretrefresh_tokena_lot_of_characters_in_base_64Dondegrant_type es refresh_token.client_id es el identificador publico de la aplicacion. Una aplicacion es el resultado de registrar un nuevo cliente en nuestro servidor OAuth. El mismo que usamos para obtener el anterior token.client_secret es una contrasena o secreto que generaremos en el servidor de OAuth en relacion con el cliente la aplicacion. El mismo que usamos para obtener el anterior token.refresh_token el token que recibimos en el Access Token anterior.Y la respuesta vuelve a ser semejante a las demasHTTP1.1 200 OKCacheControl nostorePragma nocacheContentType applicationjsonContentLength ...    access_token a_lot_of_characters_in_base_64    token_type Bearer    expires_in 3600    refresh_token a_lot_of_characters_in_base_64ConclusionesSi en otro articulo os explicabamos el tema de JWT y como validarlo para poder proteger nuestras APIs hoy nos hemos centrado en diferentes formas de autorizar autenticar un servicio basado en OAuth 2.0 y por supuesto recibir ese JWT.Estos no son todos los metodos concesiones o Grant Types que existen aunque si los mas usados.Cuando escribo sobre temas de seguridad es muy posible que me pierda en repeticiones y referencia a RFCs. Me resulta dificil dar razones de peso o resultar util sin usar ese formato. Quiza os resulte pesado. Pero lo cierto es que no se vosotros pero dentro de varios meses seguro que vuelvo a este articulo a buscar algun detalle que no recuerdo del todo...... y hasta aqui puedo leer."
    } ,
  
    {
      "title"    : "Autenticando una API con JWT",
      "category" : "",
      "tags"     : "oauth, csharp, jwt",
      "url"      : "/2019/03/12/jwt-api-authentication",
      "date"     : "2019-03-12 05:26:33Z",
      "content"  : "Uno de los grandes problemas de ser programador hoy en dia es que tenemos tantas librerias y tantas facilidades que resulta muy sencillo ignorar el funcionamiento interno de las cosas que utilizamos. Supongo que el caso de la autenticacion autorizacion de una API al ser un factor importante dentro de una aplicacion no sera uno de estos casos. No obstante y solo por prevenir vamos a describirlo guinoguino.Lo mas comun dentro de una API moderna es exponerla publicamente siguiendo o intentando seguir las premisas de REST y el modelo de madurez de Richardson. Asi pues usaremos el protocolo HTTP para realizar la comunicacion. Y la forma mas comun de solicitar autorizacion y autenticarse con este protocolo es utilizar las cabeceras de las peticiones. Concretamente se suele usar una llamada Authorization y como valor solemos encontrar dos textos uno que dice Bearer y otro indescifrable que suele coincidir con el token que hemos solicitado a otro servicio.BearerSeguro que mas de uno se ha preguntado por que tiene que poner Bearer delante del token. Vaya chorrada Podemos poner directamente el token y listo.Esta claro que somos libres de hacer lo que queramos. Por eso somos programadores. Por esa extrana y enfermiza adiccion que tenemos a la sensacion de poder hacer lo que queramos con una maquina. Esa sensacion de que de alguna manera somos los dioses de nuestro ordenador.Pero el mundo de los estandares la mantenibilidad y las APIs no esta hecho para personas diferentes. Es un mundo para que todos sigamos las mismas normas y mantengamos una relacion de simbiosis con el todo que al final haga desaparecer nuestra propia identidad pasando a formar parte de un conjunto mas extenso. Es decir que no reinventeis la rueda cono yaDentro del RFC del protocolo HTTP existen dos esquemas de autenticacionPrimero esta el Basic que es una castana. Basicamente guino consiste en coger el nombre de usuario la contrasena separarlo por el simbolo de dos puntos y pasarlo a base 64. Super seguro.Y segundo tenemos el Digest que para no liarnos es como una especie de token con varias propiedades firmas e incluso alguna cosilla medio encriptada pero que se envia en formato de texto plano. No esta mal pero con un maninthemiddle podemos comprometer informacion sensible de un usuario y del sistema facilmente.Luego ya hay otro tipo de modelos pero nosotros nos centraremos en el uso del estandar de autorizacion OAuth 2.0 que es Bearer. Un formato que nos permite la autorizacion en conjunto con la autenticacion de usuarios. Este es el esquema que esta mas de moda hoy en dia. Viene a avisar de que detras le acompana un token de tipo JSON Web Token.JWTUn JSON Web Token o JWT es un formato estandar compacto y seguro de trasmitir Claims propiedades afirmaciones o en general informacion entre diferentes sistemas.Su gran ventaja es que pueden ser validadas ya que vienen firmadas digitalmente con una clave privada que puede ser verificada usando una clave publica.Entre otras Claims podemos enviar desde la IP de la maquina para la que se ha emitido un JWT los ambitos a los que se le permite acceder hasta la fecha y hora de expiracion del mismo. De esta forma podremos conseguir comunicaciones mucho mas seguras.El formato de un JWT se basa en tres partesHeaderGeneralmente consiste en dos valoresEl algoritmo que se ha usado para firmar el token.El tipo de token. Que es JWT.    typ JWT    alg RS256PayloadEl cuerpo del mensaje esta compuesto por las Claims que se trasmiten. Existen tres tiposRegistered Claim Names que son datos acerca del registro.iss identifica al emisor del token.sub es el asunto que coincide con el identificador la persona que se identifica.aud la audiencia para la que se ha emitido.exp la hora de expiracion a partir de la cual el token no sera valido.nbf la hora hasta la que no sera aceptado un token su omision indica 0.iat la hora a la que fue emitido.jti el identificador del token.Public Claim Names son valores personalizados pero publicos. Pueden estar representados por una URL o por un nombre. Y para evitar colisiones se recomienda registrarlos en  IANA JSON Web Token Registry por ejemplo email given_name ....Private Claim Names Esto son campos que se acuerdan compartir entre las partes diferentes de los publicos.    aud httpsmycompany.commiapp    iss httpssts.windows.netcommon    sub asdasd34asf2332r23fea    iat 1552212046    nbf 1552212046    exp 1552215946    family_name Pil    given_name Paco    ipaddr 10.0.0.1    name Paco PilFirmaLa firma se realiza usando una clave privada digital. Es un proceso bastante simple si por ejemplo elegimos com algoritmo RSA256signature  RSA256    encodeURIbase64header      .      encodeURIbase64payload    private_keyEl token completoPara ponerlo todo junto usaremos el mismo formato que el firmado anadiendo la firmaheader   ... payload   ... content  encodeURIbase64header   .    encodeURIbase64payloadsignature  RSA256content private_keyJWT  content   .   encodeURIsignatureValidando JWTA la hora de validar vamos a usar como ejemplo un JWT emitido por un Azure Active Directory. Para ello lo primero que tendremos que hacer es validar la cabecera de la peticion HTTP. Comprobaremos que tiene una cabecera llamada Authorization y un valor que puede ser dividido en dos separandolo por un espacio vacio. El primero de esos valores debera ser Bearer y el segundo nuestro token.Despues deberemos saber de donde recuperar las claves publicas para comprobar la firma. Esto se puede hacer preguntando a la configuracion de openid connect que encontraremos enhttpslogin.windows.netnuestro_tenant_id.wellknownopenidconfigurationSi desconocemos cual es el tenantId que estamos usando lo podemos leer en el caso de Azure Active Directory del propio token. Se guarda en una propiedad llamada tid. Por lo que si cogemos el cuerpo del token lo convertimos a un formato JSON y buscamos esta propiedad ya tenemos el tenantId. Ahora solo tenemos que realizar esa peticion y de entre los diferentes datos que nos envia buscar un campo llamado jwks_uri. En esa direccion web encontraremos las claves publicas para comprobar la firma de nuestro token.La respuesta de esta ultima consulta tendremos que mirar en la propiedad keys y dentro de los objetos que contiene esta propiedad las claves se almacenan en formato de string en la propiedad x5c. Ahora bastaria con coger estas cadenas que vienen en base 64 y convertirlas a un formato de clave conocido por nuestro sistema. Generalmente se tratara de un certificado con solo una clave publica.Por ultimo utilizaremos las librerias mas conocidas para que valide nuestro JWT usando los certficados que hemos creado.Ahora vamos a ver 3 escenarios en donde resolverlo TypeScript .Net Core y Asp.Net Core.TypeScriptPara TypeScript vamos a usar un paquete npm llamado jsonwebtoken que nos ayudara a tratar con JWTs. La idea es recoger las claves publicas y usar este paquete para que valide si es un token correcto o noimport  as jsonwebtoken from jsonwebtokenimport axios from axiosexport class JwtAadValidator     constructorprivate readonly jwt string        public get tenantId string         return jsonwebtoken.decodethis.jwttid as string        public async verifyoptions any Promise         options  options         options.algorithms  RS256        options.issuer  httpssts.windows.net   this.tenantId           const certificates  await this.requestSigningCertificates        let lastError  null        for let i  0 i          const url  httpslogin.windows.net   this.tenantId   .wellknownopenidconfiguration        const result  await axios.geturl        return result.data.jwks_uri as string        private async requestSigningCertificates Promise         const url  await this.requestCertificateUrl        const result  await axios.geturl        const certificates string          result.data.keys.forEachpublicKeys              publicKeys.x5c.forEachcertificate                  certificates.pushthis.convertToCertificatecertificate                            return certificates        private convertToCertificatecert string string         const beginCert  BEGIN CERTIFICATE        const endCert  END CERTIFICATE        let result  beginCert        while cert.length  0             if cert.length  64                 result   n   cert.substring0 64                cert  cert.substring64 cert.length                        else                 result   n   cert                cert                              if resultresult.length   n             result   n                result   endCert   n        return result    .Net CoreCuando estamos usando la plataforma .Net podemos generar un codigo semejante al que hicimos en TypeScript pero en este caso vamos a usar los artefactos que nos proporciona la plataforma.En este caso las claves publicas hay que convertirlas en certificados X509 primero. Y la verificacion del token se realiza usando un artefacto del paquete System.IdentityModel.Tokens.Jwt llamado JwtSecurityTokenHandler. Este objeto tiene un comportamiento bastante simple se le pasa un token y unos parametros y valida.using Microsoft.IdentityModel.Tokensusing Newtonsoft.Jsonusing Newtonsoft.Json.Linqusing Systemusing System.Collections.Genericusing System.IdentityModel.Tokens.Jwtusing System.Linqusing System.Net.Httpusing System.Security.Cryptography.X509Certificatesusing System.Threading.Tasksnamespace Security    public class JwtAadValidator            private readonly string _jwt        public JwtAadValidatorstring token                    _jwt  token                public string TenantId                    get                            return new JwtSecurityToken_jwt.Claims.FirstOrDefaultx  x.Type  tid.Value                            public async Task Verify                    var validationParameter  new TokenValidationParameters                            RequireSignedTokens  true                ValidateAudience  false ojo para ser seguro se debe validar la audiencia                ValidIssuer  httpssts.windows.netTenantId                ValidateIssuer  true                ValidateIssuerSigningKey  true                ValidateLifetime  true                IssuerSigningKeys  await RequestSigningCertificates                        try                            var handler  new JwtSecurityTokenHandler                handler.ValidateToken_jwt validationParameter out var token                return true                        catch Exception ex                            throw ex                            private async Task RequestCertificateUrl                    var url  httpslogin.windows.netTenantId.wellknownopenidconfiguration            using var client  new HttpClient                            var response  await client.GetAsyncurl                var data  await response.Content.ReadAsStringAsync                var json  JsonConvert.DeserializeObjectdata                return jsonjwks_uri.Value                            private async Task RequestSigningCertificates                    var url  await RequestCertificateUrl            var result  new List            using var client  new HttpClient                            var response  await client.GetAsyncurl                var data  await response.Content.ReadAsStringAsync                var json  JsonConvert.DeserializeObjectdata                jsonkeys.Values.ToList.ForEachkey                                     keyx5c.Values.ToList.ForEachcert                                             result.AddConvertToCertificatecert                                                            return result                private SecurityKey ConvertToCertificatestring cert                    var c  new X509Certificate2Convert.FromBase64Stringcert            return new X509SecurityKeyc            Asp.Net CoreEste es el escenario mas simple ya que existe un middleware que podemos utilizar para autenticar JWT. Asi que solo tendremos que modificar el Startup.cs para que se parezca al siguientepublic class Startup    private const key  THIS IS USED TO SIGN AND VERIFY JWT TOKENS REPLACE IT WITH YOUR OWN SECRET IT CAN BE ANY STRING    public StartupIConfiguration configuration            Configuration  configuration        public IConfiguration Configuration  get     public void ConfigureServicesIServiceCollection services            services.AddMvc.SetCompatibilityVersionCompatibilityVersion.Version_2_2        services.AddAuthenticationx                     x.DefaultAuthenticateScheme  JwtBearerDefaults.AuthenticationScheme            x.DefaultChallengeScheme  JwtBearerDefaults.AuthenticationScheme                .AddJwtBearerx                     x.RequireHttpsMetadata  false            x.SaveToken  true            x.TokenValidationParameters  new TokenValidationParameters                            ValidateIssuerSigningKey  true                IssuerSigningKey  new SymmetricSecurityKeykey                ValidateIssuer  false  ojo para ser seguro se debe validar el issuew                ValidateAudience  false ojo para ser seguro se debe validar la audiencia                            public void ConfigureIApplicationBuilder app IHostingEnvironment env            app.UseAuthentication        app.UseMvc    ConclusionesEvidentemente los ejemplos estan incompletos no se usan caches no se validan campos importantes como la audiencia y en general no tratamos identidades solo el token. Pero hemos visto a fondo como funciona el mundo de la autenticacion y autorizacion con OAuth 2.0 para nuestras APIs. Ademas hemos demostrado los principios de como realizar estas autenticacion autorizacion nosotros mismos o como funcionan internamente otros sistemas que la realizan por nosotros.Y aunque ningun sistema es completamente seguro siempre esta bien conocer su funcionamiento interno para ser capaces de prevenir ataques y fallos de seguridad."
    } ,
  
    {
      "title"    : "Método Kanban con Azure DevOps (y 2)",
      "category" : "",
      "tags"     : "azure, devops, kanban, best-practices",
      "url"      : "/2019/03/05/azure-devops-kanban-ii",
      "date"     : "2019-03-05 07:58:14Z",
      "content"  : "Si ya tienes claro que es el Metodo Kanban pero no tienes del todo claro como aplicarlo en una metodologia si ya aplicas este metodo pero te interesan las experiencias de otros equipos o si por el contrario tu objetivo en la vida es demostrarle a todo el mundo lo equivocado que esta te va a encantar lo que viene a continuacion.Como ya comentamos en el articulo anterior llevo como cinco anos detras de la metodologia perfecta basada en el Metodo Kanban. Se que estoy persiguiendo una quimera que lo que hoy consideras un valor para tu cliente manana puede convertirse en un waste a eliminar del proceso. Soy consciente de que hay politicas propuestas por la mayoria que no les gustan a todos los miembros del equipo. Y por supuesto he sido el primero que cuando los tiempos aprietan se ha saltado el limite del WIP.La mejora continua consiste en aceptar que cometemos errores e intentar ponerles solucion. Por lo tanto lo que hoy vamos a repasar no es mas que un proceso sin terminar y ni mucho menos definitivo. Pero si andas perdido en el mundo del desarrollo agil usando el metodo Kanban puede servirte de punto de partida.Inicio del proyectoEl principio de todo es la idea el concepto de un nuevo proyecto.Los proyectos en los que suelo colaborar por lo general carecen de una figura de Product Owner involucrado en el proyecto mas alla de como mucho una reunion semanal. Asi que el facilitador tiene que realizar tareas de proxy con el Owner. Por eso podreis notar que el tono general de la explicacion del proceso es desde el punto de vista del facilitador.Lo primero que haremos sera crear un nuevo proyecto en Azure DevOps recordando seleccionar el proceso AgileUna vez tenemos una idea inicial empezamos a pensar en las caracteristicas que nos gustaria tener. Aqui es donde nosotros empezamos a usar el backlog a nivel de features. Iremos a la opcion del menu de la izquierda de Boards despues al submenu Backlogs y en el selector de arriba a la derecha elegiremos FeaturesAqui podremos hacer clic en el boton de New WorkItem y empezar a escribir las caracteristicas que consideremos. Este trabajo se suele realizar junto con el Product Owner y los Stakeholders o en el caso de ser un proyecto de servicio leyendo el alcance del pliego del RFP o de la oferta que se ha realizado.Es posible que algunas caracteristicas no se lleguen a desarrollar nunca de la forma que se planeaba inicialmente o incluso que surjan nuevas a mitad del desarrollo. Por eso el listado de features no es algo estatico en nuestro proyecto y podradebera ser revisado constantemente. Lo unico que tenemos que tener en cuenta es que al finalizar cada edicion lo ideal es ordenarlas por prioridad para nuestro negocio.PlaneandoTener un listado de caracteristicas generales nos va a ayudar a mantener el foco en las necesidades del cliente y en el objetivo del proyecto pero no es demasiado generico como para empezar a desarrollar. Por esta razon anadiremos unos nuevos WorkItems hijos de las features que en Azure DevOps son de tipo user story.Una user story en nuestro modelo pertenece a una feature y tiene una plantilla que a los no neofitos en el tema seguro que os suenaAs a type of user I want some goal so that some reason.La idea es que nosotros sustituyamos el texto entre corchetes por lo que queremos en realidad.Las features las escribimos en ingles al igual que las user stories bugs y todo lo que tenga que ver con el proyecto. La programacion tambien se realizara en ingles usando los mismos nombres y conceptos expuestos en los WorkItems. De esta forma conseguimos tener un lenguaje comun entre desarrolladores y usuarios Lenguaje Ubicuo.Una vez consideramos que tenemos una buena cantidad de user stories como para dos o tres semanas de trabajo podemos priorizarlas y clasificarlas. Para ello usaremos el board. En el selector de arriba a la derecha seleccionaremos Stories y despues presionaremos en el boton de la barra superior que se titula View as BoardPara clasificarlas usaremos el sistema de etiquetas que existe en Azure DevOps. Al hacer clic en una user story se abrira un formulario de edicion. En la parte superior podremos ver un boton que se llama Add tag. Al pulsarlo nos permitira anadir una o varias tags.Las etiquetas que usaremos no tienen por que ser las mismas entre diferentes proyectos. Las tiene que ir definiendo el equipo segun se vea que resultan una ayuda. Aunque una etiqueta que si que creamos en todos los proyectos es la de blocked y la usamos cuando una historia esta bloqueada.Lo bueno de las etiquetas es que podemos crear diferentes estilos de tarjeta en dependencia de que etiquetas tenga. Si nos dirigimos a la rueda dentada de arriba a la derecha se nos abrira un formulario con varias opciones de configuracion del tablero. En la seccion de Card la opcion Styles nos permitira anadir un color de fondo o poner por ejemplo el texto en negrita cuando se cumplan unas condiciones como por ejemplo que contenga el tag con nombre blockedUna vez hemos clasificado las historias podremos juntarnos de nuevo con el equipo para priorizarlas. Aqui la mision de nuestro facilitador es ordenar el backlog segun las prioridades del cliente. Aunque sera el equipo quien tendra la ultima palabra priorizando segun dependencias tecnicas. Por ejemplo podria ser que para el Product Owner lo mas importante sea mostrar unos datos en pantalla pero el equipo considere que para mostrar unos datos antes se deberian introducir en el sistema.La parte buena de priorizar es que nos hace pensar en detalles que quiza se nos hayan pasado. Podria ser que surgieran nuevas user stories.Por ultimo pero no por ello menos importante pasamos al tema de las estimaciones. La idea es no perder mucho tiempo con esto. Nosotros usamos los Story Points de una forma poco ortodoxa. La idea es llegar rapidamente a un acuerdo teniendo en cuenta que 1 es mas o menos medio dia de trabajo y 2 es un dia completo. Sin cartas ni nada. Todo muy a pelo. Uno dice una cifra y si nadie dice nada se queda asi.Solo he comentado dos cifras 1 y 2 pero en realidad puedes poner cualquiera. En nuestro caso intentamos que todo este en esas cifras. Si se pasa de 5 lo que seria aproximadamente mas de media semana de trabajo entonces la consideramos epica y la intentamos dividir en dos. De hecho puedes crear una norma que ponga el estilo de las tarjetas con mas puntos de historia de 5 y asi marcar visualmente las que hay que dividir. Tambien es verdad que si tienes una compleja que no sabes como dividirla no pasa nada por dejarla. Pero que no todas las historias sean asi.Puedes cambiar los puntos de historia pulsando abajo a la derecha de la tarjetaEjecutandoPara poder seguir el flujo correctamente es muy importante configurar las columnas del tablero y establecer el limite del WIPEn nuestro caso los nombres de las columnas y su distribucion nos parece muy bien como vienen por defecto como punto de partida. Aunque tal vez alguna persona considere que es interesante cambiar el de Active por In progress o de la columna Resolved por In review.El limite de WIP para la columna de Active o In progress lo establecemos al numero de miembros del equipo mas uno. Esto lo hacemos para que pueda existir una tarjeta bloqueada al mismo tiempo que estamos desarrollando otras. Pero esto es solo el valor inicial recomendable depende del propio equipo cambiarlo.El proceso diario podriamos decir que empieza con la Reunion Kanban por las mananas eso que vulgarmente llamamos daily como abreviacion de daily meeting. Esta reunion como consecuencia de que los miembros del equipo se encuentran en diferentes ubicaciones la realizamos via Teams. Y para darle mayor visibilidad una persona se encarga de compartir su escritorio donde tiene abierto el board. De esta forma si encontramos algo en el tablero que no esta actualizado se modifica durante el transcurso de la reunion.Cada integrante del equipo debera responder a tres sencillas cuestionesQue hizo desde la ultima reunionQue bloqueos se ha encontradoQue tenia pensado hacer hoyAl finalizar la daily suelen surgir otras pequenas reuniones o incluso nuevas historias de usuario para anadir al tablero. Y como siempre que se anade algo al tablero hay que volver a priorizarlo.Despues de estas reuniones empieza el desarrollo y aqui la idea es muy simple el miembro del equipo se autoasigna la primera tarjeta del backlog o la que haya acordado durante la daily la mueve a la columna Active o In progress y entonces crea una rama de codigo fuente a partir de la rama raiz.Para poner nombre a las ramas utilizaremos los siguientes formatosSi es una historia de usuario featureshort_descriptionSi es un bug hotfixshort_descriptionLa idea es usar descripciones cortas explicativas que el equipo entienda. Y aunque hay personas que prefieren usar el ID de la tarjeta en nuestro caso este se asocia a cada uno de los commits que se crean. Por lo que tenemos creada una politica para que todo commit este asociado al menos a un WorkItemLa rama la podremos crear directamente desde Azure DevOps. Para ello hay que dirigirse al menu de la izquierda y seleccionar la opcion Repos del submenu Branches buscar la rama principal en nuestro caso master darle a los tres puntos que aparecen al lado del nombre y elegir la opcion de New branchUna vez hemos terminado el desarrollo y hemos cumplido con el Definition of Done pasamos esta tarjeta a la columna Resolved o In review y creamos una Pull Request desde nuestra rama a la raiz. Para lo que hay que navegar al menu Repos al submenu Pull requests presionar el boton que dice New pull request elegir como destino la rama raiz y como origen la rama en la que se estaba desarrollandoY entonces hay que volver a comenzar con el proceso cogiendo la primera tarjeta del backlog o en su defecto la que se haya comentado durante la daily.Todas estas politicas que hemos ido mencionando las hacemos explicitas anotandolas en la casilla Definition of done de la edicion de columnas con formato markdownRevisandoEl caso mas comun que suelo encontrar en los proyectos que participo es que cuando se acaba la fase de desarrollo entonces solo queda revisar con el cliente. Pero hay equipos y proyectos en los que existen mas estados para las tarjetas y por tanto mas columnas en el tablero.Semanalmente programamos una reunion de realimentacion junto con el Product Owner y los stakeholders. Para esa reunion preparamos una Release del proyecto. Para ello solemos usar una rama llamada production un Pipeline para realizar una Build y una Release que despliega el proyecto en un entorno de desarrollo en Azure.Durante la reunion semanal realizamos una demo de la aplicacion en el entorno de desarrollo y vamos recorriendo todas las tarjetas marcadas como Resolved. De esta revision sacamos nuevas historias y seguro que algun bug. Para la redaccion de un bug usamos un formato fijoWhen steps to reproduce  It actual behavior And it should expected behaviorConforme vamos revisando movemos las tarjetas de Resolved a Done indicando que ha terminado el flujo. Y a la vez alimentamos el backlog con nuevas user stories o features que se van comentando durante la reunion.El entorno queda congelado hasta la semana siguiente por si algun stakeholder esta interesado en seguir probando la aplicacion.Para aprovechar la convocatoria al terminar con las tarjetas del tablero realizamos la reunion de planificacion de entrega. En ella decidimos la prioridad del backlog desde el punto de vista de negocio y estimamos muy a ojo hasta donde se podria llegar en la proxima revision.Para finalizar la fase de revision despues de realizar este combo de dos reuniones seguidas como si fuera una realizamos la reunion de realimentacion con el equipo. Alli comentamos el feedback del cliente dejamos que se comuniquen inquietudes proponemos mejoras al proceso y descartamos las actividades que pensamos que no funcionan. Todo en consenso.Todo este proceso termina donde empezo volviendo a la fase de planificacion priorizacion y estimacion del backlog. Donde si notamos algun cambio de planificacion nos pondremos en contacto con el Product Owner lo antes posible para informarle.Final del proyectoEn todo el proceso invitamos a cualquier persona del equipo a proponer funcionalidades a hacerse preguntas las preguntas las guardamos en el tablero con unos estilos propios y si se responden con historias de usuario las relacionamos a ir un poco mas alla. A veces al Product Owner pueden interesarle y darles maxima prioridad. Otras veces simplemente las deja para mas tarde. Y muy pocas veces terminan descartandose.Pero todo proyecto tiene un fin. Y esto no quiere decir que este terminado.Por la modalidad de desarrollo basada en el Metodo Kanban es mas que probable que tengamos historias de usuario sin empezar si quiera. Y tenemos que aceptar que es asi es como funciona. Las ideas son infinitas pero no asi el dinero que cuesta ejecutarlas.Aunque el producto final no sea perfecto si que habremos conseguido que sea todo lo que nuestro cliente necesita. Y en ese momento decidira cortar el flujo de desarrollo.Lo que si os puedo decir es que si observais detenidamente la grafica del Cumulative Flow podreis llegar a predecir cuando el proyecto se empieza a poner mas horizontal y por tanto hay una bajada de flujo del WIP. Lo que en muchas ocasiones puede avisarnos de una parada cercana.ResumenPodriamos resumir todo el proceso que hemos descrito con la siguiente graficaComo bien decia al principio es un proceso de desarrollo incompleto e imperfecto. Ni mucho menos puede cubrir la mejora continua de toda la empresa. Es un proceso para la ejecucion de procesos que hay que ir mejorando y adaptando a las necesidades del equipo iteracion a iteracion.La parte buena es que todo lo descrito en mi humilde opinion es una base solida para empezar a trabajar."
    } ,
  
    {
      "title"    : "Método Kanban con Azure DevOps",
      "category" : "",
      "tags"     : "azure, devops, kanban, best-practices",
      "url"      : "/2019/02/26/azure-devops-kanban",
      "date"     : "2019-02-26 06:35:21Z",
      "content"  : "Reservar un trozo de pared dividirlo en columnas con cinta aislante y ponerle postits de colores no es Kanban. A eso se le llama guarrear la oficina. Y la idea de este articulo es que aprendas a guarrear Azure DevOps y asi puedas dejar las paredes limpias. Todos te lo agradeceran.Vosotros direis y entonces scrum scrum scrum.... Tanto que se escucha a dia de hoy se puede implantar en muchos proyectos pero no siempre es el mejor proceso para implantar en un proyecto.Open IEBS  Como desarrollar proyectos de forma agil en YouTubeNo podriamos empezar a hablar de como aplicar el Metodo Kanban sin antes realizar un breve resumen de en que consiste.Metodo KanbanEn contra de los que muchos puedan creer Kanban no es una metodologia es solo una fraccion de una un metodo.El origen del Metodo Kanban esta en una visita que hizo David J. Anderson a los Jardines del Palacio Imperial de Tokio. Resulta que al entrar cada visitante recibe una tarjeta de plastico que debe entregar a la salida. De esta forma limitan el aforo y evitan aglomeraciones haciendo la experiencia mucho mas satisfactoria para todos.El Metodo Kanban a partir de ahora cuando escribamos Kanban nos referiremos al metodo Kanban tiene como objetivo ayudar a los equipos y empresas a adoptar una cultura de mejora continua. Inicialmente fue disenado para contextos de desarrollo de software pero hoy en dia ha evolucionado hasta hacerlo aplicable a cualquier ambito.Kanban se basa en el respeto entre los miembros de los equipos. Para conseguirlo nos propone trabajar juntos compartiendo la informacion abiertamente entendiendo todas las opiniones y llegando a acuerdos. Todo ello supeditado a la consecucion del objetivo del proyecto y manteniendo un flujo de entrega de valor continuo .Al mas puro estilo manifiesto agil se nos proponen unos principios a tener en cuentaPara la gestion del cambioEmpezar con lo que estes haciendo ahora. Entender y respetar los procesos y roles tal cual se desempenan en este momento.Acordar buscar la mejora a traves del cambio evolutivo. Una vez entendemos como funcionamos podemos proponer pequenos cambios que ayuden a mejorar y comprobar si realmente lo hacen.Fomentar el liderazgo en todos los niveles.Para el despliegue del servicioEntender y focalizarse en las necesidades y expectativas de tus clientes.Gestionar el trabajo dejar que la gente se autoorganice alrededor de las tareas.Evolucionar las politicas para mejorar los resultados hacia el cliente y del negocio.Crei que me habian decorado la oficina con post its pero ya vi que es el Backlog de un proyecto AgileItza Reyes en TwitterEl Metodo Kanban nos propone el uso del Sistema Kanban para gestionar el trabajo y la visibilidad del mismo. Este sistema tiene varias aplicaciones pero en el mundo del desarrollo de software lo solemos encontrar en su forma de tablero con postits.Practicando KanbanAzure DevOps la herramienta de ALM de Microsoft en la nube producto con el dudoso honor de haber sido nombrado de 5 formas diferentes a saber TFS Online Team Foundation Services Visual Studio Online Visual Studio Team Services y hoy en dia Azure DevOps nos resulta una aplicacion muy adecuada y totalmente integrada con los flujos valores y practicas de Kanban.Jira es una mierda Agile no funciona y si hicimos eso de los postits de Kanban pero daba mucho trabajo.Manu en TwitterTodo comienza a la hora de crear un proyecto nuevo. En el formulario elegimos como plantilla la que se llama Agile Despues de esto ya estaremos preparados para aplicar Kanban siguiendo sus practicasVisualizarUn tablero Kanban no es la unica forma de implementar el sistema Kanban aunque si que es una de las mas utilizadas. Azure DevOps nos provee de tantos tableros como necesitemos dentro de nuestro proyecto. Para ver el panel por defecto tan solo tendremos que navegar hasta elKanban no establece como debemos disenar nuestro tablero ni tampoco existe un diseno concreto para usar en el sistema. La herramienta de Microsoft nos propone por omision una clasificacion que seguro que a la mayoria de los proyectos de desarrollo les viene bien dividida en 4 columnas New Active Resolved y Closed. Pero puedes anadir o quitar columnas segun la necesidad. Para hacerlo tendremos que pulsar en la rueda dentada de la parte superior derecha dentro de los Settings dirigirnos a la seccion Board y dentro de esta seccion a Columns.Aqui se nos mostrara una herramienta que nos permite modificar el nombre de las columnas anadir nuevas o borrar las que no necesitemos.El diseno de la tarjetas tambien es importante. Puedes elegir que datos van a aparecer en cada tarjeta para que de un vistazo tengas la informacion importante. Tambien se les pueden dar colores en dependencia de normas que podremos configurar. Por ejemplo podriamos destacar una historia que estuviera bloqueada con una pequena configuracionNo existe ninguna norma estricta al configurar un tablero. Lo ideal es hacerlo de la forma que mas ayude al equipo. Si de buenas a primeras no conseguimos una forma optima de hacerlo no nos tenemos que preocupar dentro del las practicas de Kanban existe una para poder modificar lo que hemos configurado en un principio.Directamente cuando elegimos el tipo de proceso agil Azure DevOps nos va a mostrar el panel a nivel de User Stories y quedara en nuestra mano si deseamos usar otros paneles a otros niveles. Los niveles que nos va a permitir usar ordenados jerarquicamente son Epics Features y Stories. Las Tasks a diferencia de otro tipo de tableros quedan relegadas a ser pequenas anotaciones a nivel de una historia. Lo que tenemos que saber es que podemos configurar todos los niveles independientemente. El objetivo es tener una visibilidad clara y sencilla.Limitar WIPEl sistema al que vulgarmente que nos referimos como A Salto de Mata ASM es lo que denominamos una estrategia de produccion push. La idea detras de esto es realizar el trabajo cuando el cliente lo demanda. Esto provoca que muchas trabajos se queden a medio hacer y que otras caduquen y se cancelen. Tener trabajo no finalizado o parcialmente completado es un desperdicio waste de tiempo y por tanto de dinero y su consecuencia es dilatar los tiempos de entrega lead time tiempo desde que llega una peticion hasta que se entrega. Al final lo que conseguimos es que los clientes esten insatisfechos los trabajadores tambien y que el proyecto no progrese como debiera.Si como consecuencia de la observacion ponemos limites al trabajo que se puede realizar al mismo tiempo WIP conseguiremos mejorar los tiempos de entrega y la calidad con la que se hace. A esto le llamaremos usar una estrategia pull donde empezamos una actividad justo despues de haber terminado o cancelado la que teniamos en marcha antes.GoKanban explicando el wip caras rarunas pero creo que lo han entendido. Empezamos sin limitar y vamos madurando improvementPepe Vazquez Sanchez en TwitterEste concepto se basa en la Ley de Little donde veremos que la forma de entregar lo mas rapido posible es tener un numero de peticiones en curso WIP que sea igual a la capacidad del equipo throughput rendimiento por ejemplo las personas que trabajan en un proyecto. Por lo que podriamos decir que si hay 3 personas desarrollando tener mas de 3 actividades al mismo tiempo aumenta el tiempo de entrega y tener menos de 3 actividades tiene como consecuencia que algunas personas del equipo esten ociosas.Limitar el WIP en Azure DevOps es muy sencillo podremos hacerlo en el tablero en las columnas de trabajo quedan excluidas la inicial y la final en las opciones de Settings que vimos anteriormente. De esta forma en a la derecha del nombre de la columna nos apareceran dos numeros el primero indica el numero de tarjetas que hay en esa columna y el segundo el limite que hemos puesto. Si superamos ese limite el numero de tarjetas activas se pondra en rojo indicando visualmente que ahi existe un problema.Kanban es una palabra japonesa  que se traduce como letrero o valla publicitaria. Esto hace que algunas personas relacionen esta palabra con las tarjetas o postits que usamos en un tablero. En el caso del Metodo Kanban una traduccion que corresponderia mas con su uso seria senal. Las senales pull que veremos en nuestro tablero son los slots vacios esas posiciones de tarjeta que no estan ocupadas y que me senalan que ahi falta algo.Gestionar el flujo de trabajoLa idea es que el flujo de trabajo maximice la entrega de valor minimice el lead time y que sea lo mas predecible posible. Todo esto se consigue realizando mediciones y ajustando el proceso y el limite del WIP en base a las conclusiones que saquemos. Para ello trabajaremos el limite del WIP para que nuestro sistema de flujo no se convierta en un sistema por lotesAzure DevOps nos aporta varias herramientas para ayudarnos. Por un lado el propio tablero donde podremos ver si tenemos cuellos de botella momentos ociosos o bloqueos.Arriba a la izquierda de nuestro tablero y a la vista de todos encontraremos una grafica llamada Cumulative Flow. El objetivo es que en esta grafica quede representado un crecimiento y WIP constante. Si vemos que el WIP es muy pequeno es que no estamos trabajando tanto como podriamos. Si es muy grande es que estamos bloqueando tareas quiza una solucion sea limitar a un numero menor el WIP. Si dos areas se juntan es posible que indique bloqueo. Y si en lugar de ir hacia arriba las areas se quedan horizontales significa que estamos parados.Y para el calculo de la velocidad a parte de esta grafica podemos configurar otra llamada Velocity en la que nos dira los puntos de historia que hemos ido solucionando por iteracion. Muy util para poder predecir el tiempo de entrega y a su vez una metrica que nos ayudara a seguir el movimiento NoEstimates.Hasta el momento lo que he leido y oido de la idea de noestimates va de la mano de NoPlanear... y pues ya saben lo que opino de Cero Planeaciones. Asi que se va a la pila de todo el bullshit que se esta colando en el submundo del agilismo flacido.Ramiro Gonzalez en TwitterHacer explicitas las politicasDentro de Kanban se propone un proceso de flujo de trabajo con unas politicas o restricciones. Si bien es verdad que se propone que estas politicas sean escasas simples bien definidas que se deban cumplir siempre y se puedan modificar lo cierto es que por el mero hecho de empezar a trabajar ya tenemos unas politicas de serie. Por ejemplo el limite del WIP es una. Afortunadamente en el tablero de Azure DevOps estara visible para todo el equipo.Otra politica que podriamos tener es el Definition of Done DoD. Que determina cuando consideramos que una historia esta terminada. Este detalle se puede anadir en la edicion de columnas y en el tablero se mostrara como un icono de informacion en el titulo de la columna. Al pulsarlo aparecera el detalle de lo que configuramos.Otras politicas podemos representarlas en tarjetas que dejaremos fijas en el tablero usando esta funcionalidad de DoD podriamos usar la Wiki que viene asociada a nuestro proyecto o como un widget en el dashboard.Feedback LoopsUn metodo para la mejora continua sin retroalimentacion periodica a traves del feedback quedaria un poco extrano. Kanban nos propone siete cadencias u oportunidades de reunion para recoger feedback y alimentar el flujo de trabajo.Revision de la estrategia donde se definen y seleccionan los servicios a prestar y su direccion. p.r trimestralmente.Revision de las operaciones para maximizar la entrega de valor en consonancia con las necesidades de los clientes p.r mensualmente.Revision de los riesgos evaluar riesgos y tratar asuntos bloqueantes p.r mensualmente.Revision de la prestacion de servicio p.r quincenalmente.Reunion de realimentacion para tratar lo que ya se ha hecho y preparar que se va a hacer p.r semanalmente.La reunion de Kanban el equipo de desarrollo se reune de pie delante del tablero donde se intentan desbloquear asuntos y planificacion del trabajo p.r diaria.Reunion de planificacion de la entrega p.r segun periodicidad de la entrega.Si un proceso iterativo no converge a una raiz real es porq el problema sta mal el comando sta mal o simplemente la entropia disminuye xDFatima Ro en TwitterMejorar y evolucionarKanban es un metodo enfocado a la mejora continua e incremental. Hay que aprovechar las cadencias para mejorar constantemente la metodologia las formas de trabajar de comunicarse de enfrentarse al cambio ... todo el equipo a la vez.No hay que dar nada por establecido y hay que cuestionarse todo. De esta forma conseguiremos buen feedback. Y si conseguimos un buen feedback es mas facil ir adaptandose y mejorando.Eso si es importante saber que nunca se llegara a la perfeccion. Asi que lo mejor es focalizarse en la mejora.Como informatico en estos 20 anos he peleado mucho contra los procesos en papel la mejora continua y esas cosas pero sin saberlo . Ahora resulta que soy un experto en transformacion digital A dar charlas de cosas para ganar dinero Cuanta tonteriakinomakino en TwitterConclusionesHoy hemos comentado Kanban desde un punto de vista teorico y ademas hemos visto como Azure DevOps se perfila como una herramienta muy valida para el trabajo colaborativo sin ensuciar las paredes y para equipos distribuidos.Un buen repaso de un metodo que llevo ya unos 5 anos intentando implementar en mi trabajo diario de una forma correcta. La semana que viene os cuento como llevo..."
    } ,
  
    {
      "title"    : "Novedades de c# 8",
      "category" : "",
      "tags"     : "csharp, dotnet, novedades",
      "url"      : "/2019/02/19/csharp-8",
      "date"     : "2019-02-19 07:52:43Z",
      "content"  : "Que fuerte Que fuerte Que fuerte Que Visual Studio 2019 ya esta aqui Bueno casi. Ahora mismo tenemos disponible la version Preview 2.2. Promete ser la ostia de rapido. Tiene un monton de novedades para programar en python. Una nueva experiencia para tratar con WorkItems de Azure DevOps. Realmente tiene innumerables nuevas features a la medida del desarrollador moderno. Pero hoy solo hay una de ellas que nos interese C 8.0.Algunos lo estabais esperando a otros os la pela al viento vamos a poner nota a las nuevas caracteristicas algunas ya implementadas otras meras ideas desarrolladas durante las mas crueles resacas. Y de la misma forma que ya sucedio con la version anterior vamos a usar para ello la escala sexiloca.Las normas son las de siempre eje vertical es lo util que nos resulta y el eje horizontal la locura de su implementacion. El objetivo de cada caracteristica es estar por encima de la diagonal Vicky Mendoza xy para poder considerarse una buena caracteristica.Y esta vez las votaciones no seran producto de mi diarrea mental. Esta vez son estadisticas bien analizadas de una muestra de desarrolladores valida y contrastada. Durante la pasada NetCoreConf  de Barcelona mi estimado amigo David Gonzalo y un servidor tuvimos el placer de realizar una encuesta durante la charla Hot Crazy C que tuvimos el placer de presentar aqui podeis encontrar su articulo relacionado.No me enrollo mas Al lioHuelga decir que algunas de estas funcionalidades no llegaran y otras podrian llegar a publicarse de una forma diferente a la que aqui exponemos.Nullable reference typesEs por todos conocida la funcionalidad para conseguir que un objeto que no puede ser nulo como por ejemplo un int acepte valores nulos. Si anadimos un simbolo de interrogacion despues de declarar el tipo este sera nullable.int entero  null  errorint entero  null  okEsta nueva funcionalidad trata de identificar valores nulos en tipos que si que son nativamente nullables por ejemplo un string o cualquier otra clase que desarrollemos. La idea es que si vamos a asignarle valores nulos lo marquemos con el simbolo interrogante. De esta manera el compilador podra darnos warnings cuando gestionemos de una forma incorrecta el valor nulostring a  null  Warning Assignment of null to nonnullable reference typestring s  nullWriteLineThe string is s  Warning Possible null reference exceptionWriteLineThe string is s  null Es un poco locura tener que marcar como nullable un reference type pero no obstante anade bastante consistencia al lenguaje haciendo semejante la gestion de valores null tanto en el heap como en el stack.ValoracionUseful  6.0Crazy  3.5Async streamsLas palabras clave async y await aparecieron en la version 5.0. y desde entonces existe un problema para iterar con objetos IEnumerable de forma asincrona.async Task GetManyResultsAsync    var list  new List    int result  1    do         result  await GetOneAsync        list.Addresult     while result  0    return listAl final todo pasaba por materializar completamente todas las llamadas. La propuesta de anadir un tipo IEnumerable que sea asincrono al iterar es algo que viene fenomenal al lenguaje. La idea es que se pueda parar de materializar resultados en cualquier momento incluidas las llamadas asincronas.async IAsyncEnumerable GetManyResultsAsync    int result  1    do         result  await GetOneAsync        yield return result     while result  0En mi opinion es una funcionalidad muy util aunque posiblemente el dia del evento en el que los asistentes votaron no supe expresarlo de la forma correcta.ValoracionUseful  7.0Crazy  4.0Range and IndexLa idea detras de los rangos y los indices es copiar ciertas funcionalidades de otros lenguajes de programacion. Se centran en interactuar con listas o arrays de una forma mas sencilla.var people  new string     ola k ase c    ocho o ke haseSin usar Linq recogeremos una porcion de este array usando los indices y rangosUn indice es un numero que indica la posicion dentro de un conjunto de objetos. Si por ejemplo indicamos el indice 0 de people este sera Elena. Si indicamos 1 sera el ultimo elemento de nuestra coleccion Skywalker. De lo que deducimos que un indice con el acento circunflejo se resta al valor de Length de nuestro objeto.Un rango es la tupla de un indice de inicio y un indice de fin separados por dos puntos 0..1. Este ejemplo recogeria los elementos desde el ola hasta hase.Veamos la funcionalidad en accionforeach var p in people0..3 Console.Writep   ola k ase cforeach var p in people0..5 Console.Writep   ola k ase cforeach var p in people4 Console.Writep   ocho o ke haseforeach var p in people6.. Console.Writep   ke haseforeach var p in people.. Console.Writep   ola k ase c ocho o ke haseOtra funcionalidad que parece que va a ser muy interesante a la hora de ahorrarnos dependencias con Linq aunque lo de que un rango sin indices sea la propia coleccion de objetos al completo no deja de ser gracioso.ValoracionUseful  5.0Crazy  6.5Recursive patternsEl nombre de esta caracteristica puede resultar un poco confuso. En realidad hace referencia a Pattern Matching. La idea es que si partimos de una coleccion de objetos Team donde podemos encontrar objetos de tipo DeveloperIEnumerable Team  ...class Developer    public string Name  get set     public bool IsCrazy  get set Podemos iterar sobre la coleccion Team y en una sentencia if mapear y filtrar las propiedades de cada uno de los elementos. De tal forma que este codigoIEnumerable GetCrazyDevelopers    foreach var p in Team            if p is Developer  p.IsCrazy                    string name  p.Name            yield return name            Lo podriamos transformar en este otro codigoIEnumerable GetCrazyDevelopers    foreach var p in Team            if p is Developer  IsCrazy false Name string name             yield return name    A pesar de ser algo mas complejo de primeras tambien resulta mas compacto. Si te ha costado mas de medio minuto entenderlo es que tu tambien deberias votar el factor Crazy con un valor elevado.ValoracionUseful  5.0Crazy  7.5Switch expressionsEsta caracteristica es semejante a la anterior con la diferencia de que en este caso vamos a realizar los patrones en sentencias switch y de diferentes formas.Inicialmente partiremos de la clase Point que simplemente tendra dos propiedades X e Yclass Point    public int X  get set     public int Y  get set Y declaramos una variable de tipo object como una instancia de Pointobject o  new PointX 10 Y 5El nuevo bloque switch nos va a permitir prescindir de las palabras clave case y default de tal forma que resumiremos cada opcion como una condicion y el cuerpo de una funcion inlinereturn o switch    Point p when p.X  0  p.Y  0   origin    Point p                             p.X p.Y    _                                   unknownLos mas avispados habreis notado que el simbolo _ sustituye a la palabra clave default. Otros estareis alucinando con la palabra clave when pero os tengo que decir que eso es de C 7.0. Lo que todos habremos entendido es que se realiza un save casting del objeto o a Point y si se cumple en la primera linea se evaluan unas condiciones si estas condiciones no se cumplen vamos a la segunda linea y en cualquier otro caso devolvera unknown.Ahora vamos a complicarlo un poco mas y a prescindir de la palabra clave when y a usar un matching semejante al de la caracteristica anteriorreturn o switch    Point  X 0 Y 0           origin    Point  X var x Y var y   x y    _                             unknownPero y si te dijera que tambien podemos hacer Pattern Matching con la deconstruccion de un objeto Aqui pensaras que alguien se ha fumado hierba buena que es un puto genio o ambas cosas. Vamos a verloPrimero tendremos que tener un objeto deconstruible caracteristica de la version de C 7.0.class Point    public int X  get set     public int Y  get set     public void Deconstructout int x out int y            x  X        y  Y    Y ahora vamos a sustituir el codigo anterior por otro tipo de matching sin cambiar el comportamientoreturn o switch    Point0 0          origin    Pointvar x var y  x y    _                    unknownEspectacular noQuien no le ve utilidad es que esta ciego...ValoracionUseful  4.0Crazy  8.0Implicit constructorsQue seria de una nueva version de C en la que no hubiera algun syntax sugar. Esta caracteristica es el primero de ellos. Es muy simple y todos lo entenderemos y usaremos siempre. La idea es inferir el tipo del constructor y asi ahorrarnos tener que escribirlo veintisiete veces cuando estemos programandoPerson people     new Elena Nito del Bosque    new Armando Bronca Segura    new Dolores Cabeza Baja    new Aitor Tilla del BosqueSi no ves realmente la mejora piensa que antes despues de cada new pondriamos Person.ValoracionUseful  7.0Crazy  2.0Using declarationEl otro syntax sugar que se esta preparando para esta version de C es el de los bloques using. Actualmente si tenemos un objeto que queremos liberar de memoria despues de usarlo lo mas recomendable es que implemente la interfaz IDisposable y usarlo dentro de un bloque using. De forma que al terminar el bloque el objeto se prepara para ser recogido por el Garbage Collectorpublic void Patata    using var disposable  CreateDisposableargs              ...      disposable is disposed herePara los mas vagos a partir de C 8.0 ya no tendran que hacer bloques. Al poner un using cuando se acabe el ambito al finalizar una funcion o cualquier bloque de codigo de disposearapublic void Patata    using var disposable  CreateDisposableargs    ...  disposable is disposed hereUna funcionalidad que en dependencia de lo espagueti que sea el codigo puede dar muchos quebraderos de cabeza. No obstante que seamos malos programadores no hace mala una caracteristica del lenguaje.ValoracionUseful  6.0Crazy  2.5Default interfacesLlevamos anos hablando de esta funcionalidad. A mi juicio una aberracion programatica que podria ser sustituida por una solucion elegante y conocida como es la multiherencia de C  . No obstante Microsoft sigue en sus trece y quiere anadir implementacion en las interfacesinterface ILogger    void LogLogLevel level string message    void LogException ex  LogLogLevel.Error ex.ToString  New overloadclass ConsoleLogger  ILogger    public void LogLogLevel level string message  ...      LogException gets default implementationEn mi libro de programacion orientada a objetos un artefacto con una implementacion por defecto y otra que se debe implementar al heredar se llama clase abstracta no interfaz.Por otro lado otros lenguajes como Java ya traen esta funcionalidad y ObjectiveC tiene multiherencia por lo que podria ser muy interesante de cara a que Xamarin fuera todavia mejor de lo que es_unaizc_ fernandoescolar jjane90 muy importante para el interop de C con Java Swift y ObejctiveC Miguel de Icaza migueldeicaza 21 de diciembre de 2016Por esta razon creo que las votaciones con respecto esta caracteristica fueron mas moderadasValoracionUseful  4.5Crazy  5.5ConclusionesDespues del evento publicamos una pagina web con las estadisticas de las votaciones pero como no creo que la tengamos para siempre online vamos a hacer unas capturas a continuacionLas conclusiones son simples a la gente no le gusta el Pattern Matching El mundo se va a la mierda."
    } ,
  
    {
      "title"    : "Azure Functions con TypeScript",
      "category" : "",
      "tags"     : "azure-functions, azure, functions, typescript",
      "url"      : "/2019/02/12/azure-functions-typescript",
      "date"     : "2019-02-12 07:18:25Z",
      "content"  : "No os ha pasado alguna vez que al leer un articulo en lugar de leer los textos explicativos que su autor ha anadido vais directamente al contenido de los code snipets y vais copiando y pegando... No os ha pasado que ignorais la prosa que tanto tiempo ha costado escribir y solo leeis el codigo porque ya os resulta bastante auto explicativo...  Pues hoy es el dia en el que he entendido vuestras necesidades. Voy a poner codigo y reducir mis comentarios a la minima expresion.  .notes     cursor pointer    padding 10px 0    .notes span     textdecorationline underline    textdecorationstyle wavy    textdecorationcolor gray    .notes .tip       display block      position absolute      backgroundcolor 41444F      color fff      opacity 0      transform translate3dcalc10060px100px      transition transform .15s easeinout opacity .2s      width 70      padding 8px 15px      boxshadow 0 4px 8px 0 rgba0 0 0 0.2 0 6px 20px 0 rgba0 0 0 0.19    .notesactive .tip     opacity 1    transform translate3d000 rotate3d1110deg    TypeScript con Azure Functions            Cada dia que paso trabajando con TypeScript en un contexto de equipo pienso en lo afortunados que somos de no haber elegido JavaScript. Tener un lenguaje tipado no solo te sirve de red de seguridad tambien te ayuda a entender el codigo que han hecho los demas y poder interactuar con otros artefactos facilmente gracias al intellisense.              El problema es que cuando empezamos a trabajar con Azure Functions nos encontramos con que podemos desarrollar con los lenguajes C Java JavaScript o Python pero no con TypeScript. Asi que aprovechando que estoy un poco aburrido he decidido invertir un poco de mi preciado tiempo montando lo que llamariamos un boilerplate.        Nuevo proyecto usando la consola.            Para empezar a trabajar vamos a crear un nuevo proyecto. Para ello crearemos una carpeta nueva usando la consola en mi caso uso Cmder que usa el simbolo lambda antes de cada linea.       mkdir azurefunctionstypescriptboilerplate cd azurefunctionstypescriptboilerplate  Creamos el package.json.            Cuando creamos el archivo package.json usando npm el comando nos ira preguntando por el nombre del paquete la version datos sobre el autor repositorio de codigo y la licencia. Lo rellenamos.       npm init  Instalamos las Azure Functions Tools.            Microsoft ha publicado una herramienta de consola para hacer que la creacion de Functions sea mas sencilla. Para ello tendremos que instalarla como dependencia de desarrollo de nuestro proyecto o bien como herramienta global cambiando el parametro savedev por g.              Personalmente prefiero instalar todas las dependencias de un proyecto de forma local y no global ya que no todo el mundo tiene por que tener instalados los mismos paquetes npm ni tampoco creo que desarrollar un proyecto deba implicar instalar paquetes globales. Pero esto es tan solo una opinion cada uno es libre de instalar lo que considere de forma global.       npm install savedev azurefunctionscoretools  Configurando las Azure Functions Tools.            Si has elegido instalar localmente las tools para poder usarlas tendremos que cambiar el PATH actual anadiendo la carpeta node_modules.bin de nuestro directorio de trabajo. Este cambio solo sera efectivo durante la sesion de consola. Es decir que una vez cerrada la consola actual ya no existira este cambio y volveremos al PATH original del sistema.              Por otro lado si has instalado las tools de forma global este paso lo puedes ignorar.       set PATHPATHcurrrent_folder_fullpathnode_modules.bin  Creando el proyecto de Azure Functions en node.            Usando la tool de Microsoft que acabamos de instalar nos pedira que seleccionemos el lenguaje de programacion que vamos a usar. Ahi elegiremos node.       func initSelect a worker runtimedotnetnode                               Anadiendo paquetes de TypeScript.            Cuando decimos que no hay soporte para TypeScript en Azure Functions no quiere decir que no tengamos ya los archivos de tipos publicados. Asi que vamos a instalar el lenguaje TypeScript y los tipos necesarios para desarrollar Azure Functions.       npm install savedev typescript azurefunctions  Anadiendo paquetes de utilidades.            Con el fin de tener unos comandos rapidos para borrar y copiar archivos en nuestros scripts de npm vamos a anadir los paquetes rimraf y copyfiles. Ademas del paquete azurefunctionspack que nos ayuda a crear paquetes de funciones optimizados para funcionar en la plataforma de Azure Functions.       npm install savedev copyfiles rimraf azurefunctionspack  Ordenando el codigo.            Que no te enganen Ser ordenado es una virtud no un defecto. Si cada vez que abro un proyecto de TypeScript veo una carpeta llamada src donde se a priori que voy a encontrar el codigo fuente por que este proyecto no deberia tenerla Vamos al  lio       mkdir src mv host.json local.settings.json src  Abrimos Visual Studio Code.            Ahora necesitamos un editor de codigo para retocar y crear ciertos archivos de la configuracion. Para ello usaremos Visual Studio Code.       code .  Creamos el archivo tsconfig.json.            En la raiz de nuestra solucion crearemos un nuevo archivo tsconfig.json con la configuracion necesaria para transpilar el codigo TypeScript en codigo JavaScript compatible con el runtime de las Azure Functions. Lo mas importante es tener en cuenta que usaremos el sistema de modulos de commonjs module.export... y la version de ECMAScript de 2017 con compatibilidad con async y await.        compilerOptions     module commonjs    target es2017    lib domes2017    sourceMap true    allowJs false    moduleResolution node    experimentalDecorators true    rootDir .src    forceConsistentCasingInFileNames true    suppressImplicitAnyIndexErrors true    allowSyntheticDefaultImports true    strictNullChecksfalse    noImplicitAny false    downlevelIteration true    exclude     node_modules    Editamos el package.json.            Ahora vamos a anadir una serie de scripts al archivo package.json que nos van a ayudar en el dia a dia con nuestro proyecto              new creara una nueva Function en JavaScript usando las plantillas de la herramienta de Microsoft.        serve transpilara el codigo y lo ejecutara localmente.        build creara una nueva carpeta llamada dist en la que encontraremos los archivos necesarios para su publicacion en Azure.        clean borrara los transpilados y la carpeta dist.            scripts     new cd src  func new    serve tsc  cd src  func host start  rimraf .js .map    build tsc  funcpack pack .src copyToOutput  cd src  copyfiles host.json local.settings.json .csproj ..dist  cd .funcpack  copyfiles  ....dist  cd ..  rimraf .funcpack    clean rimraf dist src.js src.map    Creando la primera Function.            Con la ayuda de los scripts de npm que hemos creado vamos a crear una nueva Function. Para ello tendremos que volver a la consola y ejecutar el comando npm run new. Durante el proceso elegiremos HTTP trigger como desencadenador y por nombre introduciremos hello_world.       npm run newSelect a templateAzure Blob Storage triggerAzure Cosmos DB triggerDurable Functions activityDurable Functions HTTP starterDurable Functions orchestratorAzure Event Grid triggerAzure Event Hub triggerHTTP trigger                     Select a template HTTP triggerFunction name HttpTrigger hello_world  Renombrar index.js a index.ts.            Nuestro script ha creado una Function basica en JavaScript. Para convertirla a TypeScript lo primero sera cambiar la extension de .js a .ts y despues tendremos que tipar el codigo. Si eres un poco vago como yo puedes copiar el codigo expuesto a continuacion.      import  Context HttpRequest  from azurefunctionsexport default async function context Context req HttpRequest     context.logTypeScript HTTP trigger function processed a request.    if req.query.name  req.body  req.body.name         context.res               status 200  Defaults to 200             body Hello    req.query.name  req.body.name                else         context.res              status 400            body Please pass a name on the query string or in the request body              Probando el proyecto.            Para finalizar tendremos que probar que todo ha salido bien. Para ello lanzaremos en consola el comando npm run serve y veremos como se lanza un Hosting de Azure Functions y cerca del final de todos los logs nos parecera la URL de nuestra Function.       npm run serveHttp Functions        hello_world GETPOST httplocalhost7071apihello_world  Abrir la URL en un navegador cualquiera.            Si abrimos la URL en un navegador cualquier y le anadimos un parametro de QueryString llamado name obtendremos un saludo.      httplocalhost7071apihello_worldnameChiquitan Chiquititantantan  Hala A cascarla            Solo queda despedirnos no sin antes poneros el enlace al proyecto de GitHub donde hemos subido este boilerplate. Si ya lo se... os podiais haber ahorrado todo este articulo. Perdon.      httpsgithub.comfernandoescolarazurefunctionstypescriptboilerplateclic aqui para disfrutar de la prosa  function showAll     var elements  document.getElementsByClassNamenotes    whileelements.length  0       for var i  0 i "
    } ,
  
    {
      "title"    : "Historia de C#",
      "category" : "",
      "tags"     : "csharp, dotnet, history",
      "url"      : "/2019/02/05/historia-csharp",
      "date"     : "2019-02-05 05:08:11Z",
      "content"  : "Dicen que la historia la escriben siempre los vencedores. Aunque en mi opinion la historia la ha escrito siempre el que tenia un medio para hacerlo. Esta afirmacion puede demostrarse hoy en dia echando un vistazo a un medio al que todo el mundo tiene acceso internet. Una plataforma donde todos podemos escribir nuestra propia historia. Asi pues al igual que cualquier otro ignorante de la red os voy a dar mi version de la historia de C. Y ademas os aseguro que esta si cuenta lo que sucedio de verdad de la buena.La historia prohibida que nunca nadie se atrevio a contar comienza en una tierra muy lejana al otro lado del charco. Con una empresa llamada Sun Microsystems.Los origenesNuestra protagonista desarrollo una plataforma de desarrollo basada en virtualizar el lenguaje maquina. La maquina virtual conocida como Java Virtual Machine aka JVM era capaz de interpretar unos archivos llamados bytecode. Estos archivos eran el resultado de compilar programas realizados en un nuevo lenguaje de programacion de alto nivel llamado Java. Y para conseguir que esta maquina virtual interactuara con el hardware existente se disenaron un conjunto de librerias que recibieron el nombre de Java Runtime Environment aka JRE.El lenguaje de programacion Java empezo a tener exito. Y como la JVM y el JRE eran open source no tardaron en unirse 3rd parties a la carrera por desarrollar su propia maquina virtual privada. Con muchas mas funcionalidades que la original pero partiendo de las implementaciones open source. La mas conocida y extendida de estas era la implementacion propia de IBM la version de hoy en dia se conoce como J9. Pero otras empresas como HewlettPackard SAP Novell o Microsoft tambien implementaron las suyas.Entre tanto tambien se estaba desarrollando una dura batalla por implementar el Integrated Development Environment IDE preferido por los desarrolladores. Sun Microsystems decidio comprar a unos estudiantes universitarios el IDE Netbeans. Aunque esto en realidad fue una debil respuesta contra una serie de IDEs que eran muy superiores y ya estaban asentados en el mercado. JBuilder producto de Borland escrito en el propio lenguaje de Java era un producto muy avanzado y extendido. Mientras que Eclipse empezaba a mostrarse como la plataforma dominante open source de la mano de IBM.Todo este lio de maquinas virtuales versiones de Runtimes e IDEs creaba bastante confusion en los desarrolladores. Potenciados por el acceso a internet comenzo a ser mas sencillo compartir codigo fuente. Pero cada porcion del mismo necesitaba tener instalada una maquina virtual diferente.De cualquier forma a Sun Microsystems no parecio importarle esto. A menos no le importo hasta que Microsoft publico Visual J  .Una herramienta basada en Visual Studio de Microsoft junto con una maquina virtual de Java totalmente integrada con las librerias win32 de Windows. Anadiendo creacion de formularios nativos mediante un editor grafico semejante al que existia para Visual Basic que mas tarde nos encontrariamos con el nombre de Windows.Forms. Pero Microsoft se olvido de un pequeno detalle dejo de lado un tema explicito de la licencia de Java la compatibilidad entre las diferentes versiones.Porque es evidente que la maquina virtual de IBM si que era compatible con todo lo anterior. clarooo. pero bueno no vamos a entrar mucho mas en esta disputa...El caso es que a Sun Microsystems no le gusto la maniobra de Microsoft y decidio denunciar a la compania en 1997. En vista del largo proceso judicial que comenzaba en 1998 Microsoft empezo a desarrollar su propio lenguaje basado en premisas de Java. Este lenguaje era mucho mas cercano a otro con el que mucha gente se sentia comoda en esa epoca C  . Ademas correria sobre una nueva plataforma llamada .Net Framework.En 2001 hubo sentencia en contra de Microsoft y lo que paso a partir de 2002 ya es historia.El nombreHay varias fuentes que comentan diferentes versiones del origen del nombre de C. Asi que hemos elegido la que mas nos gusta.En un principio existia C. Cuando este lenguaje fue extendido para soportar el paradigma de la programacion orientada a objetos paso a llamarse C  .Para C se cogio como base C   intentando hacer un lenguaje totalmente orientado a objetos. Quiza suene muy atrevido denominarlo como una especie de C     . El caso es que al superponer las parejas de   se crea una especie de cuadrado. Si le echamos un poco de imaginacion este cuadrado se puede asemejar a una almohadilla . C.La evolucion2002Como deciamos antes 2002 fue el ano en el que pudimos ver la primera version de C la 1.0. Aqui se establecieron las bases del lenguaje el uso de clases structs interfaces modelo de herencia ciclo de vida etc. Tambien asistimos al nacimiento de Microsoft .Net Framework.Un ano mas tarde y con el fin de solucionar varios problemas tuvimos la version 1.2 del lenguaje C. No obstante lo mas importante de este ano fue la version 1.1 del .Net Framework que la acompanaba. Este Runtime por fin solucionaba los problemas de liberacion de memoria del Garbage Collector.Hasta aqui todo era muy parecido a Java. Basicamente si cambiabas los import por using java. por System. y las llamadas en pascal Case por Camel Case todo funcionaba practicamente igual. Exceptuando esa cosa llamada region. Un trucazo una chama un pragma del IDE que servia para ocultar cierto codigo que se autogeneraba al usar los disenadores visuales de formularios.2005Con la version 2.0 se empezo a vislumbrar lo que es hoy en dia el lenguaje gracias a los generics iteradores los metodos anonimos la covarianza y la contravarianza... Y no nos olvidemos de las clases parciales. Era demasiado evidente que eso de  region era un poco horrible. Asi que para dar solucion al codigo generado por los disenadores de formularios de Windows.Forms y WebForms en esta version se anadieron clases que podian ser definidas en varios archivos anadiendo la palabra clave partial.2007Este ano nos encontramos con el que sin duda fue el mayor avance del lenguaje. La version 3.0 es con la que se consiguio adelantar a Java. De hecho lo hizo por la derecha y le solto las chapitas. A partir de aqui desde Java se dieron cuenta de que tenian que empezar a copiar las nuevas caracteristicas de C. Los tipos anonimos junto con las Lambdas y el ExpresionTree nos trajeron Linq posiblemente la mejor utilidad que se ha disenado para tratar con iteraciones. Tambien se incluyeron algunos detalles esteticos o syntaxsugar como las autoproperties o la palabra clave var que intentaban dar mayor limpieza al codigo.Ademas por estas fechas como habiamos llegado al limite de calor con respecto al precio en los materiales de los procesadores se empezaron a poner de moda los multicore. Quedando obsoleto cualquier programa que simplemente utilizara un solo nucleo de nuestro procesador y dejando los demas en Idle. De cara a que esto no sucediera en el .Net Framework 3.5 se introdujo la Task Parallel Library que nos ayudaria a sacar provecho de estos nuevos procesadores.Tambien de esta version salio otra libreria llamada Reactive Extensions. Hoy por hoy se utiliza mas su port para javascript en React o Angular pero la libreria original era para C. Las Rx sacaban el mayor partido de la mezcla de eventos observables e iteraciones.2010Despues de unos anos tan interesantes en el mundo del desarrollo de Microsoft 2010 nos supo un poco descafeinado. Como un ano de transicion. Era como que tenian que sacar algo y decidieron llamarlo C 4.0. Con unos pocos detalles que se habian quedado en el tintero en versiones anteriores. Destacariamos la covarianza y contravarianza en generics los namedparams o los parametros opcionales.2012Dos anos mas tarde nos encontramos con una version que basicamente solo anadia el syntaxsugar mas grande que ha visto un lenguaje de programacion en la historia. De hecho luego fue copiado por javascript. C 5.0 introdujo async y await dos palabras clave tan utiles como peligrosas. Esta caracteristica esconde una maquina de estado en IL Intermediate Language el bytecode de .Net dentro de nuestros ensamblados. Y su funcionalidad es hacer que la programacion asincrona pueda realizarse de la misma forma que la secuencial acercando un modelo de programacion basado en eventos y callbacks a los pobres programadores que solo saben programar de forma secuencial.2015Buscando la excelencia de su lenguaje bandera Microsoft nos dio un monton de syntaxsugar nuevos. La 6.0 era una version tan dulce que casi nos dio diabetes. Lambdas para hacer metodos inicializadores de autoproperties nameof...2017La ultima gran actualizacion de C la version 7.0. Lo mas destacable de esta version es que vino de la mano del nuevo Runtime multi plataforma y open source de .Net dotnet core. Ademas de un monton de funcionalidades que venian inspiradas de un lenguaje funcional de .Net F. Pattern maching tuplas deconstruccion wildcards... Ahora C es un lenguaje orientado a objetos y funcional.2019Y para este ano lo que todo el mundo espera es la version 8.0. Pero eso ya es otra historia...ACTUALIZACION 09022019Despues de una serie de comentarios que me han llegado via Twitter me veo en la obligacion de realizar una actualizacion al articuloFe de erroresEn la seccion de 2007 se menciona la creacion de la Task Parallel Library TPL y de las Reactive Extensions Rx. Si bien es verdad que existen implementaciones de ambas para la version del Framework 3.5 no fue hasta 2010 y 2011 respectivamente cuando estuvieron disponibles publicamente. Originariamente aparecieron TPL para version del Framework 4.0 y Rx para 3.5 y 4.0. Dentro de las Rx para la version del Framework 3.5 existe un backport de TPL.La TPL no salio con .Net 4 Justo lo estuve mirando el otro dia para un proyecto en 3.5 que no se puede migrar y si me dices que hay forma de usar la TPL en 3.5 me das una alegria gorda gorda Adri mancku 8 de febrero de 2019Muchas gracias AdriNotas adicionalesComo bien indica Jose Manuel Alarcon por si habia alguna duda el bytecode no fue un invento de Sun Microsystem para la plataforma Java. Es un concepto que viene desde no se si incluso antes tiempos de BASIC y que de hecho Microsoft ya uso hasta en sus implementaciones modernas de Visual Basic.Con la salvedad de que VB clasico ya usaba bytecode un gran resumen de la historia de C y .Network del amigo Fernando. httpst.cooF1HzQZBwk JM Alarcon Aguin  jm_alarcon 5 de febrero de 2019Y otro aporte excelente por parte de Pablo Alvarez Doval trata sobre como Microsoft implemento .Net a partir de una plataforma desarrollada por Colusa Software Inc. Empresa fundada en 1994 y adquirida en 1996 que desarrollaba por aquel entonces una plataforma llamada Omniware con el fin de ejecutar codigo en CC   en una maquina virtual multiplataforma llamada OmniVM.Muy buen articulo pero yo le anadiria un detalle importantisimo para lo que fue .NET y C y muy poco conocido. .NET hereda directamente de OmniVM que fue la plataforma de Colusa Omniware que Microsoft adquirio y desarrollo evitando partir de cero  Pablo Alvarez Doval PabloDoval 8 de febrero de 2019Muchas gracias a ambos"
    } ,
  
    {
      "title"    : "Serverless API Rest",
      "category" : "",
      "tags"     : "azure-functions, azure, functions, dotnet",
      "url"      : "/2019/01/29/serverless-api-rest",
      "date"     : "2019-01-29 08:59:42Z",
      "content"  : "Serverless es un concepto nacido en la Nube. Su gran exito es ser una arquitectura para backend del lado del servidor Serverside que no tiene estado de ejecucion rapida y que responde a eventos. Literalmente se traduce como sin servidor. Y aqui es donde empieza el conflicto. Y es que serverless es de las tecnologias con nombre mas traicionero que existen. Se traduce como sin servidor. Pero en realidad se refiere a un tipo de arquitectura en la que el servidor no es importante para el desarrollador. Se ejecutan en entornos aislados como en contenedores especificos. Pero evidentemente estos entornos yo contenedores se ejecutan en uno o varios servidores.La forma mas conocida de programar serverless es usar las plataformas de Function as a Service FaaS. Este tipo de servicios esta totalmente gestionado por el proveedor cloud que utilicemos. Y se caracterizan por basarse en funciones como unidad de trabajo. Estas funcionesSeran independientes pequenas y basadas en una unidad logica.Podran recibir y devolver parametros.No tendran estado.Estaran disenadas para ser rapidas y efimeras con cada llamada se instancia todo lo necesario se ejecuta la funcion y libera todos los recursos de la memoria.Deberan ser escalables. Pudiendo tener tantas instancias como sean necesarias ejecutandose en el mismo momento. Incluso en paralelo.Sus desencadenadores seran eventos bien sea una peticion HTTP un evento en una base de datos la respuesta a un mensaje en una cola...Hoy en dia tenemos muchisimas variantes de estos servicios aunque los mas importantes son los que han implementado las Big Three de la nube Amazon Google y Microsoft. Y como todos sabemos de que pie cojeo pues vamos a hablar de la implementacion de Microsoft Azure Functions.Mi primera FunctionLa forma mas facil de programar para Azure Functions es usar un Visual Studio Enterprise o Community. Desde ahi crearemos un nuevo proyecto seleccionaremos como tipo Cloud y dentro de las diferentes propuestas de plantilla la denominada como Azure Functions.Entonces nos pedira cierta configuracion adicional para el proyecto. La primera sera seleccionar el Framework que en este caso nos hemos decantado por Azure Functions v2 .Net Core. Despues como vamos a implementar una API Rest hemos escogido Http trigger como desencadenador por defecto. Y finalmente en el nivel de acceso le hemos puesto Anonymous para que sea publica y no haga falta autenticarse.Si ahora ejecutamos el proyecto nos aparecera una consola donde se indica una URL donde se ha montado nuestra Function en la maquina local. Para lanzarla abriremos el navegador e introduciremos esa URL con el parametro name y un valor. En mi caso ha sido httplocalhost7071apiFunction1nameMaxPower.Esta muy bien tener resultados con solo 30 de clics en la pantalla . Os prometo que no os robare mucho mas tiempo implementando una API Rest.Azure Function APISi queremos una API es posible que necesitemos previamente un tipo de recurso que utilizar en la misma. En este caso hemos decidido usar un Todo para implementar una API de gestion de tareaspublic class Todo    public string Id  get set   Guid.NewGuid.ToStringn    public DateTime Created  get set   DateTime.UtcNow    public string Text  get set     public bool Done  get set Para almacenar este tipo de datos nos hemos decantado por un Sql Server. Por cuestiones de facilidad hemos anadido una referencia al paquete de Nuget de Dapper. De esta forma podremos usar sus caracteristicas de mapping con las conexiones con el motor de base de datos.Asi que hacer una funcion que lea de nuestra base datos y devuelva esos datos en formato API Rest sera un codigo tan simple como el siguientepublic static class TodoApi    FunctionNameTodo_Get    public static async Task SelectAsync        HttpTriggerAuthorizationLevel.Anonymous get Route  todos        HttpRequest req        ILogger log            var cnnString  my_connection_string        using var connection  new SqlConnectioncnnString                    connection.Open            var todos  await connection.QueryAsyncselect Id Created Text Done from dbo.Todos             QueryAsync is a Dapper function. It maps the result of the query in to a IEnumerable            if todos.Count  0 return new EmptyResult  status code 204 No Content            return new OkObjectResulttodos  status code 200 Ok   body  ...todos             Uno de los tipicos retos que vamos a encontrar usando Microsoft Azure es que nos da la posibilidad de usar settings de aplicacion configurados en el propio entorno de Azure. De esta forma las aplicaciones no tienen por que conocer los datos de conexion de por ejemplo una base de datos. Esa informacion se la proveera el propio entorno.En el caso de Azure Functions existen los Application Settings que pueden ir en archivos de configuracion json o tambien en forma de variables de entorno. Para poder recoger estos valores de cualquiera de las dos fuentes tendremos que instanciar un ConfigurationBuilder para asi poder crear un IConfigurationRoot de donde leer la informacion. A este fin crearemos la siguiente funcion que devolvera una cadena de conexion llamada DefaultConnection por defectoprivate static string GetConnectionStringILogger log ExecutionContext context    var config  new ConfigurationBuilder                    .SetBasePathcontext.FunctionAppDirectory                    .AddJsonFilelocal.settings.json optional true reloadOnChange true                    .AddEnvironmentVariables                    .Build    return config.GetConnectionStringDefaultConnectionDespues en el archivo del proyecto local.settings.json anadiremos una seccion nueva con las cadenas de conexion que usamos de forma localConnectionStrings     DefaultConnection my_connection_stringY finalmente modificaremos nuestro codigo para que la Function recupere el ExecutionContext anadiendolo como parametro y para que la cadena de conexion la resuelva usando el codigo que hemos vistopublic static class TodoApi    FunctionNameTodo_Get    public static async Task SelectAsync        HttpTriggerAuthorizationLevel.Anonymous get Route  todos        HttpRequest req        ILogger log        ExecutionContext context Added ExecutionContext parameter            var cnnString  GetConnectionStringlog context  call GetConnectionString.. method        using var connection  new SqlConnectioncnnString                    connection.Open            var todos  await connection.QueryAsyncselect Id Created Text Done from dbo.Todos            if todos.Count  0 return new EmptyResult            return new OkObjectResulttodos            Por ultimo y con el fin de conseguir tener una API completa tendremos que ser capaces de leer informacion de la peticion HTTP. Hay dos viasUrl.Path parameterPara leer un parametro que nos encontramos en la propia URL de la API lo primero que tendremos que hacer es anadirlo a la ruta de la funcion. Esto se especifica en el atributo HttpTrigger en la variable Route. De la misma forma que hariamos con Asp.Net indicando el nombre de la variable entre llavesHttpTriggerAuthorizationLevel.Anonymous get Route  todosidHttpRequest reqY para recogerlo simplemente anadimos el parametro a la funcionFunctionNameTodo_GetByIdpublic static async Task SelectByIdAsync    HttpTriggerAuthorizationLevel.Anonymous get Route  todosid  the id parameter in the Route    HttpRequest req    ILogger log    ExecutionContext context    string id the id parameter as a function parameter     ...Request.BodyPor otro lado si queremos recoger el valor del cuerpo de la peticion HTTP tendremos que usar un deserializador de json. En nuestro proyecto ya tendremos incluido el NewtonSoft.Json. Asi que deberiamos serializarlo leyendolo de la RequestFunctionNameTodo_Createpublic static async Task CreateAsync    HttpTriggerAuthorizationLevel.Anonymous post Route  todos    HttpRequest req    ILogger log    ExecutionContext context    var requestBody  await new StreamReaderreq.Body.ReadToEndAsync    var input  JsonConvert.DeserializeObjectrequestBody     ...API RestConociendo estos detalles ya no tendremos problemas en implementar una version completa de una API Rest basada en Azure Functions y con una base de datos hospedada en un Azure Sql Database. La podeis ver a continuacionpublic static class TodoApi    FunctionNameTodo_Get    public static async Task SelectAsync        HttpTriggerAuthorizationLevel.Anonymous get Route  todos        HttpRequest req        ILogger log        ExecutionContext context            var cnnString  GetConnectionStringlog context        using var connection  new SqlConnectioncnnString                    connection.Open            var todos  await connection.QueryAsyncselect Id Created Text Done from dbo.Todos            if todos.Count  0 return new EmptyResult            return new OkObjectResulttodos                FunctionNameTodo_GetById    public static async Task SelectByIdAsync        HttpTriggerAuthorizationLevel.Anonymous get Route  todosid        HttpRequest req        ILogger log        ExecutionContext context        string id            var cnnString  GetConnectionStringlog context        using var connection  new SqlConnectioncnnString                    connection.Open            var todos  await connection.QueryAsyncselect Id Created Text Done from dbo.Todos where Id  id new  id             if todos.Count  0 return new NotFoundResult            return new OkObjectResulttodos.First                FunctionNameTodo_Create    public static async Task CreateAsync        HttpTriggerAuthorizationLevel.Anonymous post Route  todos        HttpRequest req        ILogger log        ExecutionContext context            var requestBody  await new StreamReaderreq.Body.ReadToEndAsync        var input  JsonConvert.DeserializeObjectrequestBody        var cnnString  GetConnectionStringlog context        using var connection  new SqlConnectioncnnString                    connection.Open            await connection.ExecuteAsyncinsert into dbo.Todos Id Created Text Done values Id Created Text Done input            var location  req.Schemereq.Hostreq.Pathreq.QueryStringinput.Id            return new CreatedResultlocation input                FunctionNameTodo_Update    public static async Task UpdateAsync        HttpTriggerAuthorizationLevel.Anonymous put Route  todosid        HttpRequest req        ILogger log        ExecutionContext context        string id            var requestBody  await new StreamReaderreq.Body.ReadToEndAsync        var input  JsonConvert.DeserializeObjectrequestBody        var cnnString  GetConnectionStringlog context        using var connection  new SqlConnectioncnnString                    connection.Open            await connection.ExecuteAsyncupdate dbo.Todos set Text  Text Done  Done where Id  Id input            return new OkObjectResultinput                FunctionNameTodo_Delete    public static async Task DeleteAsync        HttpTriggerAuthorizationLevel.Anonymous delete Route  todosid        HttpRequest req        ILogger log        ExecutionContext context        string id            var cnnString  GetConnectionStringlog context        using var connection  new SqlConnectioncnnString                    connection.Open            await connection.ExecuteAsyncdelete from dbo.Todos where Id  id new   id             return new OkObjectResultid                private static string GetConnectionStringILogger log ExecutionContext context            var config  new ConfigurationBuilder                        .SetBasePathcontext.FunctionAppDirectory                        .AddJsonFilelocal.settings.json optional true reloadOnChange true                        .AddEnvironmentVariables                        .Build        return config.GetConnectionStringDefaultConnection    ConclusionesDecidir desarrollar una arquitectura basada en serverless va a proporcionarnos muchas ventajasUn coste de infraestructura basado solamente en el tiempo de ejecucion. Sin tener que administrar nada en absoluto.Servicios mas pequenos y mejor divididos. Siendo mas facil responder al cambio. Necesitando menos experiencia en complejas arquitecturas.Facilidad de automatizacion y menor timetomarket.Herramientas de monitorizacion outofthebox.Pero siempre tendremos que conocer ciertas limitacionesEs una tecnologia nueva y por tanto inmadura. Sin bestpractices claras.Al tener todo tan dividido se multiplica la posibilidad de crear bloqueos de recursos y se anade mayor dificultad a la hora de controlarlos.El equipo de desarrollo tiene que tener muy interiorizada la filosofia de serverless a la hora de desarrollar.No es una arquitectura valida para procesos con gran carga de CPU yo una larga duracion.La arquitectura no aporta la velocidad necesaria en aplicaciones de tiempo real.Asi que queda en los problemas y consideraciones de cada proyecto el aplicar este modelo o no."
    } ,
  
    {
      "title"    : "IoT con MxChip (I)",
      "category" : "",
      "tags"     : "",
      "url"      : "/2019/01/21/iot-con-mxchip-i",
      "date"     : "2019-01-21 14:02:53Z",
      "content"  : "Hoy empezamos una pequena serie sobre IoT usando un dispositivo MxChip y la plataforma Azure. Nada como empezar el ano escribiendo un nuevo articulo sobre una tecnologia de la que nunca antes lo habia hecho.Hola Soy Troy McClure y tal vez me recuerden de otros articulos en internet como Los decimales que el sistema robo a mi float o Por que Linq to Entities apesta.Senor McClureCaramba Hola Bobby...Soy Jimmy... Me gustaria saber como hacer chorradas de esas de IoT en Azure.Woow Woow Mas despacio Jimmy Eso es como querrer programar el alma...Si queremos graduarnos en la Universidad de las Cosas necesitaremos una Cosa que utilizar. La Thing mas recomendable es una placa llamada MxChip IoT DevKit.Esta placa ya viene certificada para trabajar con Azure IoT. Ademas tiene un monton de sensores incorporados humedad temperatura presion acelerometro giroscopio microfono... Las suelen regalar en eventos de Microsoft con tematicas de IoT. Pero siempre podremos adquirir una comprandola online.No es obligatorio usarla. Pero simplificara toda la interaccion con el hardware. De esta forma podremos centrarnos en lo interesante el desarrollo.Una vez tengamos una placa MxChip tendremos que empezar a trabajar con ellaConfigurar la wifiEnchufaremos el MxChip a una fuente de alimentacion por ejemplo un puerto USB de nuestro ordenador. Entonces mantendremos pulsado el boton B. Pulsaremos el boton reset y lo soltaremos. Despues soltaremos el boton BLa placa entrara en un modo especial en el que sirve una red wifi. Deberemos conectar nuestro ordenador con la red wifi que dice la pantalla de la placaY una vez conectado deberemos abrir un navegador de internet y dirigirnos a http mas la IP que aparece en la pantalla de la placaUna vez en esa pagina solo debemos configurar la Wifi a la que deseamos que se conecte la placa.Actualizar firmwareActualizar el firmware es de las operaciones mas simples que se pueden encontrar. Lo unico que hay que hacer es descargar la ultima version desde la pagina oficialhttpsaka.msdevkitprodfirmwarelatestDespues conectar el MxChip a nuestro ordenador usando el cable USB. De esta forma se montara una unidad de disco. Finalmente copiaremos el archivo del firmware en la unidad de disco de la placaAutomaticamente el dispositivo se actualizara usando ese archivo y se reiniciara.Preparar el entorno de desarrolloLa placa MxChip trae como microcontrolador una unidad SMT32 de arquitectura ARMv7M y un core CortexM3. Un modelo que podemos programar usando el amigable IDE de arduino.Aunque usaremos como herramienta principal Visual Studio Code vscode necesitaremos instalar previamente el IDE de arduino. Esto es porque todas las herramientas de vscode estan basadas en librerias de este IDE. Asi que nos dirigiremos a la web del IDE de arduino descargaremos el Windows installer y lo instalaremosDespues en vscode en las extensiones buscaremos una llamada Arduino desarrollada por Microsoft y la instalaremosAhora tendremos que configurar la extension de arduino para que use las librerias del IDE que instalamos al inicio. Nos dirigiremos al menu File  Preferences  Settings. Una vez ahi pulsaremos sobre el icono de editar el archivo settings.jsonY anadiremos dentro de las llaves el siguiente par de parejas clavevalorarduino.path CProgram Files x86Arduinoarduino.additionalUrls httpsraw.githubusercontent.comVSChinaazureiotdevkit_toolsmasterpackage_azureboard_index.jsonTambien instalaremos la extension Azure IoT Tools para tener instegracion con la plataforma en la nube para IoT de MicrosoftY por ultimo instalaremos un driver de windows que nos servira de interfaz USB para poder comunicarnos entre el MxChip y nuestra maquina STLinkV2. Lo podreis descargar despues de introducir vuestro email desde la pagina oficialhttpwww.st.comendevelopmenttoolsstswlink009.htmlUna vez hemos completado todos estos pasos desconectamos de nuestro ordenador el MxChip y reiniciamos vscode Ctrl  Shift   P y Reload Window. Al conectar de nuevo el MxChip con nuestro ordenador y estando vscode abierto veremos como se abre automaticamente una pagina con ejemplos de desarrolloLo que querra decir que ya estamos listos para programar el MxChip.Senor McClure pero esto es muy aburrido...ContinuaraNo te enganes Jimmy aunque este articulo haya sido un penazo es una informacion muy util para lo que puede venir.Tiene razon. Con esa afirmacion he demostrado que soy un autentico pardillo.Ha ha ha Si que lo eres Jimmy... si que lo eres..."
    } ,
  
    {
      "title"    : "GAB18 - Por qué todo lo que subo a azure está mal",
      "category" : "",
      "tags"     : "azure, webapp, appservice, cosmos",
      "url"      : "/video/2018/04/26/gab18-todo-subo-azure-esta-mal",
      "date"     : "2018-04-26 00:00:00Z",
      "content"  : "Azure es muy lento o Me voy a amazon son solo un par de las frases que puedes oir cuando ayudas a alguien con las migracion de su aplicacion onpremises que en la mayoria de los casos es del tipo onmylaptop a Microsoft Azure. Pero Que hay detras de este tipo de comentariosDespues de varios anos ayudando a toda clase de empresas con sus migraciones a la nube de Microsoft en serio no os miento un monton en esta charla daremos un repaso a los errores mas tipicos o al menos los que nos parecen mas graciosos que nos suelen comentar. Y por supuesto intentaremos proponer diferentes soluciones a los mismos.Aqui podras encontrar el video grabado de la sesion de la Global Azure Bootcamp Madrid 2018 sobre consejos generales para usar diferentes servicios de Microsoft Azure"
    } ,
  
    {
      "title"    : "Por qué todo lo que subo a azure está mal",
      "category" : "",
      "tags"     : "azure, webapp, appservice, cosmos",
      "url"      : "/2018/04/27/todo-subo-azure-esta-mal",
      "date"     : "2017-04-27 09:06:12Z",
      "content"  : "Hoy he venido contaros mis experiencias ayudando a diferentes empresas en la subida de sus aplicaciones a azure y concretamente a azure PaaS. Asi que en realidad no son mis experiencias son las de esas empresas. Y si soy totalmente sincero no son sus experiencias en realidad de lo que os voy a hablar es de sus quejas.Este documento corresponde con el guion realizado para dar la charla de Por que todo lo que subo a azure esta mal para la Global Azure Bootcamp de 2018. Si lo prefieres puedes ver el video de la charla en este enlace.Un poco de historiaDejad que os cuente una pequena historiaTodo empezo en 1981 cuando naci. Aqui en Madrid. Desde entonces he estado muy cerca de los ordenadores. La decada de los 90 mi padre me estuvo ensenando C  . Ahi estaba yo con Visual Studio y las Microsoft Foundation Classes. Sin entender muy bien exactamente por que se dibujaba un boton en la pantalla.El caso es que aunque mi vida es muy divertida en realidad lo que hoy nos interesa es la experiencia que puedo aportar en azure. Esto empezo en 2010 cuando aun lo llamabamos Windows Azure. El portal de gestion estaba hecho en una tecnologia llamada a petarlo en programacion web como es... era Silverlight. Ahi conocimos los storages y cloud services. Nos peleamos con el auto escalado usando Wasabi y en relidad todo era mucho mas dificil que ahora.Por ahi por el 2015 Microsoft nos propuso a Tokiota colaborar con ellos en unas jornadas que servian para ayudar a otras empresas a subir a sus aplicaciones a azure. Y ahi es cuando realmente empezamos a pelearnos y a aprender con retos de todo tipo que hoy veremos.El camino a la nubeSupongo que todos los aqui presentes habran hecho alguno de los tutoriales de la extensa y a la vez excelsa documentacion que Microsoft tiene a nuestra disposicion en internet. Todo funciona genial. Creo que la primera vez que escuche eso de azure nos obliga a programar bien fue a Gisela Torres. En una charla no muy diferente de esta. Y es verdad. Esto esta genial. Pero no cuando ya tienes una aplicacion desarrollada que no ha sido pensada para ser subida a la nube la cosa cambia.Asi que decidimos hacer un ABC. Una especie de metodologia guia o industrializacion de las migraciones que estabamos realizando. Y lo llamamos el camino a la nube. Lo simplificamos de tal manera que lo resumimos en 4 sencillos pasos.Basicamente se trataba de conseguir que las aplicaciones de estas empresas fueran escalables horizontalmente. Asi que nos fijamos solo en los recursosLos archivos que se generan o que se suben a esta aplicacion ya no pueden estar en el propio file system del servidor que la contiene. Al poder tener muchas instancias hay que almacenarlos en una unidad compartida. Por ejemplo un blob storage.Las variables de sesion. Si ya se que aqui todos programamos servicios stateless. Pero esto no ha sido asi siempre. A lo que iba Las variables de sesion deberian estar en un almacen compartido y que sea rapido. Como por ejemplo un redis cache.Y esto nos lleva al uso de una cache. Si nuestra aplicacion con el fin de tener los datos de una forma mas dinamica usa una memoria cache esta tiene que ser provista por un servicio externo y compartido. Como por ejemplo el ya mencionado azure redis cache.Para terminar nos teniamos que fijar en la base de datos. Hay ciertas incompatibilidades con respecto un sql server completo a uno en la nube. Y afortunadamente disponemos de varias herramientas que nos pueden ayudar con esta tares.Si teniamos resueltos estos casos ya estabamos listos para poder subir a la nube... o noEmpiezan los problemasNuestro camino a la nube se habia visto muy eficaz en la migracion de muchas clases de WebApps pero fue justo entonces cuando comenzaron los problemas. Las companias a las que ayudabamos estaban poniendo sus aplicaciones en produccion en la nube estaban empezando a investigar por su cuenta y a tomarse licencias fuera de lo que nosotros considerabamos como una mera prueba de concepto. Un pequeno empujon que les ayudaria a usar los servicios PaaS de azure.Nuestros desarrollos internos funcionaban genial. Es mas en Tokiota no tenemos servidores usamos azure. Y estabamos muy contentos con la calidad del servicio.Pero para las empresas que ayudabamos la cosa no estaba yendo bien. Quiza nos habiamos confundido quiza azure no era tan bueno es posible que tuvieramos que revisar todo nuestro proceso y prestar mas atencion a los detallesPosiblemente si. Y es aqui donde surge el enfado. El enfado lleva al odio. Y al final desemboca en una fase de apatia. no en el lado oscuro PPara evitar este devenir de los sentimientos teniamos un plan. Apuntamos lo que nos decian esas empresas y debajo anadiamos nuestras notas sobre la solucion. Al documento resultando lo llamamos Mis Notas. Asi todos los consultores lo considerarian suyo. Podrian modificarlo. Anadir o borrar nueva informacion.Y lo que hoy os traigo son lagunas de las notas mas destacadas de Mis NotasEN MI MAQUINA VA GENIAL PERO CUANDO SUBO LO MISMO A AZURE VA SUPER LENTOEsta frase es la sustitucion del mitico Works in my machine o En mi maquina funciona. Y jamas he puesto en duda semejante afirmacion. Pero claro si mi portatil es un i7 con 16 Gb de RAM y 512 Gb de SSD y en azure me decanto por una maquina con 1 core y 175Gb de RAM y HDD virtual... Y eso sin contar con el tema de que en mi maquina estoy lanzando un servidor de desarrollo al que solo accedo yo como podria ser IIS Express mientras que en azure estan entrando tropecientos usuarios a la vez.Aunque en realidad nosotros en nuestras aplicaciones con 100 usuario concurrentes y una instancia de las mas pequenas no hemos encontrado problemas...De cualquier forma uno de los puntos que siempre hay que revisar en azure es que todos los servicios de usemos en una aplicacion esten hospedados en la misma region. En el mismo data center. Primero por los temas de performance pero segundo porque ademas el trafico de azure a azure en el mismo datacenter no nos lo cobran.Otro detalle que es importante es usar al menos instancias estandar o S en nuestros servicios de produccion. Y a partir de estos tamanos buscar el mas adecuado para hospedar nuestro servicio. A parte de los tipicos valores de CPU RAM y disco tambien vienen asociadas con el tamano de una instancia otros factores como el numero de conexiones las IOs... que son muy importantes tambien.Y el ultimo consejo en general para usar cualquier servicio de azure es realizar siempre procesos cortos. Si por A o por B tenemos un proceso que nos va a llevar mucho tiempo y cuando digo mucho es mas de 1 segundo lo mejor que podemos hacer es distribuirlo y ejecutarlo en los servicios adecuados para estas tareas como por ejemplo WebJobs Azure Functions o si tenemos algo grande de verdad un azure batch.Si AAD ya tiene gestor de usuarios para que hacer el mio dentro de mi aplicacionNo cometais este grabe error. Os perseguira el resto de vuestra vida laboral.Lo primero que tenemos que hacer es conocer que es lo que hace AAD cuando delegamos la autenticacion y autorizacion de usuarios. Autenticacion es ese proceso en el que validamos que un usuario es en realidad quien dice ser. Autorizacion es cuando decidimos si un usuario tiene permisos para ver un recurso en concreto nuestra webapp.Si dejamos que nuestro gestor de usuarios sea el AAD que pasa si borramos un usuario Perderiamos todos sus datos como no podemos hacer login con ese usuario queda en un estado de limbo dentro de nuestra aplicacion o como va esoSiempre recomendare que la autenticacion la realice AAD por nosotros. E incluso el tema de autorizacion para entrar en nuestra pagina web. Pero el tema de la autorizacion sobre datos concretos de nuestra aplicacion tenemos que gestionarla nosotros.Asi que os recomiendoAlmacenar los usuarios de AAD en nuestra aplicacion. Incluso podriamos decidir crear un usuario si no existe en el login.Este usuario relacionarlo con el usuario del AAD via el UPN. Pero que tenga su propio ID.Almacenar los permisos dentro de la aplicacion si es que son necesarios en nuestra aplicacion y relacionados con el usuario de nuestra aplicacion.Si necesitamos relacionar algun dato con un usuario no usar el UPN mas que para el propio usuario. En el resto de datos relacionarlo con los usuarios internos de la aplicacion.EL SERVICIO DE AZURE REDIS CACHE ES MUY LENTO...Redis es de las bases de datos relacionales mas rapidas que existen. Trabaja en memoria lo que la hace ideal para comportarse como un servicio de cache. Asi que si redis funciona lento lo mas razonable es que el problema no sea de redis.Lo primero que debes hacer es activar desde el portal el Redis cache advisor un asistenteherramienta que detectara y avisara sobre los defectos tipicos que podamos tener a la hora de gestionar redis.Por mi experiencia los problemas suelen estar en almacenar valores muy grandes dentro de una sola key. Algo muy comun cuando heredamos una aplicacion que se programo usando variables de sesion en memoria local. Aunque redis soporta sin problemas valores bastante grandes si queremos sacarle partido a sus caracteristicas lo mejor es almacenar muchas keys con valores mas pequenos. Si necesitamos recoger y pre filtrar varias de esas opciones en una sola consulta redis nos puede ayudar dandonos muestras de lo realmente rapido que es.Igual que nos sucede con los demas servicios de azure pero en redis lo miramos menos porque es un poco mas caro hay que conocer los detalles de limites de conexiones concurrentes en cada tamano de instancia de redis. Puede ser que los problemas que experimentamos de rendimiento se deban a que tenemos muchas mas conexiones de las que somos capaces de gestionar y esto hace que se vayan creando nuevas conexiones cada poco. Lo cual enlentece mucho la experiencia.Como nota aunque nada recomendable os puedo decir que el cifrado SSL le sienta especialmente mal al protocolo que usa redis para comunicarse. Asi que si teneis redis en dentro de una vnet y sin acceso desde otro lugar que no sea esa vnet podeis desactivar el SSL y ver como se incrementa la velocidad.Y por ultimo sobre todo para aquellos que trabajais con lenguajes como PHP o nodejs existe un gateway simulado dentro de los appservices para poder usar redis usando el protocolo de memcached. Y es realmente eficiente si solo queremos usarlo como cache. Aunque asi nos perderiamos las otras grandes ventajas que tiene redis...ESTOY MUY CONTENTO CON EL SERVICIO DE COSMOS DB PERO ES CARISIMOQue entendemos como caroEsta frase es muy frecuente y es porque dentro de Cosmos DB una coleccion es tratada por asi decirlo como una base de datos. Las colecciones se facturan por separado y la coleccion minima que podemos contratar es de 400 RU. Teniendo en cuenta que el precio por RU es mas o menos 5  por cada 100 el precio minimo de una coleccion es de unos 20 .Si a esto le sumamos que cada entidad como si fueran tablas de entity framework la guardamos en una coleccion diferente y tenemos por decir algo 18 entidades serian unos 300  al mes con el anadido de que estamos infra usando 7200 RUs.Si lo vemos asi no es tan caro. Asi que os proponemos 3 ideasLa primera es que una coleccion de cosmos DB almacena documentos sin esquema. Eso se traduce en que podemos tener documentos con diferentes propiedades y por tanto diferentes tipos de documento en una sola coleccion.La segunda es que las colecciones se pueden particionar por algun valor de alguna propiedad. Si por ejemplo creamos en todas nuestras entidades una propiedad que sea el tipo de entidad como un string podriamos particionar fisicamente los datos para que fuera mas eficiente acceder a ellos.Y la tercera es que no penseis en tablas cuando useis cosmos DB pensad en documentos. Y si no usad mejor sql database.Como bola extra os comentare que cuando usamos el cliente de .net lo mejor es usar las llamadas asincronas y la conexion hacerla por TCP y activando el direct mode.SE ME BLOQUEA LA BASE DE DATOS EN LOS MOMENTOS DE MAYOR USO Y YA NO PUEDO ESCALAR MASCon el servicio de SQL database nos hemos encontrado un festival de problemas de performance. Aunque generalmente se debe a que la gente esta acostumbrada al cluster de su empresa de SQL Server que tiene 3 servidores con 64Gb de RAM las cabinas de la leche que se conectan con fibra y bueno la factura que da miedo.Estos servidores suelen ocultar las operaciones que tienen una performance fatal. Asi que cuando vamos al servicio PaaS de azure de base de datos como usamos instancias pequenas se hacen evidentes. Pero no os estoy diciendo que useis tamanos mas grandes de SQL Database. Creo que sale mas rentable invertir en mejorar nuestra queries.La primera recomendacion cuando usamos SQL Database es que migremos usando los asistentes de SSMS.Lo segundo que debemos hacer es activar el SQL Database Advisor que al igual que el de redis nos va a avisar de aquellos defectos tipicos que encuentre. E incluso nos propondra realizar modificaciones en el esquema de la base de datos. Como por ejemplo proponiendonos indices nuevos.Supongo que no soy el primero en decirlo pero no almaceneis la logica de negocio embebida en la base de datos en forma de stored procedures con transacciones. Si estos procesos son muy largos al final lo que vais a conseguir es bloquear muchas tablas y como consecuencia a todos los demas usuarios que acceden a la web.Usad la herramienta que nos proporciona el portal de azure query performance insight para poder medir la eficiencia de vuestras queries.Para saber el tamano de instancia que necesitamos en azure como se usa el termino DTU para ello tenemos un DTU calculator online que nos puede ayudar.CLARO COMO AZURE ES DE MICROSOFT LAS APLICACIONES DESARROLLADAS EN NODE VAN FATALAhora empezamos con la ronda de mentiras. Nodejs funciona en los application services genial. Pero si que es verdad que tenemos que tener en cuenta que si usamos instancias con windows nodejs se usa mediante el modulo de IIS llamado IISNODE. Y este modulo tiene ciertas peculiaridadesLos node cluster haciendo master y slave no funcionan a traves del IISNODE. A cambio hay unos parametros de configuracion que se le pueden pasar en el web.config o en un fichero a parte llamado iisnode.yml donde ademas de muchas otras cosas podremos configurar cuantas instancias de node queremos que lance el IISNODE.Si vamos a usar sockets tenemos que activarlo en la configuracion del portal. Si no simplemente se bloquean.Si no tenemos buena performance podemos probar a usar el paquete de npm llamado v8profiler que se integra con chrome y bueno nos puede resolver muchas dudas sobre donde estamos teniendo problemas.MICROSOFT HACE QUE LAS APLICACIONES DE PHP SE ARRASTREN PARA QUE PROGRAMES EN .NETSi me conoceis sabreis que soy programador fundamentalmente de .net no de PHP ni Java. Asi que en estos puntos no os podre dar tantos consejos. Pero si que tengo algunos sobre las aplicaciones en PHP como por ejemplo wordpressEl app services windows el archivo php.ini se sustituye por el .user.ini. Jugad ahi con las diferentes configuraciones generales de PHP.Podemos anadir extensiones a PHP en forma de binarios subiendolos a una carpeta junto con nuestra aplicacion e indicando que las cargue bien por un parametro de configuracion del servicio o bien en el archivo .ini. Asi que ya no hay escusas para no usar memcached .Usad siempre la version mas alta de PHP. Sobre todo a partir de la 7 donde mejora muchisimo la performace en comparacion con por ejemplo la 4.X.Y por ultimo hay pequenos trucos mezclando memcached con IIS y con las caches que mediante configuraciones que podreis encontrar en internet nos ayudaran a que haya paginas de php que sean guardadas en la cache y que la velocidad de nuestra aplicacion se incremente muchisimo.FUNCIONA TOMCAT EN AZURE BUENO YO TENGO MI PROPIO TOMCAT CUSTOMIZADO...Aun recuerdo la primera vez que una empresa vino con su Tomcat customizado. Y las que les han seguido. Tomcat funciona en App Services. Y puedes subir el tuyo customizado. Lo mas recomendable es buscar en la galeria una Web App con la ultima version de Tomcat. Este template se descarga el tomcat desde los servidores oficiales a la WebApp. Una vez ahi solo tienes que sustituirlo por el tuyo. Aunque es muy posible que le tengas que hacer retoques. No es lo mismo un tomcat en tu datacenter que en azure...A parte de las tipicas recomendaciones de cuidado con el tamano de la instancia y el usa caches alli donde puedas nos encontramos tambien una pequena curiosidadUsando Hibernate el conocido ORM de java tienes que garantizar que todas las cadenas de tu base de datos sean nvarchar y no varchar a secas. Resulta que Hibernate trabaja con caracteres unicode si o si asi que si no usas nvarchar los convierte. Esto hace que la performance caiga en picado.ES QUE AZURE ES MUY CAROY hemos llegado a la penultima frase que mas hemos oido migrando aplicaciones a azure es muy caro.Azure no es valido para todo el mundo. Si una empresa tiene un pequeno aplicativo en un hosting de 100 al ano que es muy lento y no tiene SLAs azure le va a resultar muy caro. Posiblemente la plataforma no sea para ellos.Pero si tenemos en cuenta que 24x7 los SLAs de 99959 de disponibilidad que por ejemplo 1Tb con sus 3 replicas solo cuestan 16  al mes... Creo que ya lo hemos dicho todo...SI no me lo arregla Microsoft ME VOY A IR A AMAZONY por ultimo el ganador. La persona que como no esta contenta con su aplicacion te suelta una amenaza. A mi ni me va ni me viene. Yo no trabajo en Microsoft. Pero la gente te lo dice igualmente.Mi respuesta se divide en dos puntosAWS es una gran plataforma cloud. En algunos aspectos es muy posible que sea superior a Azure. Y en otros sera Azure quien es superior. En el tema de PaaS a mi personalmente me gusta mucho azure y la forma de exponer sus precios. Pero es valido tener AWS e incluso mas de un proveedor de cloud. La diversificacion es buena para una empresa. No atarse...Y por ultimo si en Azure te funciona mal... en amazon por el mismo precio no lo va a hacer mejor.En resumenAsi que intentando responder a la pregunta retorica que formulabamos al inicio de la sesion Por que todo lo que subo a azure esta malPues posiblemente porque esta mal.Y encima la culpa es mia."
    } ,
  
    {
      "title"    : "Zork (en Español) - Bot",
      "category" : "",
      "tags"     : "zork, bot, azure",
      "url"      : "/2017/02/20/zork-en-espanol",
      "date"     : "2017-02-20 10:47:23Z",
      "content"  : "Corriendo rodee la casa hasta su parte trasera donde encontre una ventana mal cerrada. La abri y me cole. Como esperaba habia entrado por la cocina. En el centro de aquel espacio habia una mesa de madera y sobre ella reposaba un saco marron alargado y una botella de agua. Aun lado habia una chimenea y una escalera que conducia al desvan. Un distribuidor situado a la izquierda conduciaalsalon. Igual que en eljuego.Ernest Cline  Ready Player OneSi os habeis leido el libro Ready Player One si no ya estais tardando sabreis de que estamos hablando. Quiza tambien si habeis visto un capitulo de la segunda temporada de The Big Bang Theory.Zork fue uno de los primeros videojuegos de ficcion interactiva en modo texto. Por lo que tenia muchos numeros para ser portado a Bot Framework de Microsoft. Y este post trata de eso exactamente.Como prueba de concepto juntando un emulador de ZMachine anadiendo alguna implementacion mas descargando el juego de Zork traducido y usando el Bot Framework puedo decir que ya esta terminado. Al menos ha terminado la prueba.Ademas guarda las partidas en un Azure Storage. Para ver el codigo fuente o mas informacion puedes visitar el repositorio en github.Si no has jugado ya no tienes excusas . Espero que lo disfruteis..."
    } ,
  
    {
      "title"    : "Duelo de espadas: el bot definitivo",
      "category" : "",
      "tags"     : "bot, azure",
      "url"      : "/2017/02/15/duelo-espadas-bot-definitivo",
      "date"     : "2017-02-16 09:08:03Z",
      "content"  : "Los 90 fueron una decada estupenda. En cuanto a videojuegos quiza sea por la edad pero creo que eran los mejores. Y aprovechando este momento melancolico decidi volver a jugar al Monkey Island. Una saga que como todos sabreis consta de dos partes. En el que teniamos duelos de espadainsultos delirantes. Pero algunas veces eran combates bastante complejos. Quiza la nueva tecnologia me podria ayudar con esto. Dos horas mas tarde el proyecto terminado. He aqui el diario del pirata. Las notas de desarrollo del bot definitivo El duelo de espadas.0200 PM Dia 1Me encuentro en la isla Melee. Un pequeno trozo de tierra en medio del caribe. Mi mision convertirme en pirata. Pero no se por donde empezar.He encontrado un bar llamado Scumm. Ahi he conocido a Alex Campos. Me ha dado mucha informacion. Me ha hablado de tres pruebas que tres piratas iban a proponerme. Y me ha dicho como superarlas. Necesitare buscar todas las frases posibles de duelos de insultos algo llamado Question And Answers Azure Bot Services y un pollo con polea.0227 PM Dia 1Una navegacion rapida por la Deep Web me ha aportado todo el material que necesito. Unas 60 frases y sus respuestas. Solo he tenido que recogerlas y formatearlas. Tarea sencilla. Una linea por cada una con el siguiente formatoFrase TAB Respuesta TAB JuegoAqui esta el resultado.0252 PM Dia 1Ya he encontrado toda la informacion sobre esa magia oscura llamada QnA Maker de Microsoft. Todas las pistas me han dirigido a su pagina web. Parece ser que siguiendo unos pequenos pasos e importando la informacion que he recolectado anteriormente conseguire un sistema que responda adecuadamente a cada frase de un duelo de Monkey Island.He creado un nuevo servicio con QnA Maker. Ahi he puesto el nombre de mi aplicacion Duelo de Espadas.En la seccion donde me ha solicitado archivos FAQ he insertado el archivo formateado resultado de mi recoleccion de frases de duelo de insultos.Despues de crear el nuevo servicio he comprobado que habia importado los datos que le he pasado. Los ha clasificado correctamente.He encontrado una funcionalidad genial. Desde la parte de Test de QnA Maker no solo puedes probar tu api. Tambien puedes entrenarla anadiendo diferentes formas de realizar la misma pregunta0300 PM Dia 1Me he adentrado en un nuevo lugar el portal de azure. Dentro de sus laberinticos menus he hallado el recurso definitivo Bot Services. Asi que he decido crear uno nuevo. Estoy esperando a que termine el deploy.0304 PM Dia 1Al abrir el recurso nuevo he tenido que anadir un App ID y un password que la propia plataforma me ha ido generando a traves de un wizard.De repente un nuevo panel se ha mostrado. Ahi me proponia elegir un lenguaje de programacion y un tipo de Bot nuevo a generar. Como buen pirata he seleccionado NodeJS y el ultimo tipo de Bot Question And Answer.Entonces un popup salvaje ha aparecido preguntandome si queria asociar el bot con un nuevo servicio o con el Duelo de espadas que cree unas anotaciones atras. Me he decantado por esta ultima opcion. Parecia la adecuada...0331 PM Dia 1Tras un merecido descanso al volver al portal de azure he descubierto que mi bot ya estaba creado. He podido ver el codigo que me ha generado. Ademas me ha permitido retocarlo con el fin de que fuera mas eficiente.Una vez todo estaba en orden he navegado de la opcion de Develop a la de Channels. Ahi he podido seleccionar las plataformas donde he querido que este disponible mi querido nuevo bot.Para terminar con mi experiencia he anadido la informacion necesaria un nombre una descripcion y un nuevo logo.0355 PM Dia 1He abierto el bot Duelo de Espadas en Skype. Me ha aportado todas las respuestas correctas para los duelos de insultos de mi partida de Monkey Island.Mi viaje ha terminado.ConclusionesHan sido dos horas muy productivas. Sin tener demasiada idea de como funcionaban las diferentes tecnologias en muy poco tiempo he conseguido un resultado muy satisfactorio. Esto no significa que este bot este terminado. Podriamos aportarle mas inteligencia. Tenemos el codigo fuente para modificarlo a nuestro gusto. La parte buena es que con estos conocimientos podriamos hacer un bot de asistencia para una pagina web en menos de un dia.Me gusta mucho la direccion que estan tomando los Cognitive Services de Microsoft y lo mucho que nos pueden ayudar."
    } ,
  
    {
      "title"    : "ReConnect(); //2016 - Microsoft ALM End to End",
      "category" : "",
      "tags"     : "azure, devops",
      "url"      : "/video/2016/12/12/reconnect-alm-end-to-end",
      "date"     : "2016-12-12 00:00:00Z",
      "content"  : "Veremos como hacer una una pull request con modificaciones lanzar compilaciones con sonar para comprobar la calidad release y deployment automatico en varios entornos y finalmente como controlar el ciclo de vida completo con VSTSTFS.Aqui podras encontrar el video grabado de la sesion de la ReConnect Madrid 2016 sobre Microsoft ALM junto con Adrian Diaz"
    } ,
  
    {
      "title"    : "Novedades de c# 7",
      "category" : "",
      "tags"     : "csharp, dotnet, novedades",
      "url"      : "/2016/11/16/csharp-7",
      "date"     : "2016-11-16 00:00:00Z",
      "content"  : "La nueva version de c se acerca. Sera la numero 7 ya. C 7 lo llaman. Aunque posiblemente le acompanara una version de .Net Framework 4.X.X. O quiza una dotNet Core 1.X. De cualquier forma no podemos ignorarlo. Pero no es mi intencion realizar un aburrido recorrido sobre sus novedosas caracteristicas. El objetivo del post es realizar un aburrido recorrido sobre sus novedosas caracteristicas y ponerles nota.El problema es que puede resultar un poco simplista usar el manido sistema de una escala de valores del 1 al 10. Las nuevas caracteristicas de c se merecen un sistema de puntuacion que nos aporte mas granularidad del detalle...Recuerdo que en un capitulo de la serie How I Met Your Mother nuestro estimado Barney Stinson exponia un sistema de calificacion llamado escala sexiloca.Asi que he decidido coger prestado este sistema. La idea es queen el eje vertical encontraremos como de buena resulta la caracteristica para el lenguajey en el horizontal como de loco nos parece que es implementarla como la han implementadoDeterminaremos que toda puntuacion que se encuentra por encima de la diagonal Vicky Mendoza es una buena caracteristica. Y lo que se encuentre por debajo... ejem...Nota la diagonal Vicky Mendoza es la recta formada por la funcion x  y.Variables outSi alguna vez hemos trabajado con este tipo de variables sabremos lo engorroso que resulta tenerlas que declarar antes de llamar a la funcion que las asigna. En c 7 esto deja de ser un problema. Ahora las podemos declarar inline. Ademas despues de declararlas de esta forma estan disponibles para su uso en el scope principalpublic void PrintCoordinatesPoint p    p.GetCoordinatesout int x out int y    Console.WriteLinex yY como ademas conocemos el tipo que se va a devolver ahora se permite el uso de var en sus declaracionespublic void PrintCoordinatesPoint p    p.GetCoordinatesout var x out var y    Console.WriteLinex yValoracionUseful  9Crazy  2Pattern matchingNo se si alguno habra trabajado con F alguna vez. Si lo has hecho te sonara esta funcionalidad. Si no estas de suerte ahora ya nada mas que tu ignorancia impedira que programes usando el paradigma funcional.Pattern matching es un patron de programacion que nos ayuda a determinar si un objeto cumple unas caracteristicas determindas. Algo asi como un monton de ifs muy potentes o un super switch.En un ifLa mejor forma de ver que es pattern matching es un ejemplopublic void PrintStarsobject o    if o is null return      determina si un objeto es nulo    if o is int i return  determina si un objeto es un int y lo asigna a i    WriteLinenew string i  note que el scope de i no es solo el ifUna forma de usarlo que parece un poco magia negra seria al transformar un object en un intif o is int i  o is string s  int.TryParses out i   use i  En un switchSiempre me ha hecho gracia cuando hablas sobre que switch is evil que algun iluminado salta con un en mi codigo nunca uso switch. Pero luego vas a su codigo y tiene una cantidad de ifs encadenados que da miedo. Un if es como un switch. Por lo que en este tipo de clausulas tambien podremos usar pattern matchingswitchshape    case Circle c  determina si es de tipo Circle y lo asigna a c        WriteLinecircle with radius c.Radius        break    case Rectangle s when s.Length  s.Height  determina si es un cuadrado        WriteLines.Length x s.Height square        break    case Rectangle r  si es un rectangulo        WriteLiner.Length x r.Height rectangle        break    default  si no tenemos ni pajolera idea de que es        WriteLine        break    case null  o si es null        throw new ArgumentNullExceptionnameofshapeAqui tendremos que tener en cuenta varias cosas que son importantesEl orden de lo case importa.La clausula default siempre sera evaluada la ultima. Aunque este rodeada de cases.El case null del final si que es posible que se ejecute.La variable que se declara dentro de un case solo esta disponible en el scope de ese case no en el de todo el switch.ValoracionUseful  8Crazy  7TuplasCuando necesitas que una funcion devuelva mas de una variable y te parece que eso del encapsulamiento de la programacion orientada a objetos es una pamplina solo puedes hacer alguna napaUsar parametros out pero no va con asyncawaitUsar el objeto System.Tuple como valor a devolver por la funcion mehUsar metodos anonimos no sirve para staticPero tranquilo. C 7 trae la solucion para todos aquellos programadores que piensan que la cohesion es una fuerza fisica.Las tuplas estan aqui y han venido para quedarse. Esto es una forma mas o menos elegante de declarar objetos anonimos al vuelo sin necesidad de definir nombres pero si tipos. Un ejemplo seria si quisieramos que una funcion nos devolviera 3 stringsstring string string LookupNamelong id  devuelce una tupla formada por 3 strings    ...  realizamos nuestra movida con las variables first middle y last    return first middle last  y devolvemos la tupla de 3 stringsLa eficiencia de esto es incuestionable. El saber que demonios devuelve la funcion es otra cosa...Vamos ahora a consumir esta funcionvar names  LookupNameidConsole.WriteLinefound names.Item1 names.Item3.Como podeis ver solo hace falta que llamemos a las propiedades desde Item1 hasta ItemN de nuestra tupla. Esta claro que este codigo puede confundir mas que ayudar. Asi que tambien se permite poner nombre a los diferentes objetos que devolvemos en una tupla y menos malstring first string middle string last LookupNamelong id  tuplas con nombres de elementosPara devolver una tupla con nombres se nos permite usarlos en su creacionreturn first first middle middle last last  creando una tupla usando los nombresDe tal forma que al llamarla el codigo resultante sera mas intuitivovar names  LookupNameidWriteLinefound names.first names.last.Una tupla es para el sistema un tipo de valor. Sus elementos son publicos y mutables. Ademas si comparamos dos tuplas con los mismos elementos podemos determinar si son iguales o no.ValoracionUseful  7Crazy  7DeconstructionOtra forma de utilizar tuplas es la deconstruccion. Esto es una sintaxis que nos permite declarar variables individuales a las que le asignamos las diferentes propiedades de una tupla. Un ejemplostring first string middle string last  LookupNameid1  deconstruccion por constructorConsole.WriteLinefound first last.  podemos acceder a las variablesEn la deconstruccion podemos usar tambien tipo inferido varvar first var middle var last  LookupNameid1  var insideO incluso abreviarlovar first middle last  LookupNameid1  var outsideTambien se puede deconstruir una tupla usando variables ya existentesstring first middle lastfirst middle last  LookupNameid2  deconstruccion con asignacionPero cuidado. Una deconstruccion no es solo para tuplas. Vale para cualquier tipo de objeto que tengamos. Solo tenemos que crear un metodo Deconstruct que pase como parametros de salida out las variables que queremos deconstruir. La firma del metodo podria ser estapublic void Deconstructout T1 x1 ... out Tn xn  ... La parte graciosa de todo esto es que no hay herencia de ningun tipo. Cada objeto tendra que implementar su propio metodo con diferentes parametros de salida. Asi que por ahora no espereis un IDeconstructable ni nada por el estilo que le de algo de consistencia a nuestro codigo.Un ejemplo de uso de esta featureclass Point    public int X  get     public int Y  get     public Pointint x int y  X  x Y  y     public void Deconstructout int x out int y  x  X y  Y var myX var myY  GetPoint  esto llama a Deconstructout myX out myYYa estamos esperando un analyzer de roslyn que compruebe que tenemos un constructor y una funcion Deconstruct con los mismos parametros para que tenga sentido todo esto.ValoracionUseful  7Crazy  8WildcardsEl nuevo sistema de wildcards nos permite usar el simbolo asterisco para no tener que definir algo que no queremos. Por ejemplo si tenemos una variable out que no necesitamos podemos usarlop.GetCoordinatesout int x out   solo me preocupa la xO en el caso de usar deconstruccion tambien podemos usarlovar myX   GetPoint  I only care about myXValoracionUseful  4Crazy  8Funciones localesSi eres de los que echa de menos la forma de programar de javascript. Si te mola un monton tener funciones anidadas unas dentro de otras. Si te piensas que el espagueti code se hace solo con ifs. Ahora tenemos una de las novedades de c 7 funciones locales.Y esto que es lo que es Te preguntaras... Pues muy facil declarar funciones en medio de una funcion. Asipublic int Fibonacciint x    if x Con hoisting. Una funcionalidad muy util cuando tienes una funcion recursiva y otra que realiza la llamada de la funcion recursiva .Ademas de estar en el mismo contexto de una funcion prevendra a otros objetos llamar a este metodo sin comprobar que efectivamente el numero x es mayor de 1.ValoracionUseful  2Crazy  9LiteralesNunca has tenido en tu codigo un magic number que era muy dificil de leer por ser muy largo C 7 tiene la solucion. Ahora puedes separar los digitos literales con _. Y este simbolo no alterara su valor. Por ejemplovar d  123_456var x  0xAB_CD_EFComo hemos podido vivir sin esta funcionalidad antes Podra parecer una tonteria pero si os contamos que tambien se han anadido literales binarios entonces empieza a cobrar algo de sentido la posibilidad de anadir legibilidad a este tipo de cifrasvar b  0b1010_1011_1100_1101_1110_1111Si no trabajas directamente en binario es porque no quieres.ValoracionUseful  3Crazy  8Ref returnsSi actualmente te ves obligado a devolver una referencia de un valor. Si estas aficionado al uso de ref. Ahora puedes devolver una referencia directamente con una funcion anadiendo ref delante del tipopublic ref int Findint number int numbers    for int i  0 i Y es que he estado pensando mucho acerca de esta funcionalidad. Se parece mucho a usar punteros. Quiza sea lo mismo. Podria ser que no haya punteros en c 7. Y tendria sentido que las wildcards al usar el caracter que usan los punteros fueran las culpables. MalditasValoracionUseful  5Crazy  2Mas definiciones inlineEn c 6 se anadio la definicion de metodos inline una funcionalidad que en c 7 se extiende. Ahora podremos declarar inline todo lo que antes no se podiaclass Person    private static ConcurrentDictionary names  new ConcurrentDictionary    private int id  GetId    public Personstring name  names.TryAddid name  constructores    Person  names.TryRemoveid out                destructores    public string Name            get  namesid                                  getters        set  namesid  value                          setters    Y lo que es mejor es una funcionalidad que viene de la comunidad. Viva el open sourceValoracionUseful  9Crazy  1Throw en expresionesOtra de las nuevas ventajas de c 7 es poder lanzar excepciones dentro de expresiones. Suena un poco raro pero creo que es la funcionalidad que cierra el circulo de las definiciones inlineclass Person    public string Name  get     public Personstring name  Name  name  throw new ArgumentNullExceptionname    public string GetFirstName            var parts  Name.Split         return parts.Length  0  parts0  throw new InvalidOperationExceptionNo name        public string GetLastName  throw new NotImplementedExceptionValoracionUseful  8Crazy  1Bola extra default interface implementationsEsta es una funcionalidad que esta muy en el aire. Pero se comenta que en user voice se comenta aqui para ser exactos httpsgithub.comdotnetroslynissues73 que un degenerado solicito imitar la peor feature de java de los ultimos anos en c definir una implementacion por defecto para una interfaz. Por ahora esto es lo que planteaninterface ISomeInterface  string Property  get   default string Format      return string.Format 0 1 GetType.Name Property  class SomeClass  ISomeInterface  public string Property  get set Sin comentarios...ValoracionUseful  1Crazy  1000ConclusionesEstas caracteristicas no es seguro que aparezcan en c 7. Aunque estoy seguro de que muchas de ellas si. Actualmente hemos podido jugar con versiones preview. De cualquier forma esta muy bien que sigan trayendo novedades cuando ya han pasado 7 versiones de este lenguaje de programacion orientado a objetos. Quiza por ser demasiadas versiones y no poder sacar mas chicha de este paradigma se haya optado por incluir caracteristicas de lenguajes funcionales.A continuacion unas graficas asi parece que sabemos de lo que hablamos cientificamenteNota hemos excluido las default interface implementations porque nos jodia las graficasDisclaimer Todo esto pueden ser patranas. No te lo tomes en serio. La informacion ha sido extraida de httpsblogs.msdn.microsoft.comdotnet20160824whatsnewincsharp70 httpsgithub.comdotnetroslynissues73 y de la preview 4 de c 7.Y yo me pregunto Para cuando palabra clave let"
    } ,
  
    {
      "title"    : "VSTS: Package Management",
      "category" : "",
      "tags"     : "nuget-server, nuget, dotnet, azure, devops",
      "url"      : "/2016/10/03/vsts-package-management",
      "date"     : "2016-10-03 00:00:00Z",
      "content"  : "Os acordais de John McClane Lo dejamos salvando la navidad en Los Angeles. Ahora esta en Nueva York. Feliz. Hasta que Simon el hermano de Hans aparece con ganas de bronca. Quiere robar el oro de los Estados Unidos de America. Pero esta vez John McClane cuenta con la ayuda de Samuel L. Jackson. Podran superar juntos el juego de Simon diceHan sido pocos los que me han pedido la segunda parte de La Jungla de Cristal. En realidad ninguno. No obstante aqui la teneis. Una historia semejante. Tenemos un cliente Holly. Con un nuevo problema Simon. Pero se parece mucho al de los proyectos que tienen una libreria compartida y diferentes versiones de la misma Hans. Y esta vez queremos usar Visual Studio Team Services Samuel L. Jackson. Es nuestra herramienta preferida. Por que ibamos a necesitar instalar algo nuevo. Afortunadamente contamos con la extension Package Management John McClane.Un paseo por HarlemPackage Management llego sin hacer demasiado ruido. Al menos no tanto como deberia. Como cuando la policia tiene que buscar a John McClane para cumplir los deseos de un psicopata que se hace llamar Simon.La idea es poder integrar dentro de Visual Studio Team Services un servidor de paquetes. En un principio soportan NuGet. Porque es una version preview. Pero a futuro les gustaria integrar otros tipos de paquetes como npm. Tambien a futuro esperan tener la extension disponible para TFS onpremises.La forma de organizar que se ha disenado consiste en Feeds. Cada Feed podria contener sus propios paquetes su propia seguridad y por su puesto su propio endpoint.Instalar esta extension en nuestra cuenta de VSTS es relativamente sencillo. Tendremos que ir a la Marketplace de Visual Studio Team Services. Alli buscar Package Management. Y nos encontraremos con este resultado.Entonces presionamos instalar y listo. Al dirigirnos a nuestro site veremos que ha aparecido una nueva opcion en la toolbar Packages.Esto significara que todo ha ido correctamente.El metro descarrilaA partir de aqui la cosa se complica. Si queremos usar este servicio tendremos que crear un nuevo FeedUn dialogo nos dara la opcion de poner un nombre una descripcion y de elegir los permisos. En un principio hemos seleccionado que todos los usuarios de la cuenta puedan acceder. Ademas de decirle que solo el sistema de builds pueda subir paquetes.Despues pulsaremos el boton de Connect to feedDe la pantalla emergente copiaremos el valor de Package source URL. Esto nos servira para dos cosas subir los paquetes a nuestro feed y para poderlos descargar despues con Visual Studio.En este punto VSTS Samuel L. Jackson ya es capaz de servirnos paquetes de NuGet ha llegado al telefono de la estacion de metro. Pero aun no hemos publicado ningun paquete dentro del feed John McClane llega tarde.Asi que vamos a intentar publicar unas librerias propias. Para ello crearemos una nueva build. Si es que no tenemos una ya.Iremos a la seccion de builds pulsaremos sobre   New y elegiremos la de tipo Visual Studio.Cuando nos aparezca el panel de edicion anadiremos dos nuevos build steps NuGet Packager y NuGet Publisher.El Packager lo colocaremos detras de los tests y el Publisher al final del todo.Entonces editaremos las opciones del NuGet Packager y marcaremos el checkbox de Include referenced projects.Despues editaremos las opciones del NuGet Publisher. Marcaremos la opcion de Internal NuGet Feed. Y anadiremos la URL de nuestro feed.Para finalizar guardaremos y encolaremos una nueva build.Al terminar el proceso y si todo ha ido bien si volvemos a la pestana de Packages encontraremos nuestros nuevos paquetes de NuGet. Ademas podremos ver los detalles de los mismosCon ello habremos conseguido sobrevivir a esta dificil prueba. Pero el final aun no esta cerca...El problema de las garrafas y los galones de aguaEl problema es que tenmos una garrafa de 5 galones otra de 3 galones una fuente para llenar las garrafas de agua y una bascula con detonador. Si no pesamos en la bascula 4 galones exactos estamos muertos.He pensado mucho en este problema y he llegado a alcanzar dos soluciones posiblesLlenamos la de 3 la pasamos a la de 5. Volvemos a llenar la de 3 y volvemos a pasar el contenido a la de 5. Esta vez no cabe todo. Asi que vaciamos la de 5. Volcamos en la de 5 lo que sobraba de la de 3. Volvemos a llenar la de 3 y lo pasamos a la de 5. Practicamente 4 galones. Siempre que no se nos caiga nada por el camino.Llenamos la garrafa de 5 y pasamos su contenido a la de 3. Vaciamos la de 3 y la rellenamos con los dos galones que quedaban en la de 5. Llenamos la de 5 de nuevo rellenamos el galon que quedaba en la de 3 y ya tenemos 4 galones. Quiza con menos probabilidad de que se nos haya caido tanto por el camino.El problema con VSTS Package Management es que es preview y puede pasar que derramemos mucho liquido en el proceso sin darnos cuenta. Y si no que le pregunten a Alex Casquete que el otro dia me comentaba que a algunos de su equipo les iba y a otros no...De cualquier forma nos dirigiremos a Visual Studio. Alli abriremos la opcion de Manage NuGet Packages for solution.Seleccionaremos la ruedita dentada de la parte superior derecha.Alli anadiremos una nueva fuente de paquetes. Le pondremos un nombre. Copiaremos la URL de VSTS que utilizamos anteriormente. Pulsaremos Update. Y la cerraremos dandole a Ok.Al terminar podremos seleccionar nuestro VSTS como fuente de paquetes. Entonces nos deberia pedir las credenciales. Tendremos que utilizar las mismas que usamos en el portal. Si todo va bien podremos instalar esos paquetes que acabamos de generar en la nube.ConclusionesJohn McClane ha vuelto a salvar el dia. Muchas gracias John McClaneUna de las cosas que mas se echaban en falta en VSTS era la gestion de paquetes. Muchas herramientas semejantes ya lo tienen desde hace tiempo como Team City. Asi que siempre es de agradecer que nos den acceso a esta version preview.Personalmente me ha ido bien pero hay gente que esta experimentando diferentes errores. Bien por autentificacion de los usuarios. Por temas de permisos. Y tambien por la version de NuGet que usa. Esta version aun no tiene compatibilidad con netcore. Esto puede causar diversos problemas a la hora de empaquetar ciertos paquetes con dependencias de la standard library...De cualquier forma es una muy buena solucion. Integrada con una herramienta que mucha gente usamos para gestionar el ALM. Aunque sera mejor previsiblemente a finales de este ano. Asi que aunque no sea nuestro repositorio principal de paquetes merece la pena ir acostumbrandonos.yippeekiyay hijo de puta..."
    } ,
  
    {
      "title"    : "VSTS: Migrar TFVC a Git",
      "category" : "",
      "tags"     : "azure, devops, git",
      "url"      : "/2016/09/15/vsts-migrar-tfvc-a-git",
      "date"     : "2016-09-15 00:00:00Z",
      "content"  : "Hace no mucho El Bruno me invito a participar en uno de sus conocidos podcast. En este caso la tematica trataba de un articulo que podreis leer en esta misma Web Por que odio Git. Dejando de lado lo agradecido que estoy por esta oportunidad dentro de la conversacion me pregunto si me habian pedido migrar a Git muchos clientes. La verdad es que no. Pero me dio una buena idea sobre la que escribir.Y es que la mejor forma de migrar de TFVC a Git es no migrar. Si usamos directamente Git nos quitamos todos los problemas.Pero si para ti esto es imposible. Si ya tienes un proyecto en Visual Studio Team Services. Si tienes un historico una trazabilidad. Si te confundiste elegiste TFVC como repositorio. Si ya estas harto de TFVC. O si eres un fanboy de Git. Si quieres cambiar el repositorio pero no quieres perder nada. Tengo una buena noticia para ti si se puedeLo primero que tenemos que hacer es crear un nuevo repositorio de codigo fuente dentro del proyecto. Una opcion que encontraremos en la seccion de codigo del portal de VSTSPero esta vez hay que elegir Git. Y ponerle un nombre bonitoUna opcion de hacker es activar el checkbox de crear el archivo readme.md. Esto es un archivo con el leeme del proyecto. Muy util si pensais pasarlo a GitHub en algun momento.Al terminar de crear el repositorio os aparecera una pantalla donde encontrareis la url del mismo. Ademas de una opcion para crear unas credenciales para poder conectarosRecordad guardaros tanto la url como las credenciales que a continuacion seran necesarias.El problema de hoy en dia es que existen multitud de opciones. Multitud de herramientas para hacer lo mismo. Nos encontramos en una permanente disyuntiva. Cerveza negra o rubia. Cristiano Ronaldo o Messi. Allen o Johansson. Cuando queremos migrar de TFVC a Git es lo mismo. Nunca recuerdo cual era la aplicacion buena gittf o gittfs. Siempre me confundo. Al final termino instalandome ambas. Y como eran los dichosos comandosSi comandos. Las herramientas que conozco que nos pueden ayudar se ejecutan en modo consola. Por eso nos crearemos un directorio de trabajo. Por ejemplo cwat que sera la ruta desde donde lanzaremos las instrucciones.You Cant Write Perfect Software. Did that hurt It shouldnt. Accept it as an axiom of life. Embrace it. Celebrate it. Because perfect software doesnt exist. No one in the brief history of computing has ever written a piece of perfect software.Andrew Hunt The Pragmatic Programmer From Journeyman to MasterGitTfGitTf es la primera herramienta que conoci. Por eso la menciono primero. Esta escrita en java. Esto la hace compatible con sistemas operativos windows linux o macos. Es muy simple usarla. Si queremos instalarla tenemos la opcion de usar chocolateychoco install gittf yO bien descargarla de su web.Una vez lo hemos hecho abriremos una consola. Y ejecutaremos un comando para descargarnos nuestro codigo fuente original. GitTf lo transformara a formato de repositorio GitCwat gittf clone http.visualstudio.comDefaultCollection  deepDondeYourName es el nombre de tu cuenta de Visual Studio Team Services.TeamProjectName es el nombre de tu proyecto.Path si no deseas descargar el proyecto entero solo una carpeta source o algo asi lo especificas aqui.Esto nos descargara todo el codigo fuente con su historial desde el TFS. Desde nuestro repositorio TFVC.Como anteriormente ya creamos el repositorio de Git copiaremos la url del mismo que tendra un formato del estilohttps.visualstudio.comDefaultCollection_gitEn la consola nos situaremos en la carpeta cwat. Dentro de esta buscaremos una subcarpeta con el nombre del proyecto que hemos descargado de TFVC anteriormente. Situaremos la consola en esa carpeta y anadiremos el repositorio de Git como un nuevo remoteCwat cd Cwat git remote add origin https.visualstudio.comDefaultCollection_gitLo que habremos hecho es relacionar nuestro repositorio local el que acabamos de clonar desde TFVC con el repositorio Git. A continuacion para iniciar la subida al repositorio Git realizaremos un pushCwat git push origin masterNos pedira el usuario y la contrasena para realizar la subida. Los introducimos y esperamos a que suba todos los datos.Cuando termine esta operacion tendremos en nuestro repositorio de Git todos los changesets que teniamos en TFVC y ya podremos trabajar con el.GitTfsUna alternativa a GitTf es GitTfs. Es una herramienta semejante. Aunque a mi me gusta mas. Esta escrita en c. Luego si no la migran a core solo se podra usar en sistemas windows. Para instalarla podemos seguir la misma mecanica. O usar chocolateychoco install gittfs yO dirigirnos a su web a la seccion de releases.Los comandos que tendremos que utilizar son muy parecidos. Aunque en este caso usaremos algun parametro mas. Lo primero clonaremos el repositorio de TFVC original y cambiaremos su formato a Gitcwat git tfs clone http.visualstudio.comDefaultCollection  . branchesall export exportworkitemmappingcwatmappingfile.txtDondeYourName es el nombre de tu cuenta de Visual Studio Team Services.TeamProjectName es el nombre de tu proyecto.Path si no deseas descargar el proyecto entero solo una carpeta source o algo asi lo especificas aqui.La parte buena de GitTfs es que es capaz de copiar tambien las branches. Ademas tiene un sistema de mapping de workitems. Muy util si en lugar de migrar dentro de la misma cuenta de VSTS lo estamos haciendo a otra. O incluso a un TFS on premises.La contra es que al ser una herramienta mas completa tambien se toma ciertas libertades. Una de ellas es cambiar los mensajes de TFVC con un extra de metadatos. Asi que el segundo comando que ejecutaremos limpiara esos mensajesCwat cd Cwat git filterbranch f msgfilter sed sgittfsid.g  allDespues verficaremos que todo esta correcto y borraremos la siguiente carpeta Cwat.gitrefsoriginal. Esto borrara branches antiguas.Para terminar incluiremos el repositorio de Git que creamos al principio y enviaremos todas las modificacionesCwat git remote add origin https.visualstudio.comDefaultCollection_gitCwat git push all originAl finalizar ya tendremos nuestro repositorio Git en VSTS listo para usar. Ademas conservara todos los changesets que realizamos en el anterior repositorio.ConclusionesEl coste de migrar a Git es relativamente pequeno. Pero el coste de que todos los desarrolladores de un equipo aprendan Git es algo mayor. A mi personalmente el uso de Git me ha ayudado.Si quieres realizar una migracion rapida. Si no tienesquieres migrar branches. GitTf es tu herramienta. Pero hay que tener en cuenta que no tiene versiones nuevas desde 2013.Si quieres realizar una migracion con todas las ramas. Si quieres luego seguir gestionando esta compatibilidad entre un TFVC y un Git. GitTfs es lo que deberias usar. Es muy potente. Mucho mas de lo que se expone en este articulo. Pero tambien mas complicada.Y si os preguntais si estos comandos solo valen para una misma cuenta de Visual Studio Team Services os puedo decir que no es asi. Vale para TFS tambien. Ademas no tiene por que ser ni el mismo TFS ni VSTS el que recibe los cambios. Puedes recogerlos de una version onpremises con TFVC y migrarlos a un repositorio Git de una cuenta de VSTS en la nube. Pero para esta ultima operacion os recomiendo que useis GitTfs."
    } ,
  
    {
      "title"    : "Quiero mi propio NuGet",
      "category" : "",
      "tags"     : "nuget-server, nuget, dotnet",
      "url"      : "/2016/09/08/quiero-mi-propio-nuget",
      "date"     : "2016-09-08 00:00:00Z",
      "content"  : "Recuerdas la primera vez que viste a John McClane Nada mas llegar en avion le llevaron a la torre Nakatomi. La idea era recoger a Holly y llevarla de vuelta a casa. En navidad. Todo era estupendo. Hasta Allen disfrutaba de un dia tranquilo. Pero un maldito bastardo llamado Hans tuvo que venir a joder el cotarro.Esta misma situacion nos la encontramos a diario en el mundo del desarrollo. Y generalmente terminamos llamando al John McClane de turno para que nos solucione el fregao.La historia comienza con un cliente Holly. Tiene una libreria que se usa en muchos proyectos. Y esta libreria o bien se copia a mano o bien se copia todo el codigo fuente de solucion en solucion Hans. Nos encontramos en la tesitura de tener que anadir una funcionalidad nueva a esta libreria Proyecto Nakatomi. Y a la hora de actualizar todos los proyectos es cuando nos encontramos los problemas Navidad. La unica solucion sera usar un gestor de paquetes versionados que contengan esta libreria. Pero no podemos usar el gestor de paquetes por defecto ya que nos obligaria a publicar para todo el mundo este ensamblado. Asi que decidimos montar un servidor de NuGet interno John McClane.Existen varias alternativas a esto como servicios tipo MyGet. Pero entonces esto no seria una jungla de cristal...Como me hago un NuGet.ServerNo es la primera vez que me encuentro en disposicion de hacer un John McClane instalar un servidor de NuGet. Recuerdo bien los pasos. Es muy sencillo Creas una aplicacion Web vacia con Visual Studio. Buscas el paquete de NuGet llamado NuGet.Server. Lo instalas. Cambias un par de parametros de configuracion. Y a publicar.Asi que me pongo manos a la obra. Completo todos los pasos. Compilo. Falla se rompen las ventanas dejando los trozos de cristal a mi lado y yo descalzo.Buscando la solucion no tardamos en encontrar la web del proyecto. Ahora hospedado en github y bajo el manto protector del equipo de Open Source de Microsoft Ahora tengo una ametralladora Jo Jo Jo.A partir de aqui dos caminos se abren. Pero solo se puede elegir unoBajar por el hueco de los ascensoresAprovechando que conocemos la web con el codigo fuente httpsgithub.comNuGetNuGet.Server. Solo tenemos que seleccionar la rama estable de releaseDespues buscamos el enlace para descargar el codigo en forma de archivo zipY una vez finalizada la descarga descomprimimos el archivo.Usar el conducto de la ventilacionEl camino complejo pero seguro consiste en abrir un terminal bash o consola. Ahi nos pelearemos con comandos Git Terroristas clonamos el repositorio en la carpeta nugetserver git clone httpsgithub.comNuGetNuGet.Server.git nugetserverDespues elegimos la branch de release cd nugetserver git checkout releaseConfigurando NuGet.ServerIndependientemente del camino elegido llegamos al punto de tener que buscar el archivo de la solucion NuGet.Server.sln y abrirlo con Visual Studio.Alli nos dirigiremos al archivo Web.config dentro de la seccion appSettings a una linea que anade la clave apiKeyAqui deberemos sustituir el valor por el que mas rabia nos de. En mi caso puse yippeekiyay.Para terminar publicaremos la solucion en una WebApp de Azure. O si aun no tenemos una cuenta en la nube la podremos publicar en forma de aplicacion Web en cualquier IIS.Habiamos conseguido arreglar el dia de navidad. Ademas con el bonus extra de tener que escribir en directo comandos en una ventana negra con texto blanquecino. BrujeriaComo hago que Visual Studio use mi servidor NuGetEsta claro que de nada sirve un servidor si nadie consume los servicios de los que provee. Pero que no cunda el panico. Simplemente tendremos que abrir Visual Studio y navegar por el menu tools  NuGet Package Manager  Package Manager SettingsEn la ventana que nos aparece elegiremos Package SourcesY pulsaremos el simbolo de   verde. Entonces nos aparecera una linea nueva. En la parte inferior podremos cambiar su configuracionEn Name pondremos el nombre con el que queremos que aparezca nuestro servidor de NuGet. Y En Source pondremos la url de nuestro servidor de NuGet con un path a nuget. Entonces pulsaremos el boton de Update y despues al boton de OK.Como publico un paquete en mi servidor NuGetSi configuramos nuestro servidor de NuGet en Visual Studio pero no tenemos ningun paquete pierde sentido todo este escrito. Pero podeis estar tranquilos. Publicar un paquete en nuestro servidor de NuGet es solo un comando nuget.exe push My NuGet Package My NuGet Server My NuGet API KeyDondeMy NuGet Package es la ruta a nuestro paquete de NuGet. Por ejemplo AGoodDayToDieHard.nupkg.My NuGet Server es la url de nuestro servidor de NuGet. Por ejemplo httpnuget.nakatomi.com.My NuGet API Key es el token que escribimos en las configuraciones del portal. En el archivo web.config. Por ejemplo yippeekiyay.Bonus almacenar los paquetes en una Storage Account de AzureNo os voy a desvelar todas los detalles pero yo me fijaria en dos archivos del codigo fuente que os habeis descargado NuGet.Server.Infrastructure.IServerPackageStore y NuGet.Server.IServiceResolver.ConclusionesNo hay mucho mas que anadir. NuGet es facil. No es el mejor. Pero tampoco malo. Es util. Y esto ultimo no se puede decir de todas las herramientas que hay.Quien no tiene uno es porque no quiere..."
    } ,
  
    {
      "title"    : "El preguntón y cierre evento",
      "category" : "",
      "tags"     : "azure, bootcamp",
      "url"      : "/video/2016/04/20/gab16-pregunton",
      "date"     : "2016-04-20 00:00:00Z",
      "content"  : "Cierre del Global Azure Bootcamp 2016 con el concurso El Pregunton."
    } ,
  
    {
      "title"    : "Por qué odio Git",
      "category" : "",
      "tags"     : "git, best-practices",
      "url"      : "/2016/02/16/por-que-odio-git",
      "date"     : "2016-02-16 09:52:03Z",
      "content"  : "El sistema de control de version que mas se ha extendido en los ultimos anos posiblemente sea Git. Algo que no ha pasado desapercibido para los desarrolladores de Microsoft. Asi que han decidido integrarlo en todas sus herramientas. Y este hecho... Me ha destrozado la vida.Como en muchas empresas en la mia usamos para la gestion del ALM Visual Studio Team Services. El antiguo Visual Studio Online. A su vez el antiguo TFServices. O tambien conocido como TFS Online. Y es que poner nombres es muy dificil. El caso es que a lo largo de 2013 tanto en esta herramienta como en el IDE Visual Studio se anadio soporte para Git. Asi que no tardaron los earlyadopters en pedir una migracion a este tipo de repositorios de codigo fuente. Y hace unos seis meses que todos estamos funcionando con este sistema.Estoy cansando de que todos hablen siempre bien de Git. Llevan asi desde que aparecio ahi por 2002. Concebido como open source por Linus Torvalds. Posiblemente porque se aburria. Cuentan las leyendas que lo hizo en un fin de semana y ahora ya no trabaja en ello. Quiza esto le deberia hacer perder credibilidad a la herramienta. Pero por lo visto causa el efecto contrario. Segun comenta el propio senor Torvalds lo diseno pensando en sus necesidades. Esas que no le aportaban otros SCM. Su idea era que fuera distribuido facil rapido y poco pesado. Esto me recuerda a un paseo por el Gran Bazar de Estambul. Alli todo cumple con la norma de las 3 Bs Bueno Bonito y Barato. No me fie cuando estuve en Turquia de la misma forma que no me fio de este producto. Y es que no me gustan los cambios. Pero no soy un tipo cerrado que solo este disgustado porque le han sacado de su zona de confort. Hay mas...Voy a contaros mis experienciasAdios a irse pronto a casaAntiguamente usabamos un sistema centralizado en el un servidor. Cuando no funcionaba internet no podias trabajar. Visual Studio te daba problemas. Lo unico que podias hacer era trabajar offline. Y las vueltas a online solian ser bastante dolorosas. La unica solucion razonable era irte a casa o tomar unas cervezas.Pero Git es distribuido. Esto se traduce a que cuando te clonas un repositorio en tu ordenador automaticamente estas creando un repositorio como el del servidor. Pero de forma local. Asi trabajas en tu maquina hasta que decides sincronizar con el servidor todas las modificaciones que has realizado. Las consecuencias de esto son nefastas ya no te puedes ir a casa. Aunque no tengas conexion puedes seguir trabajando. Ya sincronizaras mas tarde. este comando lo usamos para conectarnos con un repositorio de git git clone myremote httpmygit.comrepositoryLa unica satisfaccion que aun conservamos es al menos poder quejarnos de la empresa que nos provee la conexion.El codigo es mioEn este punto supongo que mas de uno se estara preguntando Que es eso de que los cambios los guardas en local Como que subes cuando quieres Un desarrollador puede tener un cambio y no subirlo hasta final de mes No tiene por que guardar directamente los cambios en el servidor. Es evidente que quien desarrollo Git no tiene claro el concepto de Continuous Integration...Dentro de estas funcionalidades absurdas encontramos que un repo Git puede tener varios remote. Un remote es un servidor con el que sincronizar el repositorio local. Y un mismo repositorio que tienes en local puede estar vinculado con varios servidores diferentes. con esto anadimos otro repositorio servidor con alias tokiota a nuestro repositorio local git remote add tokiota httpstokiota.comgitsrepo.git asi listamos todos los servidores que tenemos asociados git remote vAhora me imagino a un proveedor diciendo que guardan en sus servidores el codigo que estan escribiendo para nosotros. Y que nos digan que ya sincronizaran con nuestros servidores de vez en cuando... De eso nada. Les estamos pagando. Por lo que ese codigo es nuestro. No queremos que lo tengan en sus servidores.Y mi cafeMi rutina diaria siempre ha sido muy normal. Me gusta llegar a trabajar por la manana encender el ordenador y le darle a descargar la ultima version del codigo. Mientras me acerco a la cafetera y me tomo un ristretto. Para cuando vuelvo a mi puesto terminan de descargarse los ultimos datos.Git me ha robado el momento zen del cafe matutino. Internamente maneja un tamano de paquetes diferenciales snapshots que imagino que seran bastante pequenos. Espero que no pierda cambios por ello. El resultado es que las descargas no dan tiempo a ese cafe. Y me dejan sin excusa para tomarlo. comando para recoger los cambios del servidor git pull myremoteOtro cafe que ya no me puedo tomar es el de cuando te descargas una branch. Mientras se descargaba podias tener una conversacion interesante con los companeros. Eso tambien lo hemos perdido. Git maneja un sistema de branches totalmente diferente. Para este sistema una rama es solo un punto en el camino. Marca una posicion en la que se aplican solo una serie de cambios. No todos. Asi que cuando sincronizas el codigo ya te descargas todas las ramas. asi creamos una rama git checkout b mybranch asi cambiamos a esa rama git checkout mybranchParece que a los programadores de Git no les gusta el cafe. No me fio de las personas a las que no les gusta el cafe.Encima tengo que hacer TDDHace tiempo que dentro del Definition Of Done figura tener pruebas unitarias para todo el codigo que subamos. Unas comodas pruebas. Una vez has terminado la funcionalidad copias el codigo y cambias un par de detalles. Asi consigues una prueba valida rapidamente. Sin esfuerzo. Y eso sin contar con el apoyo de grandes herramientas como Pex.Pues resulta que ahora como tenemos Git no hace falta que un commit vaya directo al servidor. Ese cambio puede residir en nuestro repositorio local. Ahora resulta que tenemos sincronizar el codigo con al menos tres cambios bien diferenciados red green y refactor. creamos un conjunto de cambios con el comentario entrecomillado git commit m Tarea  Red otro commit para cuando pasan los tests git commit m Tarea  Green creamos el ultimo conjunto de cambios git commit m Tarea  Refactor enviamos todos los commits locales al servidor git push myremoteTodos los desarrolladores tenemos que pensar primero en una prueba escribirla y que no pase. Luego hacer el codigo necesario para que pase ese test. Para finalizar poner todo el codigo anterior bonito y que las pruebas sigan pasando. Lo que se conoce vulgarmente como Test Driven Development. Que te obliga a pensar mas. Y en cualquier momento alguien puede revisar si lo has estado aplicando correctamente. Una verdadera caza de brujas.Veo commits por todas partesLos commit se pueden aplicar sueltos. Hay bifurcaciones. Puedes montar un puzzle de cambios sin problemas. Una branch puede tener unos commits totalmente diferentes a los de otra. Ademas la forma de identificarlos son GUIDs. Que no se si alguno os dice algo un GUID pero a mi no me aporta ningun valor. asi sacamos los commits de la branch actual git logcommit a8a7bfff366350be2e7c21b8de9cc6504678a61bAuthor Me Date   ...commit e5e3e4c1ef46ae64aa08e8ab3f988bc917ee1ce4Author Me Date   ......Un ejemplo grafico gracias a una herramienta que nos muestra un mapa de cambiosOtro ejemplo es uno de twitter El GITar Hero. Si alguien entiende algo de esto que venga y me lo explique por favor.La culpa es miaLa parte buena de tener semejante conjunto de cambios son los merges grandes. Creo que todos hemos vivido ese momento en el que se junta la rama de hotfix con la principal. La persona encargada suele perder algun codigo importante en la operacion. Es la forma ideal de ocultar otros problemas y ganar tiempo en los desarrollos. Ademas nadie te va a echar nada en cara porque la culpa ha sido de quien hizo el merge.En Git esta operacion de merge se realiza en el momento que descargas el codigo del servidor. Lo hace cada uno en local Esto git pull myremote Es aproximadamente esto git fetch myremote git merge mybranchDe esta forma los administradores de VSTS tiran balones fuera. Que los merge los haga cada desarrollador. Que nos comamos nosotros el marron. Y lo que es peor al haber muchos commits es muy facil encontrar quien ha metido la pata en el codigo. Sigue la caza de brujas...Es complicadoNo se si os habeis percatado de que voy poniendo codesnippets a lo largo de todo este escrito. Esta hecho a posta. Si hay una forma de manejar bien Git y sacarle todo el jugo es usar comandos de consola. Como si estuvieramos en el medievo.Alguno me dira que Visual Studio tiene soporte para Git. Pero realmente no tiene todos los comandos implementados. Si la mayoria. Pero no todos. Para hacer ciertas operaciones al final tienes que abrir la consola. Tambien hay gente que usa Source Tree. Una aplicacion de Atlassian. Me niego en rotundo a instalarme mas aplicaciones... Aunque la recomiende Martin Fowler.Dentro del mundo de todos los comandos que tiene Git encontramos que podemos resolver un mismo problema de muchas formas diferentes. Un ejemplo seria echar para atras un commit. Tendriamos las siguientes alternativasMover a un commit en concreto de forma temporal Desatachamos el HEAD que es lo mismo que no tener ninguna rama desprotegidagit checkout dd1d7ab32 si quieres hacer cambios mientras estas en este estado extranogit checkout b oldstate dd1d7ab32Si queremos borrar definitivamente un cambio que no hemos llegado a publicar en el servidor Primero no tendras que tener nada modificado. Todo en commits. Despues borramos el cambiogit reset hard dd1d7ab32 Si quieres guardar el codigo modificado porque no quieres hacer un commitgit stashgit reset hard dd1d7ab32git stash popSi queremos borrar un commit que ya este publicado tendremos que crear otro commit Esto crea 3 commits separados que desacen los commits indicadosgit revert a867a4ad 25eff4ca a766c053 Tambien lo podemos hacer por rango. Borrando los dos ultimos commitsgit revert HEAD2..HEAD O podemos tirar para atras un commit de tipo MERGEgit revert m 1 Todos estos comandos hay que aprenderlos. Son demasiados. Demasiadas casuisticas. No es nada facil gestionar Git.No uses GitDa igual que no estes alineado con todo el mundo. Eso no es lo importante. No le digas a tus companeros que Git es estupendo. No le propongas a la gente que migre. Deja de darle publicidad gratuita. En definitiva No uses Git.Los desarrolladores de tu empresa lo agradeceran."
    } ,
  
    {
      "title"    : "Un vistazo a Roslyn",
      "category" : "",
      "tags"     : "dotnet, roslyn",
      "url"      : "/2016/02/09/un-vistazo-a-roslyn",
      "date"     : "2016-02-09 19:24:03Z",
      "content"  : "El numero de resultados que encuentras al buscar Roslyn es de 913 millones. Esta claro que esta de moda. Pero que es Roslyn y por que deberia importarmeRoslyn es un compilador de .Net. Es un analizador semantico de codigo. Es un binder. Es un refactorizador. Es open source. Es el segundo advenimiento del desarrollo. Emite IL. Esta integrado con Visual Studio. Pero funciona en cualquier plataforma. Es como el famoso jamon al que cantaban Los Berzas.Cuando escucho la palabra Roslyn me viene a la mente La Hora Chanante. Pienso en Joaquin Reyes repitiendo esta palabra. Primero mas agudo. Luego mas grave. Y para terminar alargando las vocales mientras agudiza el tono.Ultimamente he estado jugando un poco con Roslyn. Sobre todo porque el proximo 24 de febrero tengo una charla junto con mi companero Juan Bacardit en la DotNet Spain Conference 2016 en Madrid. Y he aqui los resultados de mis experimentosUna vez nos hemos instalado Roslyn en Visual Studio las plantillas y el SDK es muy facil empezar a trabajar. Tan solo tenemos que elegir una de esas plantillas y ya tendremos un ejemplo de lo que podemos hacerComo hacer tu IDE mas lentoLa plantilla que mas me fascina es la de Analyzer with Code Fix NuGet VSIX. Es un todo en uno para enlentecer tu entorno. Automaticamente te crea una regla de analisis de codigo y un refacor para corregirlo. Como resultado de compilarlo un paquete NuGet y un VSIX. Todo listo para ser instalado en todos los Visual Studio del mundo.Pero el analizador de demostracion no me gusta. Asi que vamos a implementar el nuestro propio. La idea es que cada vez que detectemos un nombre que NO contenga palabrotas nos avise. Asi que he anadido este archivopublic static class BadWordService    public static readonly string BadWords  new string  shit crap dick asshole motherfucker bastard prick jerk bitch damn fuck hell     public static bool ContainsBadWordsthis string source            return BadWords.Anys  source.ToLowerInvariant.Containss.ToLowerInvariant        public static string AddBadWordthis string source            var r  new Random        var word  BadWordsr.Next0 BadWords.Length  1        return string.Format012 source char.ToUpperInvariantword0 word.Substring1    Y he modificado el codigo del analyzer a algo como estoDiagnosticAnalyzerLanguageNames.CSharppublic class BadWordsRulesAnalyzer  DiagnosticAnalyzer    public const string DiagnosticId  BadWordsRules    private const string Title  No Bad Words in Name    private const string MessageFormat  There is no Bad Word in this name 0    private const string Description  There is no Bad Word in this name    private const string Category  Naming    private static DiagnosticDescriptor Rule  new DiagnosticDescriptorDiagnosticId Title MessageFormat Category DiagnosticSeverity.Warning isEnabledByDefault true description Description    public override ImmutableArray SupportedDiagnostics  get  return ImmutableArray.CreateRule      public override void InitializeAnalysisContext context            context.RegisterSymbolActionAnalyzeBadWordsInSymbolName SymbolKind.NamedType        private static void AnalyzeBadWordsInSymbolNameSymbolAnalysisContext context            var namedTypeSymbol  context.Symbol        var name  namedTypeSymbol.Name        AnalyzeNamecontext name namedTypeSymbol.Locations0        private static void AnalyzeNameSymbolAnalysisContext context string name Location location            if name.ContainsBadWords                    var diagnostic  Diagnostic.CreateRule location name            context.ReportDiagnosticdiagnostic            Al ejecutar se nos abrira una nueva instancia de Visual Studio. Si abrimos un proyecto veremos que nos marcara los nombres de las clases que no contengan palabras mal sonantes.Y a partir de aqui es donde viene lo bueno. Podemos hacer que el sistema nos proponga como corregir estos errores. Esto lo podriamos hacer modificando el archivo CodeFixExportCodeFixProviderLanguageNames.CSharp Name  nameofBadWordsRulesCodeFixProvider Sharedpublic class BadWordsRulesCodeFixProvider  CodeFixProvider    private const string title  Add Bad Word    public sealed override ImmutableArray FixableDiagnosticIds            get  return ImmutableArray.CreateBadWordsRulesAnalyzer.DiagnosticId         public sealed override FixAllProvider GetFixAllProvider            return WellKnownFixAllProviders.BatchFixer        public sealed override async Task RegisterCodeFixesAsyncCodeFixContext context            var root  await context.Document.GetSyntaxRootAsynccontext.CancellationToken.ConfigureAwaitfalse        var diagnostic  context.Diagnostics.First        var diagnosticSpan  diagnostic.Location.SourceSpan        var token  root.FindTokendiagnosticSpan.Start        context.RegisterCodeFix            CodeAction.Create                title title                createChangedSolution c  AddBadWordcontext.Document token c                equivalenceKey title            diagnostic        private static Task AddBadWordDocument document SyntaxToken token CancellationToken cancellationToken            var newName  token.Text.AddBadWord        return RenameAsyncdocument token cancellationToken newName        private static async Task RenameAsyncDocument document SyntaxToken token CancellationToken cancellationToken string newName            var semanticModel  await document.GetSemanticModelAsynccancellationToken        var typeSymbol  semanticModel.GetDeclaredSymboltoken.Parent cancellationToken        var originalSolution  document.Project.Solution        var optionSet  originalSolution.Workspace.Options        var newSolution  await Renamer.RenameSymbolAsyncdocument.Project.Solution typeSymbol newName optionSet cancellationToken.ConfigureAwaitfalse        return newSolution    Si ejecutamos este ultimo codigo nos propondra poner una palabrota al final de cada nombre de clase. Y al hacerlo buscara una aleatoria y la anadira. Haciendo el correspondiente refactor en todo el proyecto.Si al ejecutar no aparece esto instantaneamente dadle algo de tiempo. Como rezaba el titulo es una forma de hacer que Visual Studio vaya un poco mas lento.Una forma de no perder agilidad a la hora de programar es gastar este tiempo a la hora de compilar. Para esto sustituiremoscontext.RegisterSymbolAction...Porcontext.RegisterCompilationActionactionDonde en action tendremos que tener un rastreador del arbol de sintaxis en busca de objetos de tipo TypeDeclarationSyntax. Este es el tipo de objeto que almacena las declaraciones de las clases. Lo que nos lleva a lo mas importante a la hora de trabajar con roslyn conocer el arbol de sintaxis.A dia de hoy imagino que este arbol solo lo conocen los programadores de Roslyn. Si quieres empezar a jugar te recomiendo que abras en Visual Studio la ventana de Syntax Visualizer. Esta ventana se instala con el SDK de Roslyn. Y nos ayudara a entender que forma tiene un arbol de sintaxis para el documento que tengamos abierto.Como hacer lo mismo de antes con la mitad de trabajoOtra de las opciones es elegir el template de Code Refactoring VSIX. Esto es un template semejante al anterior. Pero haciendo solo la mitad del trabajo. Y lo mejor de todo es que mas de la mitad del codigo que ya hemos escrito nos sirve aqui.Cuando quieres proponer una modificacion pero no quieres que aparezca un aviso o que se marque aquel trozo de codigo que quieres modificar tenemos que usar los objetos CodeRefactoringProvider.Para tener la misma funcionalidad que en el ejemplo anterior solo tendriamos que insertar este codigo en la clase que se nos ha generadoExportCodeRefactoringProviderLanguageNames.CSharp Name  nameofBadWordsRefactorProvider Sharedinternal class BadWordsRefactorProvider  CodeRefactoringProvider    public sealed override async Task ComputeRefactoringsAsyncCodeRefactoringContext context            var root  await context.Document.GetSyntaxRootAsynccontext.CancellationToken.ConfigureAwaitfalse        var node  root.FindNodecontext.Span        var classDeclaration  node as ClassDeclarationSyntax        if classDeclaration  null return        var token  classDeclaration.Identifier        if token.Text.ContainsBadWords return        var action  CodeAction.CreateAdd a Bad Word c  AddBadWordcontext.Document token c        context.RegisterRefactoringaction        private static Task AddBadWordDocument document SyntaxToken token CancellationToken cancellationToken             igual que arriba        private static async Task RenameAsyncDocument document SyntaxToken token CancellationToken cancellationToken string newName             igual que arriba    Es facil verdad. El secreto de esta clase es que solo se evalua cuando estamos en una linea en concreto. Por eso el contexto tiene un Span. Esta marca nos dice donde esta el cursor en ese momento. Tan solo tenemos que comprobar que en nuestra posicion haya un tipo de objeto que esperamos.En este punto te plantearas la pregunta de como es que los desarrolladores de Resharper no van usar Roslyn Inconcebible. Me voy a hacer mi propio Resharper. Con casinos. Y furcias. Es mas paso de Resharper.Mi propio analizador outoftheboxImagina que quieres analizar tu codigo. Pero no quieres usar Visual Studio. Imagina que estas en Linux. Como puedo analizar mi codigo entonces Muy facil. Con la tercera plantilla que se instala StandAlone Code Analysis Tool.La primera sensacion cuando generas este tipo de proyecto es extrana. Un proyecto de consola vacio. En serio. Si en serio. Pero escondidas entre bastidores estan todas las referencias del mundo a todo lo que es compilar. Asi que a partir de aqui es facil seguir trabajando. Tan solo tenemos que anadir en el flamente metodo vacio Main este codigostatic void Mainstring args    var solutionFile  args1  valid VS Solution .sln    var ws  MSBuildWorkspace.Create    var soln  ws.OpenSolutionAsyncsolutionFile.Result    var proj  soln.Projects.Single    var compilation  proj.GetCompilationAsync.Result    foreach var tree in compilation.SyntaxTrees            var classes  tree.GetRoot.DescendantNodesAndSelf.Wherex  x.IsKindSyntaxKind.ClassDeclaration        foreach var c in classes                    var classDeclaration  ClassDeclarationSyntaxc            var token  classDeclaration.Identifier            if token.Text.ContainsBadWords continue            Console.WriteLinestring.FormatThere is no Bad Word in this name 0 token.Text                Console.ReadKeyEsta aplicacion al ejecutarla con la ruta de una solucion de Visual Studio nos dira que clases no tienen palabrotas en sus nombres. Creo que es una utilidad indispensable para todo Badass Developer.Rompiendose la cabezaLa ultima de las andanzas con Roslyn ha sido mirar el codigo fuente. Es open source. Lo teneis aqui httpsgithub.comdotnetroslyn.El codigo fuente de Roslyn es grande. Pero simple. Simplemente muy grande. Se puede entender. Pero hay que conocerlo. Simplemente lo conocen los que lo programan. Si quieres invertir horas y anadir una feature totalmente loca como cambiar la palabra clave static por motherfucker no sera demasiado trabajo. Solo tienes que saber que linea tocar.Tampoco me quiero extender mucho mas con este punto. El codigo fuente de Roslyn se actualiza cada poco. Y si escribimos un ejemplo con una linea en concreto quiza manana esa linea ya no este ahi.Lo que si que os recomiendo es que le echeis un vistazo. Es muy interesante. Aunque solo sea por curiosidad.Mis conclusionesRoslyn me gusta. La idea que hay detras tambien. Las herramientas que podemos crear son muy flexibles. Solo hay que esperar a que otros hagan lo que necesitamos. O si somos valientes y queremos cacharrear un poco hacerlo nosotros mismos.Si quereis ver algunos analyzers que ya esta implementando la comunidad solo teneis que echar un vistazo a esta web httpsgithub.comDotNetAnalyzers. Tiene muy buena pinta. Y aunque no esta todo al completo si que tienen muchas normas ya escritas.Pd. Para acceder a la demo final con todo tendreis que venir a vernos a la DotNet Spain Conference 2016. Sin presion P"
    } ,
  
    {
      "title"    : "SCBCN15 - #NoEstimates",
      "category" : "",
      "tags"     : "",
      "url"      : "/video/2015/10/19/scbcn15-no-estimates",
      "date"     : "2015-10-19 00:00:00Z",
      "content"  : "Aqui podras encontrar el video grabado de la sesion de la Software Craftsmanship Barcelona de 2015 sobre NoEstimates junto con Alex Casquete"
    } ,
  
    {
      "title"    : "SCBCN15 - Code Smells",
      "category" : "",
      "tags"     : "",
      "url"      : "/video/2015/10/06/scbcn15-code-smells",
      "date"     : "2015-10-06 00:00:00Z",
      "content"  : "Aqui podras encontrar el video grabado de la sesion de la Software Craftsmanship Barcelona de 2015 sobre Code Smells"
    } ,
  
    {
      "title"    : "Cach&eacute;s",
      "category" : "",
      "tags"     : "",
      "url"      : "/2015/03/18/caches",
      "date"     : "2015-03-18 10:57:29Z",
      "content"  : "La memoria cache es un tipo de almacenamiento menor que la unidad de almacenamiento principal pero con un acceso mas rapido que se utiliza en ingenieria de la computacion para aportar un mayor rendimiento al sistema. La cache no es un concepto nuevo dentro de la informatica es una memoria que se lleva utilizando anos y desde el punto de mas bajo nivel de una computadora procesador hasta el mas alto software.Fisicamente podemos encontrar 3 tipos de memorias cache diferenciadas por su cercania o lejania del procesador  Cache L1 Esta la podemos encontrar dentro del procesador muy rapida pero pequena. Es la memoria donde el procesador almacena los registros con los que hace operaciones.    Cache L2 Algo mas separada encontramos la L2 al lado de procesador mas grande que la L1 pero mas lenta. Aunque no obstante es muy rapida. Es la tipica cache que anuncian en los procesadores i7 con 4kb de cache. Esta se usa como memoria intermedia para almacenar mas datos y luego volcarlos a la L1 para operar.    Cache L3 Es la equivalente a la memoria RAM o de la del disco duro la RAM es mas rapida que el disco. Este es el nivel donde los vamos a mover a lo largo del resto de este articulo. Es la cache que como programadores de lenguajes manejados estamos capacitados para gestionar.  Tambien cabe mencionar que la memoria RAM a su vez tiene tambien una cache de acceso mas rapido que la propia RAM la SRAM donde se almacenan los datos que mas se usan. Y ademas puede usar la memoria de disco de la maquina si es que se ha llenado. Y los discos usan como cache el tipico buffer que se usa al leer un archivo. Pero quiza nos estamos desviando de la tematica principal.  El concepto que es importante que trascienda de aqui es que todo en los ordenadores usa siempre algun tipo de memoria cache .   Cache nomemory vs. inmemory En nivel de L3 se suelen diferenciar dos tipos de cache segun donde se almacenaran los datos fisicamente.Y este almacen otorgara unas caracteristicas y escenarios aplicables diferentes.  Nomemory Una cache persistida nomemory es aquella cache que queda guardada en una unidad de almacenamiento. Fisicamente quedara almacenada hasta que caduque. Un ejemplo es el outputcache de una pagina web. Al usar estos parametros los navegadores entienden que esos datos no se van a modificar. Asi que los guarda y no los vuelve a pedir al servidor usa la copia almacenada en su cache. Aunque apaguemos el ordenador estos datos no se borran la proxima vez que iniciemos estaran alli. Existen varias formas diferentes de usar la cache output dentro de una pagina web Si nos basaramos en el estandar HTML5 deberiamos crear un archivo de manifiesto donde indicariamos que archivos van a ser almacenados en la cache del navegador y cuales no. Un ejemplo de este archivo seria esteCACHE MANIFEST v1.0.0bootstrap.min.csssite.csslogo.gifmain.jsNETWORKAuthLoginFALLBACKAuthLogin Homeoffline.htmlCuando iniciamos el archivo siempre hay que declarar la directiva CACHE MANIFEST acto seguido iran los recursos que se almacenaran en la cache. Dentro de la directiva NETWORK iran aquellos recursos que siempre deben ser solicitados al servidor. Y en el caso de FALLBACK indicaremos el recurso alternativo a mostrar si es que no se encuentra una solicitud.Y por ultimo necesitaremos referenciar ese archivo en nuestra pagina webDOCTYPE htmlhtml manifestdemo.appcache    head        titleDemotitle        link rel..ContentCACHE_MANIFEST typetextcachemanifest     head    bodyltbodygthtmlAqui podemos observar dos apuntes. El primero es la declaracion de un atributo manifest en el tag html. Esto es necesario para que el navegador sepa que la pagina web actual va a usar un manifiesto. Y por convenio se suele nombrar como nombre de aplicacion.appcache. Y el segundo es la etiqueta link que apunta a nuestro archivo de manifiesto.Cualquier recurso referenciado para ser almacenado en cache de esta forma sera actualizado solo siEl usuario borra la cache del navegadorCuando el archivo de manifiesto sea modificado por eso es recomendable anadir un comentario con la version actual del archivoCuando se actualice la cache de la aplicacion programaticamentePor otro lado Asp.Net nos provee de una forma bastante sencilla de almacenar y configurar una pagina concreta en la cache del navegador.Si nos encontramos utilizando WebForms deberiamos anadir al inicio de nuestro archivo .aspx una linea semejante a esta Page LanguageC AutoEventWireuptrue CodeBehindDefault.aspx.cs Inherits...Default  OutputCache Duration3000 DOCTYPE htmlhtml xmlnshttpwww.w3.org1999xhtmlhead runatserver...Como podemos observar en el ejemplo hemos anadido una directiva OutputCache en la que indicamos que esta pagina sera almacenada en la cache durante 3000 segundos.Y si estuvieramos usando Asp.Net MVC seria algo parecido aunque en este caso tendriamos que anadirlo a nivel de accion del controladorpublic class HomeController  Controller    OutputCacheDuration  3000    public ActionResult Index            return View    Donde anadiremos un atributo OutputCache indicando tambien la duracion de 3000 segundos.InmemoryLa cache inmemory por otro lado se almacena en la memoria RAM de la maquina. De forma que si reiniciamos la maquina estos datos se borran y habra que volverlos a cargar en la cache. Un ejemplo de esto seria el HttpRuntimeCache de Asp.Net una cache que perdura en memoria y que gestiona por nosotros IISAsp.Net. Se le puede dar muchos usos pero uno de los mas tipicos seria el ejemplo de un combo de paises o ciudades. Datos que no suelen cambiar y que no queremos estar cargando constantemente de la base de datos. Los almacenamos en una memoria cache y accedemos a ellos de forma muy rapida.Implementar este ejemplo en una pagina web podria ser algo parecido a estoprotected void Page_Loadobject sender EventArgs e    if this.IsPostBack            var cities  HttpRuntime.Cache.Getcities as string        if cities  null                     cargar las ciudades en la variable cities            HttpRuntime.Cache.Insertcities cities null DateTime.Now.AddYears1 TimeSpan.FromSeconds3000                 cargar el combo con el listado de ciudades    Aqui intentariamos recoger de HttpRuntime.Cache un objeto que almacena las ciudades. Si no lo encontramos en la cache entonces lo cargaremos y lo almacenaremos dentro de la misma. Como detalle podemos observar dos parametros TimeSpan a la hora de insertar algo en la cache. El primero seria el tiempo de caducidad absoluto es decir que cuando se cumpla ese periodo de tiempo el objeto se borrara. Y el segundo es el tiempo de refresco. Este valor es el periodo de tiempo que pasa desde el ultimo acceso a ese objeto de la cache para que caduque. Es decir que cada vez que usamos ese valor de cache se empieza a contar desde cero hasta llegar a ese tiempo.HttpRuntime.Cache es de por si threadsafe. Esto significa que puede ser accedido desde diferentes threads hilos de ejecucion a la vez sin problemas. Pero las operaciones de lectura de datos no tienen por que ser threadsafe. Si en nuestro escenario ocurriera este problema entonces tendriamos que anadir un bloqueo a estas operacionesprivate static readonly object locker  new objectprotected void Page_Loadobject sender EventArgs e    if this.IsPostBack            lock locker                    var cities  HttpRuntime.Cache.Getcities as string            if cities  null                             cargar las ciudades en la variable cities                HttpRuntime.Cache.Insertcities cities null DateTime.Now.AddYears1 TimeSpan.FromSeconds3000                             cargar el combo con el listado de ciudades    Y si en lugar de tratarse de una pagina web y quisieramos tener una cache desde por ejemplo una aplicacion de Windows .Net nos provee del objeto MemoryCache que se encuentra en el ensamblado System.Runtime.Caching.dllusing System.Runtime.Cachingpublic void LoadData    var cache  MemoryCache.Defaultvar cities  cache.Getcities as stringif cities  null     cargar cities del almacen de datos    var item  new CacheItemcities cities    var policy   new CacheItemPolicy    policy.AbsoluteExpiration  DateTime.Now.AddYears1    policy.SlidingExpiration  TimeSpan.FromSeconds3000    cache.Setitem policyEn este ejemplo podemos encontrarnos el mismo escenario que el anterior pero esta vez usando MemoryCache. La mayor diferencia es la necesidad de usar un objeto de tipo CacheItem para almacenar la informacion y otro CacheItemPolicy para la gestion de la caducidad.Tipos de datos que manejamos en una cacheCuando hablamos de almacenar informacion dentro de una memoria cache podemos diferenciar el tipo de informacion en dependencia de como de volatil es esa informacion y quien puede acceder a la misma. Y encontramos tres grandes bloques de informacionDatos de referencia practicamente de solo lectura. No se modifican demasiado y son comunes a todos los usuarios. Por ejemplo el listado de productos de una tienda virtual o los spritesimagenes que luego se dibujan en pantalla en un videojuego.Datos de actividad son datos de lectura y escritura fundamentalmente cuyo ciclo de vida es el mismo que el de la sesion de un usuario. Por ejemplo el carrito de la compra de una tienda virtual o la puntuacion de un jugador en un videojuego.Datos de recursos Son datos de lectura y escritura compartidos por todos los usuarios pero que pueden cambiar con frecuencia. Estos datos tienen la peculiaridad de poder ser accedidos por muchos usuarios. Un ejemplo seria el stockage de los productos de una tienda virtual o las puntuaciones globales de todos los usuarios en un videojuego. Esta en manos del desarrollador saber en cada momento y aplicacion que datos seria mejor almacenar en que tipo de cache. No hay una formula magica para decidirlo aunque si algunos truquillos. Y tambien es importante saber que tampoco sirve la opcion de almacenar todo dentro de la cache. No es un martillo dorado que sirva para todo. Lo que si hay que tener en cuenta es que una cache puede perder los datos en algun momento por lo que no tenemos la seguridad de que esten ahi siempre aunque los hayamos almacenado ahi al iniciar la aplicacion... Cacheaside PatternUn concepto que tenemos que tener muy claro a la hora de utilizar memorias cache es que los datos que se almacenan en una cache no tienen por que estar ahi. Existen muchas razones por las cuales podrian no estar desde que no han sido cargados esos datos hasta que hayan caducado o incluso que el sistema haya decidido que necesita mas espacio y se haya deshecho de ellos.Por estas razones existe el patron cacheaside. Una forma muy simple de recogercargar datos de una memoria cache sin tener que tener miedo a que no existan. Su forma de funcionar es muy simpleY como muchos habran deducido ya hemos realizado una implementacion del mismo en los ejemplos anteriores. Aunque esa implementacion del patron era especifica no generica. Una implementacion generica del patron podria ser estapublic abstract class CacheBase    public T GetOrSetTstring key FuncT getter TimeSpan expiration  null            try                    if this.Existskey                            return this.GetTkey                            catch                    Trace.WriteEl objeto buscado en la cache no existe CachingInfo            var value  getter    try            this.SetltTgtkey value expiration        catch            Trace.WriteNo se ha podido almacenar un objeto en la cache CachingError        return valuepublic abstract bool Existsstring keypublic abstract T GetltTgtstring keypublic abstract void SetltTgtstring key T value TimeSpan expiration  nullAunque tambien hay algunos proveedores de cache que no tienen la instruccion Exists y que si no encuentran un objeto simplemente devuelven null. Esto se basa en la teoria de que una cache no debe devolver errores ya que perderiamos mas tiempo gestionandolos que lo que ganamos usando la cache en si. Para esos proveedores una implementacion mas cercana podria ser la siguientepublic abstract class CacheBase    public T GetOrSetTstring key FuncT getter TimeSpan expiration  null            try                    var data  this.GetTkey            if data  null                            return data                            catch                    Trace.WriteEl objeto buscado en la cache no existe CachingInfo            var value  getter    try            this.SetltTgtkey value expiration        catch            Trace.WriteNo se ha podido almacenar un objeto en la cache CachingError        return valuepublic abstract T GetltTgtstring keypublic abstract void SetltTgtstring key T value TimeSpan expiration  nullLimitaciones de una cacheEs tan importante conocer las virtudes de usar cache como sus posibles limitaciones. Dentro de estas podriamos enumerarTiempo de vida de objetos en cache limitadouna gran parte de las memorias cache que existen implementan una politica de expiracion de los objetos que almacenamos en ella. Esto significa que un objeto tiene un tiempo de vida concreto. Si introducimos un valor de expiracion muy alto es posible que terminemos teniendo un objeto ya no valido. El uso de la cache esta recomendado sobre todo para datos practicamente estaticos o que se leen con mucha frecuencia.Desalojo de datos tambien la mayoria de las caches implementan un sistema automatico de desalojo de objetos. Como la memoria de la cache es limitada cuando esta se llena se opta por borrar los objetos mas antiguos. Aunque podriamos implementar un modo de desalojo global con un tiempo determinado para cualquier objeto podria ser que haya objetos que sea recomendable saltarse estas limitaciones por razones de performance y de que el dato no varia con facilidad.Preparacion inicial de cache Muchos programas a la hora de arrancar realizan un llenado inicial de varios objetos en la memoria cache y alguno de ellos pueden ser usados en el proceso de inicio. Si usamos el patron cacheaside podemos garantizar que leemos correctamente estos datos aunque hayan caducado.Consistencia de la informacion Lo primero que perdemos al utilizar memorias cache es la consistencia de los datos. No tiene por que coincidir el objeto que esta almacenado en cache con el que esta en el almacen de datos. Y este problema se acentua en la medida de la frecuencia de modificacion de un dato que tenemos almacenado en la memoria cache.Caches locales Si usamos una aplicacion con varios servidores balanceados y cada uno usa una cache local ademas de tener un dato replicado en diferentes sitios podria ser que existiera inconsistencia entre datos de la cache de un servidor y de otro. Esto tiene una facil solucion gestionando caches distribuidas como veremos mas adelante. Alternativas a usar una cacheComo ya hemos mencionado no siempre usar una cache sera la solucion a nuestros problemas. Existen alternativas para disminuir la latencia de un sistema. Algunas de las mas comunes serianEscalar el sistema lo que vulgarmente conocemos como anadir mas hierro. Cuando escalamos horizontalmente o verticalmente un sistema mejoramos la performance porque las funciones que antes se realizaban usando una serie de recursos ahora pueden utiliza muchos mas. Pero el problema del escalado es que por lo general es caro. Ademas podemos estar escondiendo problemas mas serios en nuestra aplicacion a base de hacer que funciones en maquinas mas potentes.Podriamos mejorar la performance de los procesos. Que puede resultar mas caro que anadir mas hierro. Pero esto nos asegura tener una mejor aplicacion. Aunque hay que entender que los procesos estan limitados por la tecnologia y los sistemas.Almacenar y cargar agregados y no pequenas entidades de una en una. Almacenar agregados tampoco nos va a aportar una velocidad tan grande aunque es muy recomendable.Usar variables de sesion o la cache output. Pero las variables de sesion estan solo disponibles para una instancia de una aplicacion concreta no para todas. A no ser que la persistamos con lo que perdemos ese plus de velocidad. Y la cache output la gestiona el cliente bajo unas directrices del servidor por lo que no podemos borrarla bajo demanda. Solo el propio cliente es el que la puede manejar. Probablemente si nuestro sistema tiene problemas de performance la solucion no sera solo implementar caches en algunos puntos concretos de la aplicacion. Es muy posible que tengamos que aplicar estas tecnicas que mencionamos y otras tantas para conseguir subsanar todos los problemas actuales y futuros que nos encontremos.Cache distribuidaEn los entornos actuales una aplicacion puede estar alojada en diferentes maquinas. Puede escalar en diferentes instancias. Hoy en dia nos encontramos con sistemas hibridos cloudonpremises o sistemas totalmente en la nube. Para todos estos escenarios las caches locales inmemory se nos antojan algo desfasadas porque nuestras arquitecturas requieren de un solo componente que no tenga que estar replicado por cada una de las instancias de una aplicacion. Y es aqui cuando aparece una forma nueva de usar la cache cache distribuida.Un sistema de Cache distribuido no consiste mas que en tratar la memoria cache como un nuevo servicio que pueden consumir el resto de aplicaciones de un datacenter.Pero gestionar y asegurar una alta disponibilidad en un sistema de cache distribuido puede ser un problema debido a que surgen nuevos retos Hay que pensar en el nuevo hierro.Debemos gestionar estos sistemas mantenerlos actualizados configurarlos. Tenemos que ser capaces de integrar esa cache distribuida con el resto de los sistemas. Hay que tener un plan de escalabilidad si es que en un momento determinado se nos quedan cortos los recursos. Todo servicio en produccion debe de constar de un plan de contingencia para tratar los problemas. .Todos estos problemas nos llevan a utilizar herramientas de terceros para tener una solucion fiable y robusta. Y dentro de las soluciones de terceros tenemos las caches distribuidas en las nubes de las grandes empresas como son los diferentes servicios de cache de Microsoft Azure.Cache en azureDentro de la plataforma Microsoft Azure tenemos varios servicios de cache en los que no tendremos que preocuparnos mas que de decidir su tamano. Y si estos sistemas no cubren nuestras necesidades siempre estara posibilidad de crear una maquina virtual donde podemos instalar el servidor que mas nos guste de caches. En este articulo hemos tratado las siguientes opcionesAzure Cache InRoleAzure Cache ServicesMaquina virtual con memcachedAzure Redis CacheAzure Cache InRole El Azure Cache InRole es un servicio disponible en principio para los Web Roles y los Worker Roles de los Cloud Services. Nos permite utilizar una cantidad de memoria de uno de estos roles o bien Worker Roles dedicados como memoria cache. La cache InRole funciona como un cluster podemos crear varias instancias de los diferentes roles y todos estos sumaran su memoria a la memoria cache del Cloud Service. Ademas no tendremos que preocuparnos de dificiles configuraciones ya que es un proceso automatico.Para poder desplegar esta memoria de una forma sencilla la primera opcion pasa por crear un nuevo proyecto Cloud usando Visual StudioPara despues en el dialogo de configuracion del Cloud Service anadir un rol de cache dedicadoCon esto ya tendriamos nuestro proyecto preparado para usar la cache InRole. Para configurar una cantidad de memoria u otra tendremos que jugar con el tamano de la maquina que lo hospedara y el numero de instancias recordemos que todas suman. Esto lo podemos configurar en la pagina de propiedades del rol en el proyecto de Cloud ServiceY para poder consumir esta cache en otro de los roles que tengamos dentro del mismo Cloud Service tendremos que anadir una referencia al paquete de nuget Microsoft.WindowsAzure.Caching. Entonces veremos que en el archivo de configuracion de nuestro rol ha anadido una nueva seccion donde tan solo tendremos que introducir el nombre que le hemos puesto al rol que actuara como cachedataCacheClients  dataCacheClient namedefault    autoDiscover isEnabledtrue identifiernombre del rol   dataCacheClientdataCacheClientsY para poder operar desde c podremos heredar de nuestra clase CacheBase e implementar las funciones necesariaspublic class AzureCache  CacheBase    private readonly DataCacheFactory factory  new DataCacheFactorypublic MicrosoftCache    factory  new DataCacheFactoryprotected override T OnGetltTgtstring key    var cache  this.GetCurrentCache    return Tcache.Getkeyprotected override void OnSetltTgtstring key T value TimeSpan expiration  null    var cache  this.GetCurrentCache    if expiration.HasValue        cache.Putkey value expiration.Value    else        cache.Putkey valueprotected override void OnRemovestring key    var cache  this.GetCurrentCache    cache.Removekeyprivate DataCache GetCurrentCache    return factory.GetDefaultCacheHay que tener en cuenta que la cache InRole para comprobar que un objeto no existe devuelve null al solicitarlo asi que no hara falta implementar la funcion Exists ya que seria una reiteracion recoger el valor dos veces.Pero la cache InRole esconde un huevo de pascua y es que contiene un gateway que nos permite utilizarla como un servidor memcached. Esto puede resultar verdaderamente util cuando queremos hospedar paginas que ya usaban este tipo de servidores.Para poderlo usar de esta forma tendremos que abrir el puerto 11211 y ponerle el nombre al endPoint de memcache_defaultNo se recomienda que este endPoint sea publico por cuestiones de seguridad es decir que carece de cualquier medida de proteccion. Por lo que es mas recomendable dejarlo como un endPoint interno para su uso exclusivo dentro del Cloud Service.Para poder consumir esta cache a traves del gateway tendremos que usar un proveedor de Memcached. En este caso lo que suelo usar es el paquete de nuget Enyim.Caching.Memcached.Y su implementacion partiendo de la clase CacheBase seria algo parecido a estopublic class MemcachedCache  CacheBase    private readonly IMemcachedClient clientpublic MemcachedCache    this.client  new MemcachedClientprotected override T OnGetltTgtstring key    return client.GetltTgtkeyprotected override void OnSetltTgtstring key T value TimeSpan expiration  null    client.StoreStoreMode.Set key valueprotected override void OnRemovestring key    client.RemovekeyAzure Cache Services Es un servicio distribuido dedicado solo a gestionar caches.Su creacion se ha de realizar desde PowerShellPS C AddAzureAccountVERBOSE Account userdomain.com has been added.VERBOSE Subscription MySubscription is selected as the default subscription.VERBOSE To view all the subscriptions please use GetAzureSubscription.VERBOSE To switch to a different subscription please use SelectAzureSubscription.PS C NewAzureManagedCache Name mycache Location West Europe Sku Basic Memory 128MBVERBOSE Intializing parameters...VERBOSE Creating prerequisites...VERBOSE Verify cache service name...VERBOSE Creating cache service...VERBOSE Waiting for cache service to be in ready state...Name      mycacheLocation  West EuropeState     ActiveSku       BasicMemory    128MBPS CY si queremos conectarnos a este servicio bastara con usar el codigo que hemos mostrado anteriormente para conectar con Azure Cache InRole pero modificando en el web.configdataCacheClients  dataCacheClient namedefault    autoDiscover isEnabledtrue identifierurl del servicio     securityProperties modeMessage sslEnabledtrue      messageSecurity authorizationInfocontrasena del servicio     securityProperties  dataCacheClientdataCacheClientsMaquina virtual con memcached Memcached es un servidor muy extendido para caches distribuidas. Se usa en una gran cantidad de aplicaciones y resulta muy rapido. Actualmente no existe ningun servicio PaaS que nos provea de esta cache asi que tendremos que instalarlo en una maquina virtual.Para crear una maquina virtual con el servidor de memcached basta con ir al portal antiguo de azure y crear una maquina desde la galeriaUna vez ahi elegiremos una imagen de linux. En este caso hemos seleccionado un Ubuntu Server porque se basa en debian que es la distro de linux que siempre hemos manejado y con la que nos sentimos mas comodos. Pero en realidad se podria elegir cualquiera.No hay que olvidar configurar un usuario para el acceso remoto por SSH. Que es lo que necesitaremos para acceder al terminal de la maquina de forma remota y poder ejecutar comandos.Con este simple comando ya tendremos un servidor basico instaladosudo aptget install memcachedSi quisieramos hacer que fuera accesible desde una maquina diferente a la que estamos tendremos que editar el archivo etcmemcached.conf y sustituir esta linea Specify which IP address to listen on. The default is to listen on all IP addresses This parameter is one of the only security measures that memcached has so make sure its listening on a firewalled interface.l 127.0.0.1Por esta otral 0.0.0.0Y despues deberiamos reiniciar el servicio.Para acceder a esta memoria cache podriamos usar el conector que hemos creado anteriormente para Azure Cache InRole Gateway configurando correctamente el servidor.Azure Redis CacheAunque dentro de los servicios PaaS de Microsoft Azure se vende redis con un servidor de cache distribuida en realidad redis es mucho mas que esto. Es un servidor de base de datos NoSQL que almacena la informacion en forma de clavevalor. Ademas es capaz de trabajar con 5 estructuras de datos diferentes implementa colas con bloqueo pubsub transacciones y muchas mas cosas. Mucha gente define redis como un memcached con esteroides.Y por esta razon se nos antoja escaso el espacio que ha quedado dentro de este articulo para hablar de redis tan solo comentaremos que existen varios conectores muy simples de utilizar ServiceStack.Redis StackExchange.Redis y uno que estoy desarrollando actualmente con el nombre de Tokiota.Redis. Y que espero escribir a no mucho tardar un articulo mucho mas extenso sobre como utilizar redis desde plataformas .Net y sacarle todo el partido posible asi que estad atentos . Comparativas de cache en azureYa que nos hemos puesto a analizar muchos servicios de cache distribuidos sobre la plataforma azure no podriamos despedirnos sin mostrar una tabla comparativa de la performance en forma de media de tiempos de cada uno. Para realizar esta prueba se ha montado un Worker Role en azure desde donde se han ido realizando llamadas a los siguientes servicios y utilizando los proveedores senaladosLa primera deduccion que podemos sacar de aqui es que el proveedor de memcached de Enyim es posiblemente una implementacion muy buena que ademas se ve favorecida por el protocolo tan ligero que manejan este tipo de servidores.Pero aun asi vemos bastante diferencia del servidor de memcached con respecto la segunda envelocidad Azure Cache InRole a traves del gateway de memcached. Esta diferencia se basa en que el segundo se trata de un cluster donde se garantiza una alta disponibilidad y una gestion de la cache entre varias maquinas virtuales mientras que memcached es una instalacion standalone en una maquina virtual linux.Las diferencias que vemos con las dos formas de acceder al Azure Cache Service responden en exclusiva al proveedor y protocolo que son mucho mas ligeros para memcached. Pero no hay que olvidar que El proveedor de cache de Azure de Microsoft nos provee de varias opciones y configuraciones mas que en memcached no encontramos como por ejemplo el expiration sliding time.Sobre redis podemos comentar que a los drivers les queda aun camino por recorrer en cuanto a performace se refiere. La implementacion Tokiota.Redis en la que me encuentro trabajando a la hora de escribir esta articulo todavia esta incompleta aunque ya juega con tiempos mejores. Pero hay que entender que la empezamos a desarrollar con ese objetivo .Y Azure Cache Service termina como el termino medio. No es el mas rapido pero tienes tantas posibilidades en cuanto servicios de cache aunque redis tiene mas de otro tipo. Gestiona clusters muy grandes de cache y puede ser consumido desde casi cualquier lenguaje de programacion conocido.De cualquier forma todas estas cifras que mostramos son milisegundos por lo que podemos determinar que ninguno de los sistemas es especialmente lento y en nuestras aplicaciones notaremos muy poca diferencia de uno a otro. A no ser que abusemos del uso de la memoria cache.Y hasta aqui este articulo que empezaba como una pequena nota acerca de unas charlas en las que he tenido el placer de participar y donde he podido hablar de caches. Para terminar os publicamos algunas referencias que a muchos os resultaran interesantes para seguir investigando.Referenciashttpwww.wikipedia.orghttpwww.stackoverflow.comhttpwww.msdn.comhttpsmsdn.microsoft.comenuslibrarydn568099.aspx Microsoft Azure Documentationhttpredis.iohttpoldblog.antirez.composttakeadvantageofredisaddingittoyourstack.html httpwww.memcached.orghttpsgithub.comenyimEnyimMemcachedhttpsgithub.comServiceStackhttpsgithub.comStackExchangehttpsgithub.comfernandoescolarTokiota.Redis "
    } ,
  
    {
      "title"    : "Load Test - Cómo crear un Web Performance Test",
      "category" : "",
      "tags"     : "",
      "url"      : "/video/2014/02/17/load-tests-2",
      "date"     : "2014-02-17 00:01:00Z",
      "content"  : "En esta serie de videos se explicaran las pruebas de carga y como desarrollarlas usando la herramientas de Microsoft Visual Studio.Video 2 A lo largo de este video veremos como podemos crear un Web Performance Test. La unidad basica para crear pruebas de carga para nuestros servicios y paginas web.Realizados por Fernando Escolar  es un entusiasta del desarrollo y especialista en tecnologias .Net de Microsoft que trabaja como Development  ALM Advisor en Tokiota. Con mas de 10 anos en el sector es seguidor de metodologias agiles y de los grupos Agile Barcelona y Agile Spain. Ademas miembro del grupo de usuarios CatDotNet. Convencido de que hay una forma mejor de hacer las cosas y en un constante estado de aprendizaje.Musica por templateswise."
    } ,
  
    {
      "title"    : "Load Test - Introducción a las pruebas de carga",
      "category" : "",
      "tags"     : "",
      "url"      : "/video/2014/02/17/load-tests-1",
      "date"     : "2014-02-17 00:00:00Z",
      "content"  : "En esta serie de videos se explicaran las pruebas de carga y como desarrollarlas usando la herramientas de Microsoft Visual Studio.Video 1 Introduccion a las pruebas de carga. Explicacion de que son para que sirven y su diferenciacion con las pruebas de estres.Realizados por Fernando Escolar  es un entusiasta del desarrollo y especialista en tecnologias .Net de Microsoft que trabaja como Development  ALM Advisor en Tokiota. Con mas de 10 anos en el sector es seguidor de metodologias agiles y de los grupos Agile Barcelona y Agile Spain. Ademas miembro del grupo de usuarios CatDotNet. Convencido de que hay una forma mejor de hacer las cosas y en un constante estado de aprendizaje.Musica por templateswise."
    } ,
  
    {
      "title"    : "Mesa redonda sobre SPA",
      "category" : "",
      "tags"     : "",
      "url"      : "/video/2014/01/22/mesa-redonda-sobre-spa",
      "date"     : "2014-01-22 17:57:42Z",
      "content"  : "El pasado miercoles dia 15 de enero realizamos una mesa redonda via Hangout Air para tratar sobre retos estado y el por que de las apps SPA. Contamos con la colaboracion de Alfredo Fernandez afernandez_l Albert Margarit albertmargarit Marc Rubino Marc_Rubino Oscar Sotorrio osotorrio Tomas Corral amischol y Fernando Escolar fernandoescolar.Y he aqui el resultado de tan agradable reunionSobre los participantesafernandez_l Alfredo lleva mas de 6 anos desarrollando mayoritariamente centrado en proyectos web utilizando tecnologias Microsoft. Actualmente trabaja en Plain Concepts como desarrollador de software y trainer. Tambien participa en varias comunidades .NET como CatDotNet ArtaldeNet y SecondNug y colabora con varias iniciativas para la organizacion de eventos de alta calidad en todo el pais. httpbilbostack.comspeakersalfredofernandezalbertmargarit Albert Margarit es ingeniero tecnico informatico apasionado de las ultimas tecnologias y del desarrollo tanto en el frontend como en el backend. Esta especializado en tecnologias Microsoft. Su carrera como desarrollador le ha llevado a trabajar profesionalmente con sistemas orientados a servicios aunque esta centrado sobre todo en el desarrollo web. Fanatico del estudio e implantacion de arquitecturas y patrones de diseno en sistemas. Ademas de trabajar profesionalmente con JavaScript y SPA es autor del curso de Single Page Applications en CampusMVP.Marc_Rubino Marc Rubino Key Consultant en Pasiona entusiasta de la tecnologia con mas de 10 anos de experiencia en aplicaciones web y consultoria tecnica. Fundador del grupo de usuarios .Net LonetCamp. Colaborador habitual con los grupos de usuarios y la fundacion Techdencias. Microsoft MVP ASP.NETIIS desde el 2011.osotorrio Oscar Sotorrio Sanchez es desarrollador Web en ASG. Eterno aprendiz en esto de las tecnologias .NET y en especial con ASP.NET. Admirador de la filosofia de Internet y entusiasta de los nuevos modelos de negocio que rigen este mundillo.amischol Profesional del desarrollo durante anos especializado en Javascript aunque he trabajado con Java PHP y Python. Me considero sobre todo amante del conocimiento como docente y aprendiz que es mas importante si cabe. Perfeccionista y seguidor a ultranza de las metodologias agiles. Actualmente trabajo de arquitecto Javascript en Softonic.com y lo compagino con la gestion y organizacion de varios eventos como el WeLoveJS Charlas y Workshops sobre JS  Coder Dojo y VailetsHacklabs Dedicado a ensenar a ninos a programar.fernandoescolar Fernando es un entusiasta del desarrollo y especialista en tecnologias .Net de Microsoft que trabaja como Development  ALM Advisor en Tokiota. Con mas de 10 anos en el sector es seguidor de metodologias agiles y de los grupos Agile Barcelona y Agile Spain. Ademas miembro del grupo de usuarios CatDotNet. Convencido de que hay una forma mejor de hacer las cosas y en un constante estado de aprendizaje.Links de interesPagina de Hangout Air del evento original httpsplus.google.comu0eventscufio59kfjdrd74dp5iha9lr7l0 Durandal httpdurandaljs.com AngularJS httpangularjs.org Backbone.JS httpbackbonejs.org Rendr httpsgithub.comairbnbrendr i18Next httpi18next.com Hydra.JS httptcorral.github.ioHydra.js Brick httpsgithub.commozillabrick"
    } ,
  
    {
      "title"    : "La peque&ntilde;a gran gu&iacute;a sobre Load Tests",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/10/21/la-pequea-gran-gua-sobre-load-tests",
      "date"     : "2013-10-21 09:41:58Z",
      "content"  : "Las pruebas de carga se usan para observar y analizar un sistema en unas circunstancias especificas. Por ejemplo podriamos crear un portal web en el que esperamos un numero determinado de usuarios. Y antes de ponerlo en produccion queremos estar seguros de que se va a comportar correctamente cuando lleguemos al cupo esperado.La idea es montar un entorno de pruebas que simule estas condiciones y asi monitorizar los servidores implicados en busca de unas estadisticas. El resultado de ejecutar este tipo de prueba es una gran cantidad de datos que nos informaran de si el sistema se ha comportado como esperabamos y que cuellos de botella se han encontrado.Hay que tener cuidado y no confundir las pruebas de estres con pruebas de carga. Las de estres tratan de llevar al limite un sistema para poder observar si tiene estabilidad en condiciones dificiles y encontrar cual es el maximo que le podemos pedir antes de que empiece a fallar. Mientras que las de carga aunque son muy parecidas y de una se puede sacar la otra cambiando unos parametros de la configuracion no buscan llegar al limite. Solo observar el comportamiento en unas condiciones normalesesperadas.A lo largo de este articulo vamos a tratar varios temas y puede ser que resulte algo denso. La intencion no es explicar todos los detalles de este tipo de pruebas ya que seria una labor titanica. Simplemente mostraremos las actuaciones tipicas que solemos realizar en nuestros proyectosEntorno de testCreando un test de web performanceCustom extraction rules and validatorsConfigurando el test de cargaEjecutando y explotando los resultadosPAL Performance Analysis LogsConclusionesEntorno de testAntes de empezar a trabajar tenemos que tener licencias e imagenes de dvd de Visual Studio Ultimate y los Visual Studio Agents. Todo este software se puede encontrar en una cuenta de MSDN con licencia Ultimate. Los test de carga no pueden ser ejecutados desde un Visual Studio de licencia inferior simplemente no tendra las opciones.La idea de todo entorno para este tipo de pruebas es usarUn ordenador con Visual Studio como cliente que solicita el lanzamiento de una prueba y muestra los resultados.Un controlador que sirve para orquestar la ejecucion de las pruebas y recolectar los datos.Una serie de agentes cuya mision es simular el escenario ejecutar las pruebas.Para tener este entorno se recomienda bien crear una serie de maquinas virtuales en azure o bien fisicamente poseer estos equipos e instarles el software de Visual Studio Test Agent o Test Controller segun corresponda.En la configuracion del controlador se nos solicitara la cuenta que va a ejecutar el servicio opcionalmente si queremos registrar el controlador en un TFS para ejecutar las pruebas desde ahi y por ultimo un servidor de base de datos donde almacenar los resultados.En cuanto a los agentes despues de preguntar si deseamos ejecutar el agente como un proceso interactivo o como un servicio recomendamos esta ultima deberemos especificar la cuenta encargada de ejecutar el servicio y cual es el equipo que tiene instalado el controlador.En la nueva version de Visual Studio 2013 no tendremos la obligacion de crear este entorno ya que los tests de carga se podran ejecutar directamente en el TFService azureBastara con configurar un archivo de Test Settings y especificarlo. Luego en el Team Explorer habra que conectarse con una cuenta de TFService TFS en la nube y ya se ejecutaran alli automaticamente.Para terminar con las configuraciones a parte de esto tambien tendremos que desplegar nuestro aplicativo de tal forma que sea accesible por el entorno que hemos creado. Pero esto ya depende de cada uno.Volver al indiceCreando un test de web performanceUn test de carga esta compuesto por un conjunto de juegos de prueba que pueden ser de todo tipo. Pero en el caso de querer probar una aplicacion web lo mas recomendable es generar ese juego de pruebas que vamos a ejecutar en formato de Web Performance Tests.Un Web Performance Test esta compuesto por una serie peticiones web dentro de estas peticiones podemos especificar varias propiedades. Ademas podemos crear unas reglas para extraer datos que vienen en la respuesta y unos validadores que nos ayudaran a saber si el resultado de esa peticion es el esperado.Para crear un Web Performance Test vamos a necesitar tener primero un proyecto de tipo de load test abierto. Entonces a este proyecto le anadimos un nuevo elemento de tipo Web Permormance Test y se nos abrira una ventana de Internet Explorer con el plugin Web Test Recorder a mano izquierdaNuestra mision ahora sera ir ejecutando un caso de prueba desde nuestra web. Por ejemplo puede ser realizar una busquedaUna vez hayamos terminado presionamos en el boton de Stop. Entonces volvera a aparecer en primer plano el Visual Studio con las request capturadas dentro de nuestro test. Ahi el sistema analizara las relaciones entre las peticiones. Es decir que si por ejemplo un input de tipo hidden tiene un valor que se pasa en un post de un formulario hacerlo dinamicamente sin que nosotros tengamos que especificarlo. Una vez haya realizado estas comprobaciones tendremos algo como estoAqui vemos que hemos realizado dos peticiones una a la pagina principal y otra que es la busqueda que contiene una serie de datos en forma de parametros. Una de las primeras acciones que es recomendable hacer es parametrizar los servidores web presionando el segundo boton del raton y seleccionando la opcion de Parameterize Web Servers. Aqui nos propondra una variable de contexto con nuestro nombre de servidor. Asi si cambiamos de entorno solo tenemos que modificar el valor de este parametro.Ahora veremos que opciones tenemos de configuracion por cada peticion. Si marcamos la primera por ejemplo en la ventana de propiedades encontraremos algo parecido a estoAqui podremos modificar todos los datos de la request como por ejemplo si vamos a usar Cache nos puede interesar o no en este tipo de pruebas si esperamos un codigo HTTP de respuesta concreto o el Think Time. Este ultimo es importante ya que nos ayudara a imitar el comportamiento de un usuario de verdad. El Think Time es el tiempo que se anade despues de esta request en el que simulamos que un usuario puede tardar en leer hacer clic o escribir algo en pantalla. Su valor equivale a la cantidad de segundos que se esperara para ejecutar la siguiente peticion web.Si pulsamos con el segundo boton del raton sobre una de las peticiones apareceran ante nosotros un buen numero de opcionesLas primeras opciones estan relacionadas con la prueba y las demas con la peticion seleccionada. A nivel de prueba podremosInsertar una request manuamente.Crear una transaccion. Esto es un conjunto de peticiones web agrupadas. Podria ser una operacion completa como por ejemplo varias request que son necesarias para anadir un nuevo usuario en el sistema. Al final se nos mostraran estadisticas concretas respecto a las transacciones de nuestros tests.Podriamos crear un bucle que nos permitiria repetir un conjunto de peticiones segun varias cosas como por ejemplo un numero determinado de veces o incluso mientras una condicion ocurra o no.Cuando anadimos una condicion podemos dividir la prueba en diferentes partes dependiendo de como se encuentre el escenario.Tambien podriamos anadir un comentario por si es necesario especificar algo determinado.Anadir una llamada a otro Web Test que hayamos creado.Y por ultimo volver a grabar en Internet Explorer una serie de peticiones y anadirlas.Con respecto a las opciones referentes a la peticion seleccionada encontraremosPodemos anadir peticiones dependientes que podrian ser por ejemplo llamadas a servicios via Ajax dentro de una carga asincrona. En realidad en el tiempo de ejecucion el sistema encontrara varias peticiones dependientes por lo que por lo general en un escenario normal no anadiremos nada aqui. En todo caso nuestra mision podria ser recoger una request posterior y moverla a esta seccion.Anadir una cabecera que no haya detectado.Anadir parametros de Query String que son los que vienen con la URL.Tambien podemos crear nuevos parametros de tipo formulario.O simular un POST de un archivo.Cada peticion puede tener una serie de validadores que pueden ir desde encontrar algun contenido concreto hasta lo que queramos crear. Si la validacion falla entonces el test dara error.Otra caracteristica importante son las reglas de extraccion. Dentro de una peticion puede haber un dato que sea de interes para las peticiones venideras. Si usamos un extractor el dato quedara almacenado en el contexto de la prueba. Este contexto es comun para todas las peticiones de la misma.Podemos ayudarnos de plugins para aislar codigo que se ejecuta antes yo despues de una peticion.La opcion de buscar y remplazar busca todas las coincidencias de un texto y las remplaza por algo nuevo.Y por ultimo las conocidas opciones de copiar cortar y pegar a las que estamos acostumbrados.Como hemos dicho antes un Web Performance Test posee un contexto de ejecucion que es comun para todas las peticiones que contiene. En este contexto podemos crear y borrar datos para ponerlos a disposicion de ciertas peticiones que lo necesiten. Para hacer uso de estas variables de contexto se ha creado un sistema plantillado para poner valor a los parametros uri headers querystring o form body. Consiste en usar el mismo nombre con el que hemos guardado el dato entre dos llaves mi_valor_del_contexto. El ejemplo mas claro de esto lo veremos en nuestro test al parametrizar el servidor web que ha cambiado todas las URI de las peticiones para que contengan WebServer1 en lugar del nombre del servidor.Para cambiar los valores y sustituirlos por parametros tendremos que seleccionar uno de ellos y en la ventana de propiedades veremos esta serie de opcionesComo vemos en la captura podremos cambiar el valor por uno de los parametros contextuales o incluso anadir una fuente de datos como podria ser un Excel o un XML de donde podremos recogerlos. Si el parametro que queremos usar no se encuentra en el listado podremos escribir el literal con las llaves.Una vez que tenemos el test a nuestro gusto podremos ejecutarlo y en los resultados observaremos todo lo que ha ido pasando.Si observamos detenidamente los detalles veremos que no solo ha cargado las requests que le hemos definido si no que se ha ido operando como un navegador de internet descargando el contenido dependiente y respondiendo de la forma correcta cuando se encuentre escenarios que lo requieran.Volver al indiceCustom extraction rules and validatorsAunque existen ya de por si muchas reglas de extraccion y validacion puede ser que estas no se cinan a nuestras necesidades especificas. Aunque antes de desarrollar una nueva recomendamos echar un vistazo a todas las opciones ya que incluso existe una norma de extraccion generica donde podemos introducir una expresion regular.Para crear una regla de extraccion simplemente heredaremos de la clase base ExtractionRule anadiremos unos decoradores de descripcion y nombre para que se muestre en los dialogos de configuracion de una forma correcta y sobrescribiremos la funcion de ExtractDescriptionThis is a demo extraction ruleDisplayNameMy Custom Extraction Rulepublic class CustomExtractionRule  ExtractionRule    public override void Extractobject sender ExtractionEventArgs e            var bodyText  e.Response.BodyString         search something ...        e.WebTest.Contextthis.ContextParameterName  found    Lo que tendremos que hacer en el metodo de extraccion sera recoger un dato de la respuesta del servidor y anadirlo al contexto para que este disponible para otras pruebas venideras. Es simple pero a la vez podemos complicarlo todo lo que queramos.En cuanto a las reglas de validacion funcionan con la misma dinamica con la diferencia de que heredaremos de la clase ValidationRule y el metodo que sobrescribiremos sera ValidateDescriptionThis is a demo validation ruleDisplayNameMy Custom Validation Rulepublic class CustomValidationRule  ValidationRule    public override void Validateobject sender ValidationEventArgs e            var bodyText  e.Response.BodyString         validate something        e.IsValid  result        e.Message  result  Ok  Something went wrong    Una vez hayamos validado la informacion que nos interesa indicaremos los resultados en los argumentos de la funcion. Si la regla de validacion responde como que no es valido la prueba que se esta ejecutando en ese momento sera fallida.Volver al indiceConfigurando el test de cargaUna vez hemos creado un buen juego de pruebas web concretamente las que consideramos que van a realizar los usuarios en el sistema ya podemos definir una prueba de carga.Para esto anadiremos a la solucion un nuevo Load Test. Al hacerlo aparecera una ventana tipo Wizard que nos guiara en el proceso de definicion de la prueba.En la primera pestana relevante tendremos que dar un nombre a un escenario. Un Load Test puede contener mas de un escenario. Y un escenario no es mas que un contexto de ejecucion con sus normas y comportamientos correspondientes.Tambien tendremos que decidir el perfil para calculos de Think Time. Esto es el tiempo de espera entre peticiones un valor que ya asignamos a la hora de crear los Web Permormance Tests. Se nos propondra usar el valor que ya tenemos grabado usar una distribucion normal segun el valor que grabamos  20 o no usar Think Times.Y por ultimo podremos especificar el Think Time entre cada una de las iteraciones de la propia prueba de carga.En las siguientes paginas seguiremos definiendo las caracteristicas de escenario. En la segunda tendremos que senalar la forma en la que queremos simular los diferentes usuarios del sistema y su numero. Tendremos la opcion de que un numero constante de usuarios este conectado o bien hacer una carga progresiva esperando con un numero inferior que se va incrementando hasta un punto determinado con el tiempo.Aunque la pagina que nos tocaria ahora seria la de Text Mix Model personalmente prefiero antes visitar la de Text Mix donde definiremos las pruebas que cada usuario va a lanzarBasicamente tendremos que ir anadiendo o borrando Web Performance Test a la lista y distribuirlos de la forma que nos parezca mas correcta. Por ejemplo si un usuario realiza una busqueda podriamos pensar que lo normal seria que visitara los resultados de esa busqueda antes que realizar una nueva. Por lo que si hacemos dos pruebas web para estas actividades la correspondiente a la busqueda tendra un porcentaje menor para que se ejecute menos veces.Como nota si queremos anadir una prueba y vemos que no esta en el listado que se nos propone quiza es que primero tengamos que compilar la solucion. Entonces al intentarlo de nuevo veremos que ahora si aparece.Volviendo atras a la pagina de Test Mix Model lo que vamos a tener que decidir es como queremos que los usuarios simulados vayan ejecutando las pruebas definidas en el Test MixBasados en el numero total de pruebas el sistema ira administrando a los usuarios para que entre todos los porcentajes de distribucion sean los especificados.Basandonos en el numero total de usuarios virtuales el sistema determinara cuantas pruebas va a ejecutar cada usuario distribuyendolas de tal forma que en cualquier momento de la ejecucion del Load Test este tenga el porcentaje especificado de distribucion.Basado en el ritmo de cada usuario en este modelo se asignara un numero total de pruebas por usuario y por hora. Asi habra usuarios con un ritmo mas alto que otros y se puede generar un escenario con peticiones mas anarquicas no todas a la vez.Basado en el orden secuencial cada usuario ejecutara las pruebas en el orden que se ha especificado y cuando termine volvera a empezar.Aqui no hay ninguna recomendacion cada uno tendremos que decidir cual se ajusta mejor al escenario que queremos definir.En la siguiente pantalla se nos permitira definir el Network Mix. Esto no es mas que el porcentaje de conexiones que vendran de los diferentes tipos de redes. Por defecto Visual Studio tiene definidos unos cuantos pero si quisieramos crear un tipo de red que sea diferente tenemos aqui un articulo que nos puede ayudar muchohttpblogs.msdn.combrubelarchive20110605loadtesthowtocreateacustomnetworkprofilefornetworkemulation.aspxLa ultima caracteristica que vamos a definir del escenario es el Browser Mix. Porque podria ser que tuvieramos un comportamiento en la pagina web diferente en dependencia del navegador como por ejemplo si te conectas desde un dispositivo movil. Aqui lo mas recomendable es ser consecuente con el escenario donde se va a distribuir nuestra aplicacion. No es lo mismo ponerla en una zona publica donde tendremos que dejarnos guiar por las estadisticas normales de internet que en la intranet de la empresa donde solo dejan usar Internet Explorer 8.Cuando ya hemos terminado de definir el escenario tocara el turno de anadir los servidores que ejecutan la aplicacion y los contadores de los mismos que queremos observar. Por ejemplo si tenemos un servidor de base de datos anadiremos su nombre host y solo marcaremos los contadores de SQL.Una nota aqui es que los servicios remotos de logs y alertas de performance tendran que estar iniciados en la maquina servidora para poder recoger estos datos.La ultima pagina del Wizard sera la que defina los datos correspondientes a la ejecucion de la prueba de cargaLa duracion donde podremos especificar un warmup tiempo que se espera para que los servicios se levanten y un tiempo de duracion de la prueba. O si lo preferimos un numero concreto de iteraciones.El sampling rate que es el tiempo que pasa entre la recogida de datos de los contadores de las maquinas que hemos decidido observar. Cuanto mayor sea este valor los datos seran menos exactos. Pero querer capturar datos cada segundo puede acabar con que nuestra prueba pierde mas tiempo recolectando informacion que ejecutando la prueba en si.Una descripcion.Si queremos grabar logs cuando haya fallo en una prueba concreta.Y el nivel de validacion.Una vez hayamos finalizado el proceso se nos abrira una ventana con los detalles de la prueba de cargaSeleccionando cada componente y mirando en la ventana de propiedades podremos cambiar toda la configuracion que hemos creado en el Wizard. Ademas presionando con el segundo boton del raton podremos entrar en ventanas similares a las que usamos durante la creacion del Load Test.Aqui habra dos puntos en los que habra que detenerse a mirar ya que nos aporta ciertas opciones extras que anteriormente no hemos tenido la posibilidad de configurar.Si pulsamos sobre Run Settings el que este activo en la ventana de propiedades veremos muchas mas opciones de las que definimos. Cosas como gestion de errores o guardar logs de ejecucion no solo cuando fallen hasta el cooldown time que no es mas que un tiempo que se espera a que las pruebas paren para dar tiempo al sistema a estabilizarse despues de una ejecucion.Y por ultimo si tuvieramos un escenario en el que el usuario realiza una actividad antes de empezar con todas las pruebas como por ejemplo un login yo otra para terminarlas como por ejemplo un logout podremos crear un Web Permormance Test para realizar estas actividades y anadirlo al inicio o al final de toda la duracion de la prueba.Para ello tendremos que con el segundo boton del raton pulsar sobre Text Mix. Alli seleccionaremos la opcion de Edit Test Mix. En la nueva pantalla que aparecera veremos en la parte inferior unas opciones de anadir una prueba que se inicia antes que el resto de pruebas sean ejecutadas una vez para cada usuario y otra para la de despuesAqui podremos marcar la casilla y seleccionar la prueba que se dedica a esa actividad.Volver al indiceEjecutando y explotando los resultadosAntes de ejecutar nuestras pruebas de carga tendremos que configurar los Test Settings. En todo proyecto de pruebas de carga tiene que haber a nivel de solucion un archivo de configuracion de la prueba podriamos crear uno nuevo si no existe o varios diferentes para poder ejecutar las pruebas en diferentes entornos.Al hacer doble clic en un archivo de Test Settings nos aparecera una nueva ventana donde podemos configurar la ejecucion de las pruebasEn la primera pagina encontraremos la descripcion de la configuracion y podremos decidir solo si estamos en VS2013 si queremos ejecutarlos en un controlador local o usando el de Team Foundation.Si elegimos ejecutar las pruebas en una maquina local o un Test Controller apareceran nuevas opciones. Una de las mas significativas es la de RolesAqui tendremos que especificar si queremos usar el propio Visual Studio local o una maquina remota que es el controlador. Si marcamos esta ultima tendremos que indicar el host del controller.Otra pagina en la que podriamos tener que configurar algo especial es la de Deployment. Cuando nuestras pruebas necesiten referencias a ensamblados que no son el propio ensamblado de la prueba tendremos que anadir estos ensamblados en como elementos que hay que desplegar junto con la propia prueba. Un ejemplo de esto podria ser un ensamblado donde guardamos normas de extraccion y validacion que usamos en los tests.El resto de opciones como los diagnosticos y demas podemos dejarlos con los valores por defecto en un principio.Para terminar con la configuracion previa en el menu de Load Test de Visual Studio tendremos que activar la configuracion que queramos usarAhora bastara con abrir la prueba de carga y presionar el boton de ejecucion de las pruebas de carga. Visual Studio se conectara con el controlador que hemos especificado y este orquestara esta ejecucion enviandonos datos de diagnostico en tiempo real que podremos ver desde el propio Visual Studio.Aqui podremos ver graficas estandar o crear las nuestras propias seleccionando contadores que vemos a mano izquierda. Tambien tendremos unas tablas a modo resumen. Asi que solo nos quedara esperar a que nuestra prueba de carga termine.Una vez terminen las pruebas podremos ver el resultado completo que el controlador almacenara en la base de datos que indicamos.Tanto en las graficas como en las tablas podremos ver si hay algun contador extrano que ha ido demasiado arriba si por ejemplo vemos que no se libera bien la memoria de la aplicacion o simplemente si los tiempos de respuesta son los que esperabamos.Otro detalle que nos puede ayudar a luego diferenciar las pruebas que hemos ido lanzando es anadir una descripcion. Para ello tendremos que pulsar en el icono senalado y en el campo descripcion anadir un comentario. Despues al buscar resultados de la ejecucion de esta prueba encontraremos este campo de descripcion que nos ayudara a identificarlaPor ultimo para acceder a estos resultados tendremos que abrir el archivo del test de carga y en el menu contextual seleccionar la opcion de abrir resultados de testsVolver al indicePAL Performance Analysis LogsPAL httpspal.codeplex.com es una herramienta desarrollado por Clint Huffman con la ayuda de Microsoft que realiza un analisis de los contadores de rendimiento del sistema en busca de los thresholds conocidos.Basicamente nos ayudara a crear unos contadores que podremos activar en los servidores de nuestra aplicacion y luego los analizara y nos devolvera un documento con graficas y umbrales que nos haran la vida mas facil.Para usarlo solo tendremos que ir a la pagina web del proyecto descargarlo e instalarlo. Y entonces nos tendremos que plantear que datos y de que servidores queremos recolectar.Lo primero que haremos es generar los contadores para eso abriremos PAL y en la pagina de Threshold File encontraremos en un combobox varias plantillas ya creadas. Generalmente suelo usar la de SQL Server y la IIS. La idea es exportar estas plantillas para luego llevarlas a nuestros servidores y activar esos contadoresAhora recogeremos el archivo xml resultante y lo llevaremos a nuestro servidor. Dentro del servidor abriremos el Performance MonitorEn Data Collector Sets tendremos que crear uno nuevo al presionar en la opcion se nos abrira un nuevo dialogo donde le daremos un nombre y le indicaremos que queremos basar el contador en una plantillaEn la siguiente pagina tendremos que presionar sobre browse y buscar el archivo que exportamos del PAL.Ahora ya podemos finalizar el Wizard.Cada vez que vayamos a ejecutar una prueba de carga solo tendremos que ir al servidor al Performance Monitor y activar el contador que hemos creado. Cuando la prueba termine lo pararemos e iremos a buscar los resultados a la ruta por defecto cPerfLogsAdminNombre del contador.Una vez ha terminado deberiamos encontrarnos con una serie de archivos parecida a estaPara obtener nuestro informe tendremos que abrir PAL de nuevo e indicarle que este es nuestro documento de entradaEn la pagina de Counter Log seleccionaremos el archivo.En Thershold File tendremos que volver a seleccionar el perfil que hemos tratado.Una vez termine se abrira una nueva ventana del explorador de internet con un informe muy completo sobre nuestro sistema. Y es muy recomendable leerlo entero en busca de los posibles problemas que se han podido encontrar en el sistema.Volver al indiceConclusionesA lo largo de este articulo hemos visto una pequena parte de la creacion y explotacion de datos en pruebas de carga. Pero este contenido no es todo. Han quedado muchas cosas en el tintero que nos pueden ser utiles en nuestros proyectos. No obstante esperamos haber podido ayudar a esas personas que no saben como realizar este tipo de pruebas. Y a los que ya las conocian de sobra a encontrar un lugar donde consultar y reforzar ideas."
    } ,
  
    {
      "title"    : "Scrum con TFS",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/01/23/scrum-con-tfs",
      "date"     : "2013-01-23 09:13:27Z",
      "content"  : "En los ultimos anos estamos asistiendo a un cambio radical en la forma de gestionar los proyectos de desarrollo de software. Empieza a resultar extrano encontrar ofertas de trabajo en las que no se mencione scrum o proyectos que empiecen en los que no se haya planteado el uso de metodologias agiles. Pero de nada nos sirve una metodologia por muy buena que pueda ser si perdemos mas tiempo gestionando el proceso que programando. Por lo que nos vemos casi obligados a empezar a usar herramientas que nos ayuden con esta tarea. A lo largo de este articulo trataremos una de esas herramientas que consideramos mas potentes TFS.Team Foundation ServerService Las siglas TFS pueden significar Team Foundation Server o Team Foundation Service. En ambos casos se trata del mismo producto pero en diferentes formas de distribucion. Este software es la herramienta que nos ayudara con la gestion del ciclo de vida de aplicaciones ALM propuesta por Microsoft. TFS nos aportara una serie de utilidades que nos facilitaran la gestion de los procesos el control de versiones del codigo fuente el testing de aplicaciones el deploy y ademas nos aportara diferentes informes sobre todo esto. Las dos formas de distribucion en las que podemos encontrar TFS hoy en dia son  Onpremises la modalidad de distribucion de software tradicional que podemos instalar en nuestras propias maquinas tantas veces como licencias hayamos adquirido. En resumen la forma en la que siempre hemos comprado software que recibe el nombre de Team Foundation Server 2012.   Saas que significa Software as a service una modalidad de venta de un software que en lugar de instalarlo lo utilizamos directamente sin preocuparnos por la infraestructura. Por lo general esto implica que ya no tendremos que realizar un desembolso economico grande para usar un producto solo pagaremos por lo que consumamos. Esta version recibe el nombre de Team Foundation Service y por ahora permanece gratuita para equipos de hasta 5 miembros.   Las diferencias entre ambas versiones sobre todo radican en que la version onpremises permite la personalizacion de las plantillas de procesos y tiene una mejor herramienta de generacion de informes. Pero para el proceso que vamos a describir a continuacion es indiferente cual de estas utilizar. En los ejemplos que veremos a lo largo de este articulo hemos utilizado la version Saas Team Foundation Service. Para ello nos dirigimos primero a la pagina oficial tfs.visualstudio.com. Alli nos dimos de alta y creamos una nueva cuenta  En nuestro caso usamos el nombre de programandonet con una cuenta de Live ID que ya teniamos anteriormente. Una vez has finalizado el proceso al entrar en misitio.visualstudio.com deberiamos encontrarnos algo parecido a esto  En nuestro caso ya tenemos algunos proyectos creados pero si es la primera vez para poder seguir los ejemplos deberiamos crear un proyecto nuevo presionando el boton de New Team Project y usando la plantilla de scrum  Para poder entender mejor la plantilla de proceso que estamos usando primero tendremos que definir la metodologia que tenemos pensado usar scrum. Que es scrum Podriamos definir scrum como un marco de trabajo un conjunto de herramientas y protocolos a seguir con el fin de obtener un objetivo comun el exito de un proyecto. Una metodologia agil que a pesar de estar enfocada en el desarrollo de software es aplicable tambien a muchos otros campos como por ejemplo el desarrollo de un vehiculo de Formula 1. La idea de un desarrollo agil se fundamenta en la reduccion de riesgos basandose en la division de un gran problema en problemas mas pequenos. Estas divisiones seran acometidas como proyectos en si mismas que deberan ser desarrollados a lo largo de un corto espacio de tiempo. A este ciclo se le denomina iteracion y esta compuesto por las fases de planificacion analisis diseno codificacion revision y documentacion. Una vez finalizamos una iteracion se realiza una retrospectiva de como han ido las cosas y se empieza una nueva iteracion con las mejoras que se han propuesto. Basicamente se asume que en un proyecto no todo va a ser como podemos pensar en un principio. Nos vamos a encontrar problemas no previstos y los requisitos van a cambiar con el paso del tiempo. Ante estos cambios lo unico que podemos hacer es adaptarnos. Por esta razon dividimos el proyecto para que las cosas que pueden ir mal vayan mal pronto y asi podamos ir afinando el proceso antes de que sea demasiado tarde. Dentro de scrum estas iteraciones reciben el nombre de sprints y suelen durar de 2 a 4 semanas. El objetivo de cada sprint es que al finalizarlo el equipo haya conseguido un producto potencialmente entregable. Es decir algo que funcione que se pueda probar y sobre lo que se puedan proponer modificaciones. Roles Antes de comenzar a trabajar debemos definir los roles de los diferentes miembros del proyecto. Scrum divide en dos grandes grupos de participantes Los comprometidos por el proyecto  Product owner representa a cliente a las personas que han solicitado el producto resultante de este proyecto. Es quien marca los requisitos y gestiona la prioridad de estos.   ScrumMaster es la persona responsable de que el proceso de scrum se ejecute correctamente. Se asegura de que se respeten las reglas y aisla al equipo de cualquier influencia externa. No hay que confundir este rol con el de jefe de equipo o project manager.   El equipo es el grupo de personas que tiene la responsabilidad del desarrollo del producto. Se autoorganiza por esa razon no existe ningun rol de lider o jefe. Por lo general debe ser multidisciplinario un conjunto de no mas de 8 personas que puedan abarcar todas las tareas que conlleva el proyecto analisis diseno desarrollo pruebas .  Y los implicados con el proyecto  Stakeholders partes implicadas Es esa gente que hace el proyecto posible los implicados de alguna manera con el resultado del desarrollo. Los clientes vendedores usuarios finales.   Administradores Es la gente que se encuentra jerarquicamente por encima del proyecto. Los gerentes y managers que establecen el ambiente para el desarrollo.  En lo que respecta a la gestion que deberiamos realizar solo serian relevantes los perfiles comprometidos con el proyecto. Para anadirlos a TFS lo primero que tendriamos que hacer es crear un equipo de trabajo. Para ello nos tendremos que dirigir al portal web principal de nuestro sitio de TFS misitio.visualstudio.com y dentro de este al panel de configuracion. Esto se hace presionando en el icono con forma de rueda arriba a la derecha  Si no lo hemos hecho ya nos aparecera una pantalla para que seleccionemos el proyecto que queremos configurar. Y al seleccionarlo veremos que automaticamente el sistema nos ha creado un equipo con el mismo nombre que nuestro proyecto. Al presionar sobre este equipo podremos gestionar los miembros del mismo  En este caso hemos anadido a todo nuestro equipo de desarrollo que esta formado por 4 personas. Otra forma que tenemos de realizar esta operacion es entrar en la pagina home dashboard de nuestro proyecto y fijarnos en el panel de equipo de nuestra derecha. Haciendo clic en Manage all members.   El proceso Como hemos comentado anteriormente scrum es iterativo. Se divide en una serie de sprints que se van realizando hasta el momento en el que el proyecto se considera terminado. Una forma rapida de explicar el proceso seria el siguiente grafico  Aqui se puede observar que todo el proceso se inicia con la recogida de requisitos y terminara cuando el resultado del feedback proporcionado por las partes implicadas sea que el producto esta terminado. Gestion del backlog Asumimos que ya estan todos los roles preparados para empezar con el proyecto o al menos ya tenemos una figura de Product Owner el propietario del producto. Entonces es el momento de empezar a recopilar requisitos. El product owner ademas de ser la voz de nuestro cliente tambien debe ser la persona que ordene y priorice aquellas necesidades que el cliente le ha comunicado. Todos los requisitos relacionados con la aplicacion se recogen en un listado priorizado que recibe el nombre de pila de producto o Backlog. Y los elementos que encontramos en este backlog se denominan user stories o historias de usuario. Estas historias no son mas que requisitos unicos cortos facilmente redactarles y que definen un requisito de forma rapida y en el propio lenguaje del usuario. Es muy comun que una historia de usuario este definida mediante la plantilla  As a user i want something so that i can achieve that Donde los parametros entre corchetes son sustituidos por el rol de la persona que lo solicita que es lo que le gustaria que pasara y cual el el objetivo que busca con dicho comportamiento.  Para la gestion del backlog TFS nos ofrece dos herramientas que podremos encontrar en la seccion Work  backlog. Una es la lista de requisitos como una lista ordenada segun prioridad que podemos ver en la imagen anterior. Y la otra es en forma de tablero en la que podremos ver visualmente la evolucion de las historias de usuario  En adicion en la parte superior conforme vayan desarrollandose los sprints tendremos dos graficas que nos daran un claro estado del proyecto y de la velocidad de nuestro equipo. Eso siempre y cuando se siga usando TFS en el resto de las fases. Preparando el entorno Antes de ponernos a programar debemos definir nuestro marco de trabajo responder preguntas como el tiempo del que dispone cada miembro de nuestro equipo cuanto va a durar cada sprint inicialmente o que tipo de especialidad tiene cada uno. Una vez tenemos respuesta a estas preguntas tendremos que introducir estos parametros en el portal de nuestro proyecto de Team Foundation Server. Asi pues en la pagina principal del proyecto buscaremos a mano derecha la opcion de Configure schedule and iterations.  Al hacer clic en esta opcion aparecera una ventana donde podremos decidir que dias tendran lugar los proximos sprints  Tampoco deberiamos obsesionarnos con configurar todos los sprints ya que la duracion de los mismos podria cambiar si el equipo considerara que son o demasiado breves o demasiado largos. En este sentido recomendamos mantener consistente la duracion y que por lo general tengamos entre ninguno y un solo cambio de duracion de un sprint por proyecto. Pero no obstante creemos que es mejor ir configurando todo sprint a sprint. Otro de los datos que seran muy utiles sobre todo cuando nos encontramos con equipos multidisciplinares es el calculo de capacidades. Una vez tenemos definido el sprint podemos seleccionarlo dentro de la pestana Work e introducir para cada uno de los miembros del equipo cual es la actividad que van a desarrollar el tiempo que van a dedicar al trabajo por dia y si tienen vacaciones durante el sprint  En realidad es una tarea que no nos tomara mas de 10 minuto y a cambio gracias a estos datos mas adelante seremos capaces de medir el trabajo que podemos asumir a lo largo de un sprint. Planificacion del sprint Para comenzar con las liturgias que engloban al equipo esta la reunion de planificacion del sprint o sprint planning. El resultado de esta reunion debe ser un listado de tareas que se van a desempenar a lo largo del sprint para conseguir un objetivo marcado. Al principio de la reunion el product owner debera explicar el backlog y proponer las historias de usuario mas prioritarias. Y el equipo debera realizar preguntas sobre las dudas que puedan surgir evaluar estos requisitos y decidir cuales son los que se compromete a entregar a final de sprint. Desde la vista del product backlog del tfs podremos arrastrar los elementos que hemos planificado encima del sprint que les corresponda  Y haciendo doble clic en la user story podremos anadir comentarios con la informacion extra que recolectemos durante la exposicion del product owner. Ademas de esto el equipo debera dividir las historias en tareas. Estas tareas seran estimadas usando el poker planning por ejemplo y en algunos equipos hasta se autoasignan segun las cualidades de cada uno. Asi que una vez hemos asignado los elementos del backlog al sprint actual podemos hacer clic la etiqueta y anadir tareas que dependan de los requisitos. Haciendo doble clic en las tareas podremos anadir detalles sobre su implementacion las estimaciones e incluso a que actividad se refiere  Lo interesante de asignar la actividad de una tarea es que automaticamente podremos ver en unas barras a la derecha si estamos excediendo el tiempo de trabajo de las personas dedicadas a esas actividades o por el contrario si podriamos comprometernos a realizar mas trabajo.  Ahora ya tememos todo preparado para que el equipo empiece a trabajar. Trabajo diario Dentro de un sprint dividimos el trabajo por dias. El empezar la jornada aunque algunos equipos lo dejan para el final del dia tiene lugar la daily meeting o standup meeting. Ambas nomenclaturas definen bien este tipo de reuniones ya que se realizan de forma diaria y los asistentes deben estar de pie. Esto responde a que esta reunion tiene que durar un maximo de 15 minutos y si permanecieramos sentados la gente se siente comoda y es mas facil extenderse. A un daily meeting puede asistir cualquier persona interesada pero solo tendran voz los roles comprometidos por el proyecto el equipo el scrummaster y el product owner. Generalmente se hace una ronda en la que cada uno de los asistentes debe responder a tres preguntas  Que hice ayer  Que problemas encontre  Que voy a hacer hoy  Esto no se hace para controlar a las personas si no para favorecer la comunicacion entre los miembros del equipo. Casi todas las liturgias de scrum van dirigidas a este objetivo. Se sustenta en la premisa de que si hay buena comunicacion es mas facil que las cosas salgan bien. La ubicacion de esta reunion deberia ser todos los dias la misma y a la misma hora para que no exista duda alguna. Y seria interesante realizarla junto a la scrumboard. Esto es una tabla sobre la que vamos a ir gestionando las tareas del sprint. Team Foundation Server nos proporciona un tablero virtual para scrum que encontraremos en las seccion Work  board  Como podemos observar automaticamente nos muestra el tablero del sprint actual y podemos agrupar las tareas por elemento del backlog o por miembro del equipo. La mejor parte de esta scrumboard virtual es que podemos ir moviendo las tareas y estas se iran actualizando en tiempo real. Eso si en scrum cuando una tarea se da por terminada se han cumplido todos los requisitos de la definicion de hecho no puede volver atras. Si por A o por B el desarrollo de esta tarea ha tenido unas consecuencias no esperadas esto deberia repercutir en la aparicion de un bug o una nueva historia que subsane los problemas que hayamos generado. Otro de los artefactos relacionados con scrum en particular y las metodologias agiles en general es el burn down chart. Una grafica que podemos ver en la parte superior derecha de la scrumboard. Al hacer clic sobre ella podremos tener una vista ampliada en la que se observaran mejor los detalles de la grafica  Es importante que el equipo tenga clara esta grafica ya que nos muestra el estado actual del sprint. Si estamos muy por encima de la linea de tendencia ideal es que vamos con retraso y si estamos por debajo podria significar que hemos sobreestimado el esfuerzo de las tareas. Pero para que TFS nos muestre todos estos datos de una forma correcta es necesario que el equipo vaya actualizando las tareas dia a dia. Y el control de codigo fuente junto con el Team Explorer de Visual Studio nos van a ayudar. Bastara con que dentro de Visual Studio cuando un programador vaya a realizar un checkin asocie mediante drag n drop una tarea. Esto se realiza en el panel de Peding Changes de la ventana de Team Explorer  El punto negativo y que podria resultar tedioso es que el esfuerzo que le hemos dedicado a la tarea deberemos introducirlo manualmente. Con respecto a este tema una practica muy util es que antes de irnos a casa cada miembro del equipo debe ser responsable de haber actualizado sus tareas y la scrumboard. O al menos si seria necesario hacerlo antes de cada daily meeting. En scrum no se aceptan tareas con una estimacion de duracion de mas de una jornada de trabajo. Si asi fuera se deberia dividir en varias tareas mas pequenas. Por lo que todos los dias debera haber movimiento en la scrumboard. El ultimo punto del trabajo a tener en cuenta desde el punto de vista de TFS seria el Continuous Integration que tendria lugar con la ayuda de la configuracion de los procesos de build. Aunque este punto quiza lo podamos tratar en un futuro con mucho mas detalle. Revision del sprint Una vez hemos llegado a la fecha de finalizacion del sprint nos encontramos en el momento de la revision del trabajo realizado. Tanto si hemos terminado todas las tareas como si no se debera informar y apuntar para tenerlo en cuenta en posteriores sprints. En el proceso de revision es necesario involucrar a todos lo roles definidos en el proceso de scrum tanto los implicados como los comprometidos. Se presentara el producto tal y como esta en ese momento. Se responden preguntas y se admiten sugerencias. La forma que nos provee TFS de recolectar el feedback de las partes interesadas del proyecto es la herramienta Microsoft Feedback Client for TFS 2012. Todo el proceso comienza con la peticion de este feedback. Si nos dirigimos a la pagina home de nuestro proyecto de Team Foundation Service encontraremos en el widget de Activities una opcion llamada Request Feedback. Al pulsarla se abrira una nueva ventana donde se nos permitira rellenar los detalles de esta peticion  Los tres detalles que nos permite rellenar son los invitados a dar feedback la forma de acceder a la aplicacion para poder probarla y los elementos que deben tener mas en cuenta por ser los objetivos principales del sprint a la hora de revisar. El resultado de enviar esta peticion sera un email a cada uno de los stakeholders especificados donde se le informara de todo. En este email ademas encontraremos un enlace para descargar la herramienta de feedback de Microsoft de forma gratuita. Despues de instalar el Microsoft Feedback Client al ejecutarlo lo primero que nos pedira es la conexion con el TFS  Acto seguido la aplicacion nos pedira que abramos la demo de prueba siguiendo las instrucciones que especificamos anteriormente. Y despues ira pasando por todos los elementos que indicamos que se debian revisar para que el usuario pueda anadir comentarios capturas de pantalla notas de voz .  Despues de enviar el feedback podremos acceder a el desde el portal de TFS Work  work items  Feedback  Es muy recomendable que la reunion de revision sea en persona y con todos los roles a la vez. No obstante de cualquier forma tambien es muy recomendable usar la herramienta de feedback para poder almacenar en TFS todos los datos y asi poderlos revisar con mas calma. Tanto si la persona que reporta esta o no en la reunion. Retrospectiva del sprint La ultima reunion que tiene lugar en a lo largo de un sprint es la retrospectiva. Una vez tenemos el feedback esta vez reuniremos al scrummaster y el equipo aunque el product owner puede estar invitado de oyente. A lo largo de esta reunion se debatiran temas como la grafica de burn down la velocidad del equipo las cosas que han ido bien en el sprint que podria haber ido mejor y lo mas importante que cosas se van a hacer de forma diferente buscando sacar el mejor resultado en el siguiente sprint. En esta reunion sera muy importante manejar todos los datos que hemos ido recolectando con TFS e incluso almacenar un resumen con las conclusiones de la reunion tambien dentro del sistema. De esta forma todo quedara archivado y accesible por todos los miembros del equipo. Es la reunion donde mas partido sacaremos a TFS de todas. Al finalizar esta reunion deberemos volver al primer paso del proceso y volver a empezar con el siguiente sprint. Eso si esta vez habiendo retroalimentado el proceso con la experiencia del sprint anterior. Conclusiones Aun sabiendo que scrum varia en cada equipo que se aplica hemos intentado exponer un proceso lo mas aseptico que hemos podido para observar la potencia de TFS. Hemos de reconocer que scrum nos parece un escenario muy bueno para el desarrollo de software. Y en este sentido tanto Team Foundation Server 2012 como Team Foundation Service han demostrado ser dos herramientas muy completas que nos ayudaran mucho con la gestion del proceso. Todo esto a pesar de que muchos mas detalles sobre la personificacion del proceso se nos han quedado en el tintero como por ejemplo las builds la entrega continua la integracion continua los code reviews o la potencia de las herramientas de testing. Nosotros lo hemos probado y hemos llegado a la conclusion de que nos gusta. Si aun no lo has hecho te invitamos a que uses una cuenta gratuita de Team Foundation Service y durante el proximo sprint realices un seguimiento en paralelo con esta herramienta para que saques tus propias conclusiones."
    } ,
  
    {
      "title"    : "Patrones de dise&ntilde;o: Repository",
      "category" : "",
      "tags"     : "",
      "url"      : "/2013/01/07/patrones-de-diseo-repository",
      "date"     : "2013-01-07 11:13:39Z",
      "content"  : "En todos los asuntos de opinion nuestros adversarios estan locos Oscar Wilde. Por suerte los locos con los que puedes compartir opiniones en la lista de correo de la fundacion Techdencias son magnificos profesionales como Marc_Rubino o mserrate. Hace unos dias tuvimos la oportunidad de discutir sobre el patron Repository.Y aunque nuestras opiniones pueden diferir me gustaria compartir algunas conclusiones personalesUn poco de historiaLa primera vez que oimos hablar del patron Repository fue en el famoso libro de Martin Fowler Patterns of Enterprise Application Architecture y la autoria se le atribuye a Edward Hieatt y Rob Mee.Para el que no se haya leido este libro ya esta tardando  lo resumiremos diciendo que se nos plantean varios patrones de diseno que ayudaran a mejorar nuestras aplicaciones. Entre tantos conceptos se nos introduce una forma de gestionar las interacciones con la capa de almacenamiento de la aplicacion la capa Data Mapper.Data MapperImaginemos que tenemos un sistema de almacenamiento de datos complejo algo asi como una base de datos. La gestion mediante nuestro lenguaje de programacion preferido de esta informacion puede resultar compleja y para nada relacionada con la forma de gestionar la informacion dentro de nuestra aplicacion.Por citar un ejemplo mas concreto no resulta sencillo ni natural recoger un dato de la una base de datos SQL Server usando c. Basicamente porque el lenguaje c esta pensado para programar orientado a objetos y el motor de SQL Server esta pensado para modelos de datos relacionales usando SQL como lenguaje de comunicacion.Una consulta sencilla que devuelva el numero de usuarios de nuestro sistema puede ocupar varias lineas. Y ademas habra que tener en cuenta los errores que puedan surgirtry   usingvar con  new SqlConnectionconnetionString         using var cmd  new SqlCommandselect count from dbo.Users con               var count  Convert.ToInt32cmd.ExecuteScalar         return count         catch Exception ex    manage exceptionPara que no se complique nuestro codigo el senor Fowler nos propone crear una capa dentro de nuestra aplicacion cuya mision sea mover la informacion entre los objetos de c y la base de datos. Ademas esta capa va a aislar el comportamiento de la base de datos del de nuestros objetos haciendo que nuestra aplicacion no este acoplada con nuestra fuente de almacenamiento SQL Server en el ejemplo.De esta forma creariamos una implementacion simple y generica de nuestra capa de Data Mapperpublic interface IDataMapperTObject where TObject  class   void InsertTObject obj   void UpdateTObject obj   void DeleteTObject obj   IEnumerableTObject GetAllpublic class UserDataMapper  IDataMapperTObject   private string connetionString   public UserDataMapperstring connetionString         this.connetionString  connetionString      public void InsertUser user         try               usingvar con  new SqlConnectionconnetionString                     var sql  INSERT INTO USER UserId Name LastName Email VALUES userId name lastName email            using var cmd  new SqlCommandsql con                           cmd.Parameters.Addnew SqlParameteruserId user.UserId               cmd.Parameters.Addnew SqlParametername user.Name               cmd.Parameters.Addnew SqlParameterlastName user.LastName               cmd.Parameters.Addnew SqlParameteremail user.Emaild               cmd.ExecuteNonQuery                                 catchException ex               throw new DataMapperExceptionError inserting a User ex            public void UpdateTObject obj  ...    public void DeleteTObject obj  ...    public IEnumerableTObject GetAll  ... Un objeto Data Mapper es un tipo de implementacion de DAO Data Access Object. La peculiaridad es que hace uso de un patron Metadata Mapper. La idea es anadir una especie de diccionario o Hashtable que contenga las equivalencias entre los objetos de c y las tablas de SQL Server. Pero esto quiza sea otra historia.RepositoryUna vez hemos montado nuestra capa de Data Mapper al ir construyendo el resto de la aplicacion nos vamos dando cuenta de que necesitamos crear un metodo que acepte condiciones y filtros para realizar un gran numero de consultas diferentes. Por ejemplo necesitamos listados de usuarios paginados listados de usuarios para mostrar en un control de tipo ComboBox e incluso los usuarios que su nombre sea Pepe. Entonces como resultado de este requerimiento tan generico tendriamos una funcion parecida a estapublic interface IDataMapperTObject where TObject  class    ... IEnumerableTObject GetFilteredstring conditions string order int pageSize int pageIndexAsi tendriamos la libertad de llamar a nuestro objeto UserDataMapper de formas diferentesvar userDataMapper  new UserDataMapperconStr seleccionar los usuarios de nombre Pepe ordenados por el apellido paginando de 10 en 10 y quiero la primera paginauserDataMapper.GetFilteredName  Pepe LastName 10 1 seleccionar los usuarios de apellido Perezsin orden con el tamano de pagina mas grande que pueda dame la paginauserDataMapper.GetFilteredLastName  Perez AND Email LIKE mail.com  int.MaxValue 1Pero el lenguaje de nuestro almacen de informacion SQL no deberia ser manejado desde la capa que gestiona el negocio de la aplicacion.Es entonces cuando quiza nos vendria bien implementar otra capa nueva de abstraccion por encima de Data Mapper. Una capa que nos ayude gestionar estas consultas de forma transparente sin necesidad de acabar escribiendo codigo SQL y manteniendo desacoplado el codigo de negocio de la forma de almacenar la informacion.El Repository surge como un sofisticado patron que da solucion a este problema. Decimos sofisticado porque se podria definir como una combinacion de otros patrones.La idea es que un objeto Repository actue como una coleccion en memoria del modelo de dominio. A esta coleccion de objetos podremos anadirle o quitarle elementos y ademas realizar busquedas filtradas utilizando el patron Specificationpublic interface IRepositoryTEntity  ICollectionTEntity    where TEntity  class    IEnumerableTEntity FilterByICriteria criteriaLa primera caracteristica que puede llamarnos la atencion al definir Repository es el concepto de modelo de dominio. Esto es un termino muy comun cuando hablamos de DDD Domain Driven Design y quiere decir que nuestra aplicacion tiene un modelo principal al que llamaremos dominio. Este modelo se diferencia del modelo de la base de datos en su concepcion. En lugar de pensar como vamos a almacenar las tablas y sus relaciones dentro de una base de datos lo que vamos a hacer es pensar en la mejor forma de gestionar los objetos dentro del contexto de nuestro lenguaje de programacion y de la forma que mejor se adapte a las tareas de negocio.Y si estudiamos la implementacion propuesta encontraremos varios detalles. Entre ellos que nuestro repositorio tendra que implementar ICollection por lo que implementara funciones como Add que anade un objeto nuevo o Remove que borra el objeto de la coleccion.Otro detalle es el objeto ICriteria que se le pasa como parametro al metodo FilterBy. Este objeto no es mas que la representacion del anteriormente mencionado patron Specification.SpecificationEl patron Specification viene a resolver un problema de crear diferentes reglas de negocio que puedan ser combinadas. Esto quiere decir que nos ayudara a crear diferentes normas que resolveran un problema concreto de formas diferentes. Pero para orientarnos mas rapido vamos a ver un ejemplo de implementacionpublic interface ISpecification   bool IsSatisfiedByobject candidatepublic interface ICompositeSpecification  ISpecificationISpecification AndISpecification otherISpecification OrISpecification otherISpecification NotImplementando de la forma correcta este patron el resultado que tendriamos es que si tuvieramos diferentes especificaciones podriamos combinarlas para conseguir algo mas concreto.Imaginemos que tenemos estas implementacionespublic class NameSpecification  ICompositeSpecification   public NameSpecificationstring nameToCompare   ...     public bool IsSatisfiedByobject candidate   candidate.Name  this.nameToCompare ...     public ISpecification AndISpecification other   ...     public ISpecification OrISpecification other   ...     public ISpecification Not   ...  public class PageIndexSpecification  ICompositeSpecification   public NameSpecificationint pageIndex int pageSize   ...     public bool IsSatisfiedByobject candidate   candidate is un pageIndex using pageSize     public ISpecification AndISpecification other   ...     public ISpecification OrISpecification other   ...     public ISpecification Not   ...  Ahora podriamos llamar a nuestro repositorio con un codigo parecido a estevar repository  new UserRepositoryvar criteria  new NameSpecificationPepe  filtramos por nombre igual que Pepecriteria  criteria.Andnew PageIndexCriteria1 10  cogemos la primera paginavar result  repositori.FilterBycriteriaLa ventaja de este patron es que podemos crear especificaciones muy genericas o muy concretas segun las necesidades de cada momentopublic class PropertyEqualsSpecification  ICompositeSpecification   public NameSpecificationstring propertyName object value   ...     public bool IsSatisfiedByobject candidate   candidate.propertyName  value ...     public ISpecification AndISpecification other   ...     public ISpecification OrISpecification other   ...     public ISpecification Not   ...  Dentro de este patron existe una implementacion muy conocida para ayudarnos a realizar sentencias SQL para bases de datos y recibe el nombre de Query Object tambien expuesto en el libro de Patterns of Enterprise Application Architecture.Lo que se propone en el patron Repository es que las especificaciones sean resueltas usando un patron Strategy para poder determinar en ultima instancia una sentencia correcta que podamos enviar a nuestro objeto Data Mapper.Y como corolario a esta definicion aquellas especificaciones que sean muy comunes y se repitan muchas veces a lo largo del codigo pueden pasar a formar parte de la definicion del propio repositorio. Imaginemos que la busqueda por nombre se realiza en 10 sitios diferentes. La solucion mas optima sera crear un metodo especifico dentro del repositoriopublic class UserRepository  IRepositoryUser    ...    public IEnumerableUser FilterByNamestring name         var criteria  new NameSpecificationname      return this.FilterBycriteria   La actualidadHasta este momento hemos hablado de la teoria del patron Repository y como lo encontramos definido la primera vez que leimos algo sobre el. Pero probablemente si realizamos una busqueda por internet sobre este patron y mas si buscamos su implementacion concreta para el lenguaje de programacion c nos encontraremos un resultado que apenas se parece con al que acabamos de definir.Vamos a poner el ejemplo de un blog simple en el que queremos almacenar las entradas que escribimos Post y los comentarios que dejan los visitantes dentro de esas entradas CommentSi seguimos a rajatabla la primera implementacion que encontremos en los buscadores mas conocidos el resultado sera algo parecido a estopublic interface IRepositoryTEntity where TEntity  class    IQueryableTEntity GetAll    void AddTEntity entity    void UpdateTEntity entity    void DeleteTEntity entitypublic interface IPostRepository  IRepositoryPostpublic interface ICommentRepository  IRepositoryCommentQue ha pasadoEn los entornos de programacion en .net ha cambiado mucho el escenario respecto de los inicios de la plataforma con respecto las librerias y evoluciones del lenguaje de hoy en dia. Y estas nuevas caracteristicas han provocado que el patron Repository tenga que ser adaptado a unas nuevas necesidades.En la version 3.5 de la framework como gran novedad se anadio de forma nativa LINQ Language INtegrated Query. Una nueva extension al lenguaje que nos iba a permitir realizar sentencias muy semejantes a las de SQL dentro de entornos de programacion .netvar results   from c in SomeCollection               where c.SomeProperty  10               select new c.SomeProperty c.OtherPropertySi tenemos una coleccion un array o cualquier objeto que implemente el patron Iterator objetos que implementan IEnumerable podemos recorrerla de forma sencilla y hacer una gestion de sus datos de una forma similar a como trabajamos en los entornos de bases de datos.Este tipo de consultas funcionan gracias a unas extensiones del lenguaje que transforman las sentencias en llamadas a una serie de funciones nuevas. Por ejemplo al compilar el ejemplo anterior lo que en realidad tenemos como resultado esvar results  SomeCollection                    .Wherec  c.SomeProperty  10                    .Select c  new  c.SomeProperty c.OtherProperty Algunos rapidamente habran encontrado que este codigo podria ser una implementacion valida del patron Specification que antes estamos mencionando. Ademas LINQ trabaja con colecciones por lo que podriamos decir que esta tecnologia realiza por nosotros la mitad del trabajo de crear un repositorio.La otra gran novedad son los ORM Object Relational Mapping modernos que antiguamente nos ayudaban a crear objetos DAO pero que hoy en dia han llegado mucho mas lejos. Librerias como Entity Framework o nHibernate han conseguido implementar complejos sistemas que gestionan las conexiones con las bases de datos mapeo automatico de todo lo que podamos encontrar a objetos planos y simples caches generacion de bases de datos automatica . e incluso su propio lenguaje intermedio para realizar sentencias estandares de bases de datos.Ademas uniendo estos ORM con LINQ se ha cerrado el circulo. Por ejemplo un contexto de EF tiene colecciones de entidades de la base de datos. A estas colecciones uno puede anadirle o quitarle elementos y tambien se pueden realizar consultas complejas expresadas en un lenguaje cercano al natural y al de una base de datospublic class MyDbContext  DbContext   public DbSetMyEntity MyEntities  get set var context  new MyDbContextvar myEntity  new MyEntity  Name  Test  anade la entidad a la base de datoscontext.MyEntities.AddmyEntity borra la entidad de la base de datoscontext.MyEntities.DeletemyEntity seleccionamos valores de la base de datos con condicionesvar results  context.MyEntities.Where e  e.Name.StartsWithT Las conclusiones que podemos sacar de esto es que los ORMs modernos ya han implementado el patron Repository por nosotros y lo han dotado de mas caracteristicas.El nuevo RepositoryPero ahora imaginemos que queremos realizar pruebas unitarias en nuestra aplicacion o que por ejemplo quisieramos migrar a un nuevo motor de base de datos NoSQL. El ejemplo de contexto anterior no estaria demasiado acoplado al ORMPara solucionar esto existen varias formas pero una de ellas es la de crear un objeto intermediario al que llamaremos Repository. Se usara ese nombre ya que va compartir ciertas caracteristicas del patron original y ademas porque servira de repositorio de informacion para nuestra aplicacion.Un repositorio dentro de una aplicacion actual va a aislar al dominio del ORM o de la conexion con la base de datos. Va a proveer de interfaces que puedan ser probadas y simuladas ademas de ocultar cualquier detalle relacionado con la forma de almacenar la informacion en nuestra aplicacion.Volviendo al ejemplo del blogpublic interface IRepositoryTEntity    IQueryableTEntity GetAll    void AddTEntity entity    void UpdateTEntity entity    void DeleteTEntity entityTodo repositorio de la aplicacion expondra al menos los metodos para anadir borrar y listar elementos.En dependencia del ORM un repositorio se implementara de una forma u otra pudiendo gestionar transacciones ciclo de vida de las conexiones mapeo de objetos etc.Otra propuestaPersonalmente esta ultima definicion no me convence del todo por varias razonesLa primera es que no aporta valor real. Por lo general crear repositorios se ha convertido en copiar y pegar codigo generarlo automaticamente con plantillas t4 o cualquier herramienta semejante y no nos paramos a pensar que es lo que esperamos de este tipo de artefacto.La segunda razon esta relacionado con devolver objetos que implementan IQueryable. Estos objetos pueden ser gestionados usando LINQ de tal forma que generen diferentes sentencias SQL en la base de datos que por lo general son muy complejas y no todo lo eficientes que deberian.Ademas al usar objetos IQueryable en niveles lejanos al ORM o el repositorio estamos dando responsabilidades extra a artefactos de nuestra aplicacion que no estan preparados para estas responsabilidades. O dicho de otra forma la ultima capa responsable de componer sentencias SQL directa o indirectamente deberia ser el repositorio.Quiza sea muy atrevido entender LINQ como la implementacion mas correcta del patron Specification dentro del contexto de un repositorio. Es posible que fuera mas correcto implementarla de nuevo para poder aislar realmente el problema de las sentencias de consulta de base de datos eficientes a un solo lugar.La ultima razon por la que no termina de convencerme esta implementacion es el asunto de la existencia de un repositorio por cada una de las entidades de la base de datos. Algo que no tiene por que adaptarse realmente a las necesidades de negocio. En el ejemplo del blog si en lugar de tener un repositorio para objetos tipo Post y otro para Comment podria ser mas logico tener uno solo para gestionar las operaciones del blog. Porque si un comentario no tiene sentido si no esta asociado con una entrada de un post tampoco tendra sentido que tenga su propio repositorio.Aplicando estas premisas personalmente y simplificando mucho el problema de la gestion de un blog propondria un repositorio que aportara valor y no expusiera objetos IQueryable e implementara su propio sistema de especificaciones. Algo mas parecido a estopublic abstract class BaseRepository    protected virtual IQueryableTEntity GetAllTEntity          where TEntity  class             ...         protected virtual void AddTEntityTEntity entity          where TEntity  class             ...         protected virtual void UpdateTEntityTEntity entity          where TEntity  class             ...         protected virtual void DeleteTEntityTEntity entity          where TEntity  class             ...     public interface IBlogRepository  IBlogRepository    ListPost GetAllPosts    ListPost GetPostsByCriteriaICriteria criteria    Post GetPostWithCommentsGuid postId    void AddPostPost post    void UpdatePostPost post    void DeletePostPost post    void InsertCommentPost post Comment comment    void DeleteCommentComment comment    ... public class BlogRepository  BaseRepository IBlogRepository    ... public class LastPostCritera  ICriteria    ... public class CategorizedCritera  ICriteria    ... public class DatedCritera  ICriteria    ... En esta implementacion creariamos un repositorio base que contendria los metodos genericos de anadir borrar y listar. Pero estos metodos han sido declarados como protected lo que repercutira en que no puedan ser invocados desde fuera del contexto de un repositorio.En cuanto repositorio expondra una interfaz diferente al tipico CRUD Create Read Update Delete y usando las funciones protegidas de la base realizaria tareas especificas de gestion de la informacion. Ademas los listados y filtros se podran realizar con un patron Specification ocultando en realidad el codigo especifico y necesario para realizar la comunicacion con la base de datos.Y en definitiva asi conseguiriamos un codigo testeable y desacoplar el acceso a los datos del resto de la aplicacion.ConclusionesEs posible que el concepto de Repository este pervertido hoy en dia con respecto al concepto inicial. Pero se ha ido adaptando a los diferentes progresos de las plataformas y los lenguajes.Hay gente que no esta de acuerdo con el nombre de Repository y que piensan que es simplemente un DAO. Aunque creo que no es del todo ni lo uno ni lo otro simplemente coge algo de cada uno.Los objetos de este tipo son muy utiles para desacoplar la aplicacion y separar los datos del negocio de verdad. Ademas nos proveen de una forma probada y que funciona para resolver este problema sin que tengamos que inventar nada nuevo. Son faciles de probar y una abstraccion muy util para proyectos grandes. Asi que si de verdad se implementa de forma que contenga la informacion acerca del almacenamiento quitando esa responsabilidad al resto de componentes es una opcion muy valida.Pero evidentemente y como pasa con todos los patrones de diseno hay que cenirse a las necesidades reales y no caer en los tipicos antipatrones de aplicarlo cuando no es necesario o aplicarlo de una forma que no resulta del todo correcta."
    } ,
  
    {
      "title"    : "Team Foundation Server: Mac OS X+Java+Eclipse+Maven",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/11/26/team-foundation-server-mac-os-xjavaeclipsemaven",
      "date"     : "2012-11-26 09:41:48Z",
      "content"  : "Hace unos meses me encontraba en un cliente hablando de la posibilidad de sustituir su conjunto de aplicativos para ALM Application Lifecycle Management o gestion del ciclo de vida de las aplicaciones por el potente TFS de Microsoft. Se mostro fascinado por la herramienta estuvimos navegando por el portal mostrandole las graficas y lo facil que era gestionar todo el trabajo. No obstante el lider tecnologico era esceptico y no confiaba en las capacidades del entorno que estabamos proponiendo. Asi que le retamos a proponernos el escenario mas complejo que podriamos encontrarnos en su empresa. Tras unos segundos de silencio su respuesta fue concisa macbook air programando en Java y que use Maven. Challenge accepted Desempolve el macmini y en lo que se instalaba el eclipse creamos un nuevo proyecto dentro de la cuenta de Team Foundation Service de Programando en .Net.  Alguno puede pensar que pecamos de soberbia utilizando un entorno mas adverso incluso que el propuesto. Pero en realidad era lo unico de lo que disponiamos sin gastar demasiado tiempo y dinero. Nuestro escenario se componia de  Un macmini de una generacion tan antigua que no recuerdo ni el numero. Pero actualizado a un Mac OS X 10.7.3.  En lugar del TFS2012 decidimos usar el TFS en azure para no perder tiempo buscando un equipo medio potente y tener que instalarlo. Sabiendo que esta version carece de ciertas caracteristicas y lo que es mas importante que se encuentra en la nube por lo que no podemos hacer trampas.  Para desarrollar usamos eclipse en la ultima version del momento. Y el Maven que viene instalado por defecto en el sistema operativo de la manzana.  El siguiente paso fue crear el proyecto de Java para Maven con el arquetipo de quickstart desde eclipse  Anadimos un pequeno hello world para no complicarnos demasiado el reto. Comprobamos que compilaba y se ejecutaba correctamente y llego el momento de anadirlo al control de codigo fuente de Team Foundation Service. Para ayudarnos ha hacerlo tuvimos que instalar un plugin para eclipse. Antiguamente era conocido como Visual Studio Team Explorer Everywhere pero hoy en dia lo encontraremos con el nombre de TFS Plugin for eclipse  Lo mejor de este plugin es que para los que estemos acostumbrados a Visual Studio la unica dificultad que vamos a encontrar es mostrar la ventana de Team Explorer. Y es que eclipse viene lleno de ventanas por defecto y tendremos que hacer una pequena busqueda para encontrar la nuestra   Una vez la hemos activado veremos exactamente el mismo control con todas en realidad no todas faltan las nuevas opciones de code review las caracteristicas que podemos encontrar en la version por asi llamarla original  Para conectar con el proyecto que creamos al inicio del articulo solo tuvimos que crear una nueva conexion anadiendo la direccion del TFS. En este caso como usamos la version de azure introducimos la URL con https por delante. Despues seleccionamos el proyecto adecuado en el listado. Y ya teniamos acceso a todos los work items las builds y control de codigo fuente.  Para continuar lo que teniamos que hacer era indicarle a eclipse que el codigo proyecto actual iba a ser gestionado por TFS. Por lo que en el explorador de paquetes seleccionamos la raiz del proyecto y buscamos la opcion de compartir con Team Foundation Server  Al realizar esta asociacion automaticamente se actualizo la informacion de la ventana de Team Explorer con los datos del proyecto listos para hacer check in.  Asi que ya teniamos la mitad del camino recorrido. Habiamos conseguido conectar el entorno y subir el codigo fuente para que a partir de ahora fuera TFS quien lo gestionara. Pero a partir de aqui las cosas se iban a poner mas dificiles porque TFS de fabrica no puede crear builds de proyectos en lenguajes que no sean de la plataforma de Microsoft. Aunque si que nos facilita integrarnos con otras tecnologias. Para crear una build automatizada de integracion continua lo primero que tenemos que hacer es exactamente lo mismo que en cualquier sistema instalar las herramientas. En este caso las herramientas que usamos eran la JDK y Maven. Y un detalle importante que hay que tener en cuenta es que TFS esta instalado en maquinas Windows por lo que tuvimos que descargar las versiones para ese sistema operativo.  El caso de Maven fue muy sencillo porque directamente desde su pagina web pudimos descargar los binarios. Pero en cuanto al SDK de Java tuvimos que pivotar en un ordenador con Windows. Ahi instalamos las herramientas y las copiamos en un pen drive para al final tenerlas en el mac.  Creamos una carpeta llamada tools en el directorio local que contenia el jdk y el Maven. Y en el control de codigo fuente creamos la misma estructura de directorios pero vacia para poder sincronizarlos. Acto seguido nos dirigimos al Team Explorer al panel de Pending Changes y en acciones buscamos la opcion de detectar cambios. Asi conseguimos sincronizar ambas carpetas.  Despues de hacer check in con estos cambios ya teniamos disponibles las herramientas que ibamos a utilizar dentro de TFS por lo que solo nos quedaba configurar la build. Entonces nos dirigimos al Team Explorer al panel de builds y ahi creamos una nueva   Ante nosotros se abrio una ventana con un wizard que nos ayudaria a definir la build. Empezamos asignando un nombre  Seguimos los pasos del wizard y en la segunda pantalla configuramos la planificacion de las ejecuciones. La idea que teniamos era crear una de tipo continuous integration que se lanzara en cada nuevo check in comprobando la integridad del nuevo codigo.  Despues configuramos los directorios de trabajo. Aqui asociamos las rutas dentro del control de codigo fuente con variables de ejecucion de la build. Diferenciamos entre el codigo fuente y las herramientas  nuestroproyectodemo  SourceDirsrc  nuestroproyectodemotools  SourceDirtools   En la siguiente pantalla definimos una carpeta dentro del control de codigo fuente donde almacenar los resultados de las diferentes ejecuciones. Lo que comunmente se conoce como drop folder.  Team Foundation Server usa proyectos de MsBuild para realizar las builds. Para poder editar el proyecto creamos una carpeta nueva en el control del codigo fuente y en la siguiente pantalla la configuramos. Esto implicaba que cuando terminaramos de definir la build podriamos dirigirnos a esta carpeta y explorar los archivos para modificar configuraciones a mano.  Como el sistema no encontro un archivo de proyecto en la carpeta que introducimos nos indico que teniamos que crear un proyecto nuevo. Asi que lo hicimos presionando el boton de create. Entonces una nueva ventana se abrio y nos dio a elegir entre una configuracion basada en un archivo de build de Ant o en un archivo POM de Maven. Seleccionamos esta ultima y buscamos en el repositorio de codigo fuente el archivo pom.xml de nuestro proyecto. Este archivo fue creado por el arquetipo de Maven dentro de eclipse de forma automatica.  Una vez ya habiamos creado el proyecto de MsBuild nos dirigimos a la ultima pantalla del wizard. Aqui se pueden definir las politicas de retencion. Es decir el lugar donde decidimos que informacion y cuando queremos guardarla sobre los resultados de la ejecucion de la build. Nosotros dejamos las opciones que vienen por defecto.  Para finalizar tuvimos que configurar unos parametros de entorno que desafortunadamente no tenian pantalla que nos ayudara a configurarlos. Asi que buscamos la carpeta con los archivos de proyecto que definimos antes y la mapeamos a un directorio local.  Entonces abrimos el archivo TFSBuild.proj. Y antes de que acabara la seccion de PropertyGroup justo antes de ItemGroup anadimos los directorios donde MsBuild va a buscar el JDK y Maven  JAVA_HOMESolutionRoottoolsjdkJAVA_HOME  M2_HOMESolutionRoottoolsmavenM2_HOME   La peculiaridad es que aunque usemos diferentes versiones de Maven la variable siempre se llamara M2_HOME. Pero en este caso subimos la ultima version del momento una 3. A partir de este momento ya teniamos la build configurada y lista para ser lanzada. Para probarlo nos dirigimos al panel correspondiente de Team Explorer y marcado la build que acababamos de crear seleccionamos la opcion del menu contextual Queue new build.  Esperamos a que la ejecucion terminara y pudimos ver en los resultados que habia sido exitosa  En adicion comprobamos que cuando insertabamos codigo erroneo concretamente borramos un  la build fallaba. Asi ya podiamos estar seguros de que todo estaba correcto en nuestro entorno. Conclusiones Con la satisfaccion de haber conseguido superar el reto en unas horas presentamos los resultados a nuestro cliente. Y aunque aun no sabemos si ha decidido migrar o no estamos seguros de que al igual que nosotros habra aprendido mucho de esta prueba de concepto. Team Foundation Server a parte de sus conocidas virtudes se ha mostrado como una herramienta muy potente y valida para equipos mixtos con diferentes lenguajes de programacion y entornos de trabajo. Y en definitiva seguiremos recomendandola siempre que un cliente nos pregunte por una buena herramienta que le ayude en la gestion de sus proyectos."
    } ,
  
    {
      "title"    : "Patrones de dise&ntilde;o: Mediator",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/10/18/patrones-de-diseno-mediator",
      "date"     : "2012-10-18 09:32:22Z",
      "content"  : "En el mundo de la programacion orientada a objetos una de las maximas que debemos cumplir si queremos desarrollar un codigo de calidad es que debemos buscar una elevada cohesion con bajo acoplamiento. Y con el fin de ayudarnos en esta ardua tarea aparece el patron Mediator.La cohesion es una de las caracteristicas mas importantes de la OOP Object Oriented Programming. Se refiere a que hay que dotar a las clases de un solo ambito de desarrollo y que a su vez todo lo referente a ese ambito quede encapsulado dentro de una sola clase. Si hemos conseguido esto diremos que tenemos una alta cohesion. Es un concepto semejante al primer principio de SOLID el principio de responsabilidad unica una clase solo debe tener una y solo una razon para se modificada.Para poder exponerlo claramente vamos a desarrollar un ejemplo de una aplicacion cualquiera de escritorioEn este mockup podemos observar que nuestra aplicacion va a constar de tres controles de usuario. Con un recuadro rojo representamos el control encargado de pedir los datos al usuario para realizar las busquedas. En azul un panel en el que se mostrara un listado de resultados de las busquedas. Y para terminar en verde mostraremos los detalles del resultado de la busqueda que seleccionemos. El codigo de los controles podria ser algo parecido a estopublic class SearchControl    public void OnSearchstring text             search in repository    public class ResultControl    public void OnItemSelectedItem item             on item selected method        public void ShowSearchIEnumerableItem items             show the items in the UI    public class DetailControl    public void ShowDetailsItem item             show the details of the item in the UI        public void ShowEmpty             clear the UI    Quiza esta no sea la mejor implementacion pero si que sirve para hacernos a la idea de la responsabilidad unica de cada una de las clases y como esto nos lleva a que todo lo referente a esa responsabilidad este encapsulado dentro de la propia clase. Es decir SearchControl realizara la busqueda segun el texto introducido por el usuario ResultControl solo mostrara el listado de resultados y DetailControl tiene como unica responsabilidad mostrar los detalles de un Item seleccionado. Asi que podemos decir que nuestro codigo goza de una alta cohesion.El codigo que acabamos de desarrollar no tiene implementado el comportamiento completo que deben tener estos controles. El control de busqueda debera indicarle al de resultados los elementos que tiene que mostrar y a su vez el de resultados debe comunicar al control de detalles cual es el elemento que esta seleccionado para mostrar sus detalles. Por lo que vamos a completar nuestro codigopublic class SearchControl    private ResultControl resultControl    private DetailControl detailControl    public void SetRelatedControlsResultControl resultControl DetailControl detailControl            this.detailControl  detailControl        this.resultControl  resultControl        public void OnSearchstring text             search in repository        resultControl.ShowSearchlistOfItems        detailControl.ShowEmpty    public class ResultControl    private DetailControl detailControl    public void SetRelatedControlDetailControl detailControl            this.detailControl  detailControl        public void OnItemSelectedItem item            detailControl.ShowDetailsitem        public void ShowSearchIEnumerableItem items             show the items in the UI    public class DetailControl    public void ShowDetailsItem item             show the details of the item in the UI        public void ShowEmpty             clear the UI    Hemos anadido una serie de funciones para referenciar los controles relacionados con cada clase y luego realizamos llamadas de uno a otro para que tenga coherencia el comportamiento de la pantalla. El problema es que haciendo esto acabamos de acoplar las clases de nuestra aplicacion.Decimos que existe acoplamiento cuando clases relacionadas necesitan conocer detalles sobre comportamiento interno unas de otras para poder desempenar correctamente su funcion. De esta forma los cambios se iran propagando de unas clases a otras y como resultado tendremos un codigo dificil de seguir leer y por lo tanto mantener.Debemos buscar un bajo acoplamiento y una alta cohesionExisten varias formas de evitar el acoplamiento entre clases. Pero cuando en nuestro escenario nos encontramos una gran cantidad de clases que necesitan interactuar entre si para funcionar correctamente podriamos crear un mecanismo para facilitar la comunicacion y asi evitar que unas clases tengan que conocer la existencia de otras.Con el fin de desarrollar esta solucion vamos a crear dos interfaces nuevas una para definir que la clase es un control que se puede comunicar con otros a la que llamaremos ICommunityControl. Y otra que servira como facilitador de la comunicacion que por su labor llamaremos IMessageBoardpublic interface ICommunityControl    IMessageBoard MessageBoard  get public interface IMessageBoard    void RegisterICommunityControl control    void NotifySearchResultIEnumerableItem items    void NotifyItemSelectedItem itemLa idea es que un control pueda comunicarse con el resto usando el MessageBoard quien sera el encargado de enviar los mensajes correspondientes al resto de los controles. Por esta razon requeriremos dos metodos para notificar los diferentes acontecimientos y otro para registrar los controles que estan en funcionamiento. La implementacion bien podria ser estapublic class MessageBoard  IMessageBoard    private ResultControl resultControl    private DetailControl detailControl    public void RegisterICommunityControl control            if control is ResultControl            this.resultControl  ResultControl control        if control is DetailControl            this.detailControl  DetailControlcontrol        public void NotifySearchResultIEnumerableItem items            this.detailControl.ShowEmpty        this.resultControl.ShowSearchitems        public void NotifyItemSelectedItem item            this.detailControl.ShowEmptyitem    De esta forma bastara con implementar el contrato ICommunityControl en cada uno de nuestras clases y enviar notificaciones a la MessageBoard en lugar de al resto de clasespublic class SearchControl  ICommunityControl    public IMessageBoard MessageBoard  get private set     public SearchControlIMessageBoard messageBoard            this.MessageBoard  messageBoard        this.MessageBoard.Registerthis        public void OnSearchstring text             search in repository        this.MessageBoard.NotifySearchResultlistOfItems    public class ResultControl  ICommunityControl    public IMessageBoard MessageBoard  get private set     public ResultControlIMessageBoard messageBoard            this.MessageBoard  messageBoard        this.MessageBoard.Registerthis        public void OnItemSelectedItem item            this.MessageBoard.NotifyItemSelecteditem        public void ShowSearchIEnumerablelgItem items             show the items in the UI    public class DetailControl  ICommunityControl    public IMessageBoard MessageBoard  get private set     public DetailControlIMessageBoard messageBoard            this.MessageBoard  messageBoard        this.MessageBoard.Registerthis        public void ShowDetailsItem item             show the details of the item in the UI        public void ShowEmpty             clear the UI    Y como podemos observar hay cierto comportamiento comun entre los controles por lo que crearemos una base comun para reaprovechar codigopublic abstract class CommunityControlBase  ICommunityControl    public IMessageBoard MessageBoard  get private set     protected CommunityControlBaseIMessageBoard messageBoard            this.MessageBoard  messageBoard        this.MessageBoard.Registerthis    public class SearchControl  CommunityControlBase    public SearchControlIMessageBoard messageBoard  basemessageBoard            public void OnSearchstring text             search in repository        this.MessageBoard.NotifySearchResultlistOfItems    public class ResultControl  CommunityControlBase    public ResultControlIMessageBoard messageBoard basemessageBoard            public void OnItemSelectedItem item            this.MessageBoard.NotifyItemSelecteditem        public void ShowSearchIEnumerableItem items             show the items in the UI    public class DetailControl  CommunityControlBase    public DetailControlIMessageBoard messageBoard basemessageBoard            public void ShowDetailsItem item             show the details of the item in the UI        public void ShowEmpty             clear the UI    Asi que hemos conseguido que nuestros controles tengan una alta cohesion con un bajo acoplamiento y a que a su vez se comuniquen unos con otros. Pero si estudiamos detenidamente este nuevo codigo nos daremos cuenta de que no hemos eliminado el acoplamiento solo lo hemos desplazado a la clase MessageBoard.La idea es que nuestro objeto de comunicacion no este acoplado tampoco con el resto de las clases por lo que vamos a cambiar la filosofia de notificaciones. En lugar de tener dos metodos con diferentes parametros que realizan acciones directamente en los controles vamos a crear un metodo generico que envie notificaciones de cualquier tipo. Y en lugar de registrar un control completo para luego gestionarlo vamos a crear acciones genericas que respondan a un tipo de datospublic interface IMessageBoard    void RegisterTDataActionTData handler    void NotifyTDataTData datapublic class MessageBoard  IMessageBoard    private readonly Listobject handlers  new Listobject    public void RegisterTDataActionTData handler            this.handlers.Addhandler        public void NotifyTDataTData data            foreach var handler in this.handlers                    var action  handler as ActionTData            if action  null                actiondata            Y para adaptar los controles a este nuevo MessageBoard tendremos que modificar algo nuestro codigopublic abstract class CommunityControlBase  ICommunityControl    public IMessageBoard MessageBoard  get private set     protected CommunityControlBaseIMessageBoard messageBoard            this.MessageBoard  messageBoard    public class SearchControl  CommunityControlBase    public SearchControlIMessageBoard messageBoard  basemessageBoard            public void OnSearchstring text             search in repository        this.MessageBoard.NotifylistOfItems    public class ResultControl  CommunityControlBase    public ResultControlIMessageBoard messageBoard basemessageBoard            this.MessageBoard.RegisterIEnumerableItemShowSearch        public void OnItemSelectedItem item            this.MessageBoard.Notifyitem        public void ShowSearchIEnumerableItem items             show the items in the UI    public class DetailControl  CommunityControlBase    public DetailControlIMessageBoard messageBoard basemessageBoard            this.MessageBoard.RegisterIEnumerableIteml  ShowEmpty        this.MessageBoard.RegisterItemShowDetails        public void ShowDetailsItem item             show the details of the item in the UI        public void ShowEmpty             clear the UI    Basicamente lo que hemos hecho es anadir en el constructor unas funciones de Register que lo que hacen es decirle al MessageBoard que cuando se notifique algo del tipo que indicamos en la funcion realice esa accion lambda. Asi logramos que los controles solo tengan un comportamiento especial para unos datosespecificosy liberamos de esta responsabilidad al MessageBoard.Con estas ultimas modificaciones por fin hemos conseguido desacoplar nuestros controles y tambien la clase MessageBoard. Y a esto se le conoce comunmente como patron Mediador Mediator Pattern.En nuestro caso particular el Mediator seria el MessageBoard y los Colleagues estarian representados por los CommunityControls. La idea de este patron es comunicar diferentes clases que implementan IColleague usando un IMediator. Y aunque aqui hemos expuesto una implementacion del mismo existen unas cuantas variaciones sobre como tiene lugar la comunicacion.A muchos despues de ver este codigo les vendra a la cabeza un artefacto llamado EventAggregator que podemos encontrar en varias frameworks de desarrollo MVC y MVVM. Y eso es porque no es ni mas ni menos que una implementacion del patron Mediator.Un patron de comportamiento y muy util en UITanto el Strategy Pattern como el Mediator son lo que se llaman patrones de diseno de comportamiento ya que definen una forma de comunicacion entre clases. Remarcamos esta caracteristica porque es sencillo caer en el error de pensar que este patron esta destinado en exclusiva a acompanar a la interfaz grafica de usuario.Un ejemplo muy comun que podemos encontrar por la red es la gestion de una aplicacion de chat. Aqui se implementa el patron Mediator al convertir a los Speakers en Colleagues y la ChatRoom en el Mediator del sistema que se dedicara a realizar broadcast de los mensajes a todos los usuarios conectados.public interface IChatRoom    void RegisterISpeaker speaker    void Sendstring messagepublic interface ISpeaker    IChatRoom ChatRoom  get set     void OnReceivestring message    void OnSendstring messagepublic class ChatRoom  IChatRoom    private readonly ListISpeaker speakers  new ListISpeaker    public void RegisterISpeaker speaker            speakers.Addspeaker        speaker.ChatRoom  this        public void Sendstring message            speakers.ForEachspeaker  speaker.OnReceivemessage    public class Speaker  ISpeaker    public IChatRoom ChatRoom  get set     public void OnReceivestring message             write message on UI        public void OnSendstring message            this.ChatRoom.Sendmessage    Aunque queda claro que es un patron que resulta muy util a la hora de conectar artefactos en UI y mas usando patrones como MVVM MVC o cualquier derivacion del Model Presenter.Por esta razon mediator se ha convertido en uno de los patrones de diseno basicos para programar en javascript. Si quisieramos implementar la solucion en formato js podriamos tener un codigo parecido a estevar mediator  function     var handlers      function registerhandler fn         if handlershandler handlershandler          handlershandler.push context this callback fn         function notifyhandler         if handlershandler return false        var args  Array.prototype.slice.callarguments 1        for var i  0 l  handlershandler.length i  l i               var subscription  handlershandleri            subscription.callback.applysubscription.context args                return         register register        notify notify    Usando este mediador podriamos comunicarnos por ejemplo entre ViewModels de KnockOut Widgets de APIs de terceros o cualquier otro tipo de artefacto que podemos usar en javascript. La forma de hacerlo seria algo parecido a estovar myListenerWidget  function mediator     mediator.registernotification function data  alertdata     received     return  var myNotifierWidget  functionmediator     var sendNotification  functiondata         mediator.notifynotification data        return         sendNotification sendNotification    var widget1  new myListenerWidgetmediatorvar widget2  new myNotifierWidgetmediatorwidget2.sendNotificationHi mediatorConclusionesEl uso del patron Mediator va hacer nuestro codigo mucho mas legible ya que favorece la cohesion. Su uso ayudara a crear clases desacopladas y por lo tanto nuestro codigo sera mas facil de probar usando tests unitarios. Ademas va a simplificar los protocolos de comunicacion al estar centralizados en un solo artefacto.Pero hay que saber que Mediator es un patron dificil de implementar. Lo mas recomendable es usarlo como pasarela de comunicacion sin logica interna porque podemos caer en el antipatron de acoplar todas nuestras clases con un mediador de logica bastante compleja. Algo que nos puede dar mas de un quebradero de cabeza."
    } ,
  
    {
      "title"    : "Patrones de diseño: Strategy",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/10/03/patrones-de-diseno-strategy",
      "date"     : "2012-10-03 14:10:06Z",
      "content"  : "La mayor parte de los problemas que nos podemos encontrar al usar patrones de diseno vienen de no ser capaces de reconocer en que contextos hay que aplicarlos.Caemos en la trampa de al intentar escribir codigo mejor generar deuda tecnica ya que el patron aplicado no resolvia el problema que teniamos en realidad. Es por eso que la pregunta mas importante que tenemos que responder cuando empezamos a estudiar un patron de diseno es para que sirve.El patron Strategy tiene sentido cuando nos encontramos en un escenario en el que para conseguir un objetivo tenemos diferentes formas de afrontarlo. Por poner un ejemplo imaginemos que estamos desarrollando un programa que anade estilos en formato HTML a un texto. Los estilos que soporta seran poner en negrita letra cursiva y subrayar.Una solucion que podriamos dar seriapublic enum Styles    Bold    Italic    Underlinepublic class Stylerpublic string SetStylestring input Styles styleswitch stylecase Styles.Boldreturn b   input   bcase Styles.Italicreturn i   input   icase Styles.Underlinereturn u   input   udefaultthrow new NotImplementedExceptionThis style is not supportedComo podemos ver hemos usado un bloque de tipo switch y en dependencia del estilo que queramos anadirle al texto de entrada input realizamos una accion u otra. El resultado final es que si dentro de dos dias tenemos el requerimiento de anadir dos o tres estilos nuevos a nuestro sistema tendremos que anadir mas bloques case a nuestro codigo. Esto en definitiva lo que provocaria es que no estuvieramos cumpliendo con el principio SOLID de Open Closed.Este principio senala que nuestro codigo deberia estar abierto a la extension pero cerrado a la modificacion. Es decir que para anadir una nueva funcionalidad no tengamos la necesidad de modificar los algoritmos que ya estan programados. Y es aqui es donde el patron strategy cobra su importancia.Con el fin de no tener que modificar nuestro bloque switch con cada nuevo estilo que queramos aplicar al texto vamos a dividir cada uno de los estilos existentes en clases mas pequenas que solo resuelvan un estilo cada unapublic class BoldStyler    public string SetStylestring input            return b   input   b    public class ItalicStylerpublic string SetStylestring inputreturn i   input   ipublic class UnderlineStylerpublic string SetStylestring inputreturn u   input   uComo podemos ver todas las clases son muy parecidas y podriamos sacar una interface comun que defina su comportamientopublic interface IStyler    string SetStylestring inputpublic class BoldStyler  IStyler...public class ItalicStyler  IStyler...public class UnderlineStyler  IStyler...Ahora tendriamos que modificar el codigo inicial para que use estas nuevas clases y seguir resolviendo el problemapublic class Styler    public string SetStylestring input IStyler styler            return styler.SetStyleinput    Y si por un casual ahora necesitaramos anadir un nuevo formato a nuestro programa solo tendriamos que desarrollar un nuevo objeto que implementara IStyler. Y asi evitariamos tener que modificar el codigo que ya tenemos escrito.Dando un repaso a los objetos que hemos ido desarrollando hasta este momento podriamos extraer un diagrama de clases como esteDonde vemos que nuestro objeto Styler consume objetos que implementan IStyler como son BoldStyler ItalicStyler . . Un diagrama semejante al que define el patron StrategyDonde los artefactos sonContext es el objeto que orquesta el funcionamiento de las estrategias. Puede recibir una de estas por parametro o gestionarlas internamente.IStrategy es la definicion comun que tiene que implementar todo algoritmo que soporte el sistema.ConcreteStrategyX los objetos que implementando la interface comun desarrollan un algoritmo concreto.En este primer ejemplo hemos creado un contexto que acepta recibir una estrategia como parametro pero quiza el problema podria ser que obligatoriamente hay que aplicar los tres formatos al texto que le pasemos. En este caso podriamos generar una clase de contexto como estapublic class Styler    private readonly ListIStyler strategies  new ListIStyler                                                                                            new BoldStyler                                                new ItalicStyler                                                new UnderlineStyler                                                public string SetStylestring input            var result  input        foreachvar strategy in this.strategies                    result   strategy.SetStyleresult            return resultY si usaramos alguna Framework de inyeccion de dependencias como Autofac StructureMaps o NInject podriamos aprovecharnos de los sistemas de escaneo de ensamblados para poder obtener en el constructor todas las estrategias de un tipo correspondientepublic class Styler    private readonly IEnumerableIStyler strategiespublic StylerIEnumerableltIStylergt strategies    this.strategies  strategiespublic string SetStylestring input    var result  input    foreachvar strategy in this.strategies            result   strategy.SetStyleresult        return resultUna de las grandes ventajas del Strategy Pattern es que no es exclusivo de c o la plataforma .Net puede ser usado con diferentes lenguajes de programacion como por ejemplo Javascript. Una implementacion de este mismo codigo podria ser estafunction HtmlStyler     this.setStyle  function input         var result  input        for var key in this.strategies             var strategy  this.strategieskey            if strategy.setStyle                result  strategy.setStyleresult            else                throw Invalid strategy            return resultHtmlStyler.prototype.strategies  HtmlStyler.prototype.strategies.boldStyler  setStyle functioninput return b   input   bHtmlStyler.prototype.strategies.italicStyler  setStyle function input return i   input   iHtmlStyler.prototype.strategies.underlineStyler  setStyle function input return u   input   uLa peculiaridad de esta implentacion en Javascript es el uso de prototype para facilitar las futuras caracteristicas nuevas que se pueden desarrollar. No hara falta modificar el programa original si no anadir una nueva propiedad a las estrategias del prototipo de nuestro objeto.Y como no tambien podriamos realizar el mismo codigo en el lenguaje de programacion de moda Typescriptinterface IStyler     setStyle input string  stringclass HTMLStyler strategies IStylerconstructor strategies IStyler     this.strategies  strategiessetStyleinput stringstring     var result string  input    for var i  0 i lt this.strategies.length i           var strategy  IStyler  this.strategiesi        result  strategy.setStyleresult        return resultclass BoldStyler implements IStyler setStyleinput string return b   input   bclass ItalicStyler implements IStyler setStyleinput string return i   input   iclass UnderlineStyler implements IStyler setStyleinput string return u   input   uvar styler  new HTMLStylernew BoldStyler new ItalicStyler new UnderlineStyleralertstyler.setStyleholaY podremos ver claramente que el resultado final se asemeja mas al que desarrollamos con c que a la solucion propuesta en javascript.ConclusionesMientras describiamos el patron Strategy hemos dejado caer alguno de sus beneficios como por ejemplo que es mas facil de leer el codigo. Tambien sera mas facil por tanto de mantener y por supuesto de ampliar con nuevas funcionalidades. Gracias a este patron vamos a cumplir con dos de los principios de SOLID el principio de responsabilidad unica al crear pequenas clases que contienen un algoritmo muy concreto y el de abiertocerrado abriendo nuestra solucion a la extension pero no a la modificacion.Tambien queda claro el problema que podria suponer a largo plazo una gran cantidad nueva de objetos para que sean gestionados por el hilo principal de nuestro programa. Por lo que para un sistema de tiempo real o donde la velocidad de respuesta y el poco consumo de recursos fueran lo mas importante no seria la implementacion ideal.No obstante teniendo en cuenta que en este ejemplo usamos lenguajes como Javascript o c en los que la velocidad no es su punto fuerte siempre deberiamos pensar en este patron antes de escribir un bloque switch."
    } ,
  
    {
      "title"    : "Analizando la seguridad de una aplicación en .net",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/09/18/analizando-seguridad-aplicacion-net",
      "date"     : "2012-09-18 09:15:52Z",
      "content"  : "Se han escrito muchas lineas acerca de la seguridad de las aplicaciones que realizamos en .Net. Se dice que son muy faciles de hackear que aun usando herramientas de ofuscacion la seguridad de los empaquetados que se distribuyen es nula.Asi pues nos hemos propuesto buscar una aplicacion comercial realizada con la Framework de .net de Microsoft y saltarnos su seguridad para poder disfrutar de todas las ventajas que tiene pero sin pasar por caja.Para evitar problemas legales y que la aplicacion se distribuya de forma ilegal no vamos a publicar su nombre y el metodo empleado no sera del todo completo. Pero si esperamos que el lector comprenda la dificultad y los pasos a dar para conseguir piratear una aplicacion.Antes de empezar hemos tenido que instalar ciertas herramientas y aplicacionesRed Gate ReflectorReflexil AddIn para ReflectorJetBrains dotPeekVisual Studio 2010Y como no la aplicacion que tenemos intencion de hackear.Lo primero que hemos hecho es observar la aplicacion y encontrar un punto donde no nos deja acceder debido a que no tenemos la version de pago. Esto ha ocurrido al intentar anadir una cuenta de usuario nueva al sistema.Hemos decidido abrir el ejecutable con la aplicacion ReflectorY nos hemos llevado una grata sorpresa al encontrar indicios del patron ModelViewViewModel. Una cosa que hemos aprendido en nuestros anos de experiencia es que todo lo que se muestra en pantalla y utiliza este patron tiene que pasar por el ViewModel con lo que no nos ha costado encontrar un lugar concreto donde la aplicacion iba a comprobar que tenemos la version de pago.A partir de este punto hemos decidido utilizar el dotPeek en lugar de Reflector ya que trae una serie de herramientas de busqueda y exploracion que no trae de serie la anterior herramientaNos hemos situado en el lugar que sabiamos que la aplicacion nos denegaba el acceso. Con la utilidad de buscar todos los usos de una variable hemos realizado un escaneo en busca del lugar donde se fija el valor de la variable que dice que no tenemos la version de pago. Como resultado final hemos alcanzado una funcion llamada CheckBoon en una clase totalmente diferente a las que estabamos buscando en un principio.Asi que hemos vuelto a la herramienta de Reflector para sacarle partido al AddIn previamente instalado Reflexil.Al activarlo encontraremos una nueva ventana debajo del codigo descompilado con unos simbolos un tanto extranos en una lista. A estos comandos se les denomina IL o Intermediate Language que es un lenguaje de programacion de bajo nivel en el que se convierten todas las aplicaciones de la plataforma .net antes de convertirse en byte codes en forma de ensamblado.La virtud mas importante de Reflexil es mostrar editar y guardar cambios en este lenguaje para un ensamblado ya compilado. Pero puede surgir el problema de que no todo el mundo conoce IL y para solventarlo usaremos el Visual Studio.La forma mas simple de conseguir el codigo IL que necesitamos es escribir una simulacion en un proyecto nuevo de Visual Studio para asi poder encontrar el codigo IL con el tandem Reflector ReflexilComo resultado hemos simulado la funcion CheckBoon con un codigo semejante al que nos gustaria que tuviera.Al coger esta nueva aplicacion y abrirla con Reflector encontraremos que Reflexil ya nos ha hecho el trabajo de convertir la funcion que deseamos a IL.A partir de aqui nuestro trabajo consistira en borrar todas las instrucciones de la funcion originalY anadir nuevas con la misma informacion transformando nombres de propiedades y tipos de objeto que podemos leer en nuestra simulacionAdemas otra de las ventajas de Reflexil es que nos facilita la tarea con una pequena herramienta para creacion de instrucciones de IL con un buscador de operadores incluidoUna vez tenemos todas las operaciones escritas el codigo no cambiara pero el IL si que estara como queriamosEntonces solo nos quedaran dos acciones la primera verificar los cambios que hemos realizado dirigiendonos al panel de nuestra izquierda y en las opciones del ejecutable buscando Reflexil  VerifySi nos dice que todo esta correctoEn el mismo menu encontraremos la opcion de guardar momento en el que recomendamos no usar exactamente el mismo nombre que el ensamblado original por si acaso.Y ejecutando nuestra version parcheada en lugar de la original ya tendriamos las opciones de una version de pago sin haber abonado el precio de la aplicacion.Ahora que cada uno saque sus propias conclusiones. Personalmente considero que un desarrollador al que realmente le importe la seguridad se puede encontrar desolado tras leer este articulo ya que Microsoft continua con una asignatura pendiente en el tema de la seguridad de los ensamblado de la plataforma .Net."
    } ,
  
    {
      "title"    : "Patrones de diseño: Singleton",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/09/10/patrones-diseno-singleton",
      "date"     : "2012-09-10 13:06:17Z",
      "content"  : "Cuando hablamos de un patron de diseno nos referimos a una solucion a un problema concreto en el desarrollo de software. Pero no cualquier solucion solo aquellas que se ha desmostrado que son eficientes en diferentes escenarios yreutilizables en gran cantidad de contextos de aplicaciones. Por lo tanto aunque los ejemplos que podamos dar esten en un lenguaje de programacion concreto la idea sera extrapolable a diferentes lenguajes de programacion orientada a objetos.SingletonEl patron singleton consiste en crear una instancia de un objeto y solo una para toda nuestra aplicacion. Seria como una especie de variable global que almacena nuestro objeto.En un primer momento esta definicion puede sonar muy extrana. Por lo general siempre se recomienda no usar variables globales en una aplicacion y mucho menos en programacion orientada a objetos. Pero cuando hablamos de singleton estamos jugando a crear una especie de variable global de forma encubierta. Como puede ser esto un patronPara responder a esta pregunta vamos a proponeros dos escenarios diferentesPiensa en una aplicacion Web que almacena en un objeto una serie de valores tipo parametros de configuracion. Estos parametros son comunes para toda la aplicacion. Se guardan en una base de datos y si son modificados por un administrador quedan modificados para todos los usuarios que acceden a la pagina.Ahora vamos a imaginar que tenemos un recurso compartido como puede ser un fichero en el que escribimos un log de la aplicacion. Este log puede ser accedido desde cualquier parte de la aplicacion. Pero sabemos que un fichero no se puede abrir si otro proceso lo abrio anteriormente y aun no lo ha cerrado.Para ambos problemas podemos encontrar una solucion usando el patron singleton. Crearemos una especie de variable global pero con unas caracteristicas concretassolo se puede instanciar una vez singleinstanceno se debe instanciar si nunca fue utilizadaes threadsafe que quiere decir que sus metodos son accesibles desde diferentes hilos de ejecucion sin crear bloqueos ni excepciones debido a la concurrenciano tiene un constructor publico luego el objeto que la usa no puede instanciarla directamenteposee un mecanismo para acceder a la instancia que se ha creado mediante una propiedad estatica por ejemploTeniendo en cuenta estas caracteristicas vamos a desarrollar una clase singletonpublic sealed class Singleton    private static Singleton instanceprivate Singletonpublic static Singleton Instance    get            if instance  null instance  new Singleton        return instance    En esta pequena porcion de codigo hemos conseguido realizar una unica instancia en el momento en el que se llama por primera vez. Ademas hemos creado un constructor con acceso privado para que nadie pueda instanciar la clase. Y para terminar hemos creado una propiedad de solo lectura con la que se puede acceder a la instancia creada. Pero esta no sera Threadsafe. Para conseguirlo podriamos modificar la clase de la siguiente formapublic sealed class Singleton    private static readonly Singleton instance  new Singletonprivate Singleton  public static Singleton Instance  get  return instance  Al crear el atributo que almacena la instancia como readonly y al ser estatica se instanciara al arrancar la aplicacion. Asi conseguiremos que sea una clase threadsafe. Es decir que no habra problemas si varios procesos acceden a esta clase al mismo tiempo. No obstante si quisieramos respetar que solo se instanciara el objeto bajo demanda deberiamos usar bloqueospublic sealed class Singleton    private static readonly object locker  new object    private static volatile Singleton instanceprivate Singleton  public static Singleton Instance    get            if instance  null                    lock locker                            if instance  null instance  new Singleton                            return instance    Gracias al bloqueo ya podremos ejecutar nuestra clase singleton en un contexto multihilo instanciandola solo cuando se ha solicitado la primera vez. A este efecto de carga en diferido se le denomina en ingles Lazy Loading. Y desde la version 4.0 de la framework .net se nos provee un objeto que nos ayuda a realizarla Lazy. Por lo que podriamos simplificar nuestro ejemplo usandolopublic sealed class Singleton    private static readonly LazySingleton instance  new LazySingleton  new Singletonprivate Singleton  public static Singleton Instance    get            return instance.Value    El objeto Lazy ya es de por si threadsafe y en su declaracion simplemente debemos indicarle de que forma se debe instanciar el objeto que contiene. Por esta razon es posiblemente la mejor implementacion del patron singleton.Si por ejemplo estuvieramos desarrollando la herramientas de log de nuestra aplicacion bastaria con que anadieramos las funciones necesarias para escribir en el log a nuestra clase singletonpublic sealed class Logger    private static readonly LazyLogger instance  new LazyLogger  new Loggerprivate Logger  public static Logger Current    get            return instance.Value    public void WriteInformationstring message    ...public void WriteWarningstring message    ...public void WriteErrorstring message    ...Viendo este codigo en nuestra aplicacion esta claro que para poder escribir en el log desde cualquier punto de la misma solo tendremos que hacer esta llamadaLogger.Current.WriteInformationUna informacionLogger.Current.WriteWarningUn avisoLogger.Current.WriteErrorUn errorAl pararnos a pensar las consecuencias de escribir este codigo caeremos en la cuenta de que singleton nos esta creando una dependencia en todo el programa donde queramos tener informacion del proceso en forma de logs eso es a lo largo de toda la aplicacion. Algo que comunmente conocemos como acoplamiento entre clases.El acoplamiento puede dar varios problemas a lo largo del ciclo de vida de un software. Como por ejemplo a la hora de realizar pruebas unitarias. Pero no es objeto de este articulo centrarse en este problema. Aunque si lo es proponer soluciones de implementacion del patron singleton que se adapten a un desarrollo solido.Si quisieramos evitar este acoplamiento es recomendable usar un IoC Container Inversion Of Control Container para respetar la D de los pincipios SOLID Dependency Inversion Principle. Esta por asi llamarla norma nos dice que debemos depender de las abstraciones las interfaces los contratos no de las concreciones clases que implementan esas interfaces.En las frameworks de inversion de control mas conocidas se han implementado mecanismos que nos permiten crear objetos singleton desde el propio contenedor. Esto quiere decir que simplemente tendriamos que crear una interfaz y una implementacion de la misma sin preocuparnos de como se intancia. Visto en forma de codigo seria estopublic interface ILogger    void WriteInformationstring message    void WriteWarningstring message    void WriteErrorstring messagepublic class Logger  ILoggerpublic Logger  ...public void WriteInformationstring message ...public void WriteWarningstring message ...public void WriteErrorstring message ...De esta forma delegariamos la gestion del ciclo de vida de las instancias al IoC Container que hayamos decidido. A continuacion mostraremos como podemos configurar una instancia singleton usando las frameworks de inyeccion de dependencias DI mas conocidasUsando Structure maps configurarObjectFactory.Initializex     x.ForILogger.Singleton.UseLogger recoger valorvar x  ObjectFactory.GetInstanceILoggerCon Ninject configurarIKernel ninject  new StandardKernelnew InlineModule              x  x.BindILogger.ToLogger              x  x.BindLogger.ToSelf.InSingletonScope recoger valorvar x  ninject.GetILoggerCon Unity configurarIUnityContainer container  new UnityContainercontainer.RegisterTypeILogger Loggernew ContainerControlledLifetimeManager recoger valorvar x  container.ResolveILoggerO con autofactvar builder  new ContainerBuilderbuilder   .Registerc  new Logger   .Asilogger   .SingleInstancevar container  builder.Build var x  container.ResolveiloggerPero esto no quiere decir que no nos sirva la implementacion de singleton que hicimos anteriormente ya que es posible que no nos fiemos o que nuestro contenedor no tenga ningun artefacto que nos facilite la implementacion singleton. Para estos casos podriamos hacer que un contenedor como Unity nos devolviera la instancia singleton que gestiona nuestra clase usando la propiedad estatica. Simplemente tendriamos que seguir usando una interface implementarla en nuestra clase singleton y registrar una instancia en lugar de una clase en el contenedorpublic interface ILogger    void WriteInformationstring message    void WriteWarningstring message    void WriteErrorstring messagepublic sealed class Logger  ILoggerprivate static readonly LazyLogger instance  new LazyLogger  new Loggerprivate Logger  public static Logger Current    get            return instance.Value    public Logger      ...public void WriteInformationstring message     ...public void WriteWarningstring message     ...public void WriteErrorstring message     ...De esta forma por ejemplo si usamos el contenedor de Unity tendriamos que registrar su valor asivar container  new UnityContainercontainer.RegisterInstanceILoggerLogger.CurrentCon este codigo seria nuestro singleton Logger quien gestione el ciclo de vida y conseguiriamos desacoplarnos de la implementacion gracias al IoC.Podriamos hacer lo mismo con Structure mapsObjectFactory.Initializex     x.ForILogger.UseLogger.Currentvar x  ObjectFactory.GetInstanceILoggerY para finalizar con NinjectIKernel ninject  new StandardKernelnew InlineModule              x  x.BindILogger.ToConstantLogger.Currentvar x  ninject.GetILoggerA lo largo de este articulo hemos visto diferentes formas de implementar el patron singleton. Un patron de desarrollo sigue siendo vigente y valido. Lo unico que tenemos que tener en cuenta es evitar aplicarlo donde no corresponde o de una forma incorrecta. Algo que conocemos como el antipatron singletonitis."
    } ,
  
    {
      "title"    : "Deuda Técnica",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/05/29/deuda-tecnica",
      "date"     : "2012-05-29 12:15:18Z",
      "content"  : "Que desarrollador de software nunca ha estado una semana programando a toda velocidad casi sin saber que hace ya que la fecha de entrega esta cerca y todo tiene que estar terminado para ayer Quien no se ha encontrado en la situacion de saber que esta programando una chapuza pensando que otro dia lo mejorara y ese dia nunca llega Quien alguna vez al ver una porcion de codigo fuente no ha exclamado indignado que narices hace este codigo o quien leches lo ha programadoA todo este codigo lo solemos calificar como chapuzas apanos napas mierdecillas. Su problema es que por lo general se va extendiendo de forma exponencial y acumulandose a lo largo del desarrollo de un proyecto. Y en consecuencia tenemos como resultado un codigo fuente que es muy dificil de mantener extender y reutilizar y un aplicativo que funcionalmente tiene problemas.Quien no ha pensado alguna vez que le seria mas facil rescribir todo un proyecto que mantenerlo o evolucionarloAl llegar a este punto podriamos pensar quealgohuele dentro delcodigo y ese olor no es bueno precisamente. Ademas asi como podemos identificar el olor que emana de nuestra papelera en relacion con lo que hemos tirado en ella y cuanto tiempo lleva tambien podemos distinguir los diferentes olores de un codigo fuente.A que huele el codigoEn castellano Code Smells se traduce como Olores de codigo. Parece ser que fue Kent Beck quien acuno este concepto a finales de la decada de los 90. Pero no se popularizo hasta su aparicion en el conocido libro de Martin Fowler Refactoring improving the Design of Existing Code.Los olores de un codigo fuente indican por lo general un problema mas profundo. La mejor forma que tenemos de tratarlos es evitando hacerlos. Y si por alguna razon no se ha podido evitar corregirlo lo antes posible.Quien no se ha encontrado con un proyecto organizado anarquicamente que no hay quien le encuentre logica o quien no ha impreso alguna vez una funcion que ocupa varias paginas de papel La conclusion es pensar que ese codigo huele. Pero A quePodemos encontrarnos diferentes olores pero se resumen por lo general en estos 7Rigidez Cuando algo es rigido significa que no se puede moldear. Un codigo rigido es aquel que resulta muy dificil de mantener y extender.Fragilidad Si alguna vez te has encontrado con un programa que al cambiar un detalle empieza a fallar en muchos puntos donde no has tocado nada ya te has encontrado con codigo fragil. Un codigo que solemos tratar con miedo y bordeamos para modificarlo lo menos posible generando asi mas codigo mal oliente.Inmovilidad Aunque la solucion tiene partes que serian interesantes en otros sistemas el esfuerzo y riesgo de separarlas del resto del codigo es tan grande que nunca se reutilizan se vuelven a escribir.Viscosidad Podemos definir como viscoso un entorno de desarrollo que es lento e ineficiente. Pero tambien es viscoso aquel codigo que es mas dificil de usar tal y como esta disenado. Por lo general preferiremos realizar unos hacks que programarlo como estaba pensado.Complejidadinnecesaria Contiene elementos muy complejos dificiles de identificar y entender su utilidad artefactos y funciones que no se usan y codigo que no es util en absoluto.Repeticiones de codigo la consecuencia de la maniobra informatica mas conocida del mundo copiar y pegar. Crear codigo repetido a lo largo de toda la aplicacion implica que no se ha conseguido identificar esas partes y agruparlas en funciones clases y contextos comunes.Opacidad Decimos que un objeto es opaco cuando no se puede ver a traves de el. Es decir un codigo que es dificil de entender y leer.El problema de generar un codigo que huele es que tarde o temprano vas a tener que enfrentarte a el. Y cuando tengas que modificarlo o ampliarlo puedes cometer el grave error de que las circunstancias te lleven a hacer una chapuza mayor. Siempre amparado en la escusa de no tocarlo demasiado ya que no entiendes muy bien del todo como o incluso que es lo que hace.Pensemos ahora en una situacion comun en la vida de una persona cualquiera que gracias al trabajo diario que realiza recibe un pago de forma mensual. Pero a pesar de tener un poco de dinero todos los meses nuestro protagonista ha visto un coche que le gusta mucho. El coche cuesta mas de lo que cobra en un mes pero menos de lo que va a cobrar durante todo el ano. Es entonces cuando teniendo en cuenta estos factores un banco le deja el dinero necesario para adquirir el coche con la condicion de que esta persona se lo vaya devolviendo a lo largo del ano. Al suceder esto decimos que esta persona tiene una deuda economica con el banco.De la misma forma si creamos codigo que huele podemos decir que estamos contrayendo unadeuda tecnicacon nuestro proyecto. Ya que algun dia tendremos que modificarlo o corregirlo y cuanto mas tiempo pase mayores seran los intereses.Endeudando el codigoEl termino Technical Debt Deuda Tecnica fue introducido en 1992 por Ward Cunningham. Es una metafora que viene a explicar que la falta de calidad en el codigo fuente de nuestro proyecto genera una deuda que repercutira en sobrecostes tanto en el mantenimiento de un software como en la propia operativa funcional de la aplicacion.Cuando hablamos de un sobrecoste puede significar desde tener que dedicarle mas tiempo o mas desarrolladores de los estimados hasta acumular malestar general y mal ambiente. Lo que nos puede llevar a rotaciones habituales en el equipo de desarrollo que nunca tendra esa experiencia que se adquiere con el tiempo en cada proyecto especifico. En adicion un numero elevado de problemas funcionales pueden crear la desconfianza del cliente y unas relaciones tensas con los comerciales el equipo de desarrollo y en general todos los involucrados en el proyecto.Asi que aunque el sobrecoste no se refiera directamente al factor economico al fin y al cabo siempre termina repercutiendo negativamente en el precio del proyecto.Y es que es facil deducir que a menor calidad de codigo mayor probabilidad de que contenga mas errores. Ademas en un codigo fuente complicado de entender tambien sera mucho mas costoso corregir problemas o programar nuevas funcionalidades.Pero el punto donde si se puede discutir es en determinar la cuantia de este coste.Mi experiencia personal dice que es bastante elevada y que ese coste sube de forma exponencial con respecto el tiempo si no se trata lo antes posible. Pero asi como no hay una formula maestra que nos calcule la calidad del codigo tampoco existe ninguna que nos permita averiguar cuanto va a repercutir la mala calidad en los costes.El problema es que algunas veces no es tan facil evitar la deuda tecnica. En muchas ocasiones el proyecto o el desconocimiento nos lleva inevitablemente a adquirirla y lo unico que podemos hacer es ser conscientes de que esta ahi. Y es por esta razon que aparece la necesidad de clasificarla.Tipos de deudasEs Martin Fowler quien confecciona elCuadrante de la Deuda Tecnica clasificandola segun dos factores La prudencia y la deliberacion de incurrir en este tipo de deuda. Combinandolos obtenemos como resultado cuatro tipos posiblesLadeuda prudente y deliberadaes la que se produce cuando somos conscientes de que estamos haciendo las cosas mal y factores externos nos obligan ha incurrir en deuda tecnica que tendremos que pagar posteriormente.Ladeuda prudente e inadvertidaaparece en todo proyecto. Y es que todo desarrollador adquiere experiencia a lo largo de un proyecto y llega un momento en el que se da cuenta de que podria haber hecho las cosas mejor gracias a los conocimientos que ha ido recolectando. Cuando llega este momento tenemos que evaluar cuando podriamos pagar esta deuda o incluso si merece la pena hacerlo.Ladeuda imprudente y deliberadaes la mas negativa. Acumular esta deuda es asumir que el proyecto que estamos desarrollando va a tener mala calidad. Puede ser producida por diferentes factores como por ejemplo el poco compromiso de los desarrolladores una mala gestion en el proyecto o un compendio de despropositos.Ladeuda imprudente e inadvertidapuede ser el resultado de una falta de formacion o experiencia necesaria. Es cuando por desconocimiento se toman las medidas equivocadas en el desarrollo.Probablemente la mas peligrosa sea esta ultima deuda. Las tres primeras pueden ser malas o peores pero al fin y al cabo siempre puedes asumirlas e intentar tomar medidas para solucionarlas. Sin embargo la cuarta debido a su naturaleza hace que permanezca oculta y pueda repercutir en el fracaso total de un proyecto.Si queremos que un proyecto llegue a buen puerto al final siempre tendremos que pagar la deuda tecnica que se ha generado.Pasando por cajaLa mejor forma de solucionar los problemas de deuda tecnica es evitarla. Y para ello un equipo bien formado comprometido y con una buena base de conocimientos es fundamental.PrevenirPara tener un software de mayor calidad es muy importante conocer de losprincipios basicos de la programacionSOLID Que es el acronimo de 5 principios.Single Responsibility una clase o funcion solo debe tener una y solo una razon para existir o ser modificada.OpenClose el codigo debe estar abierto a la extension pero cerrado a modificaciones.Liskov Substitution las clases derivadas deben poder ser sustituidas por su clase base.Interface Segregation Desgranar las interfaces lo mas fino posible para que sean lo mas especificas posible.Dependency Inversion Hay que depender de las abstracciones no de las concreciones.KISSKeep It Simple Stupid Manten tu codigo simple sencillo para que cualquier sea capaz de comprenderlo.DRYDont Repeat Yourself No repitas codigo.YAGNIYou Aint Gonna Need It No programes mas de la cuenta. Si no lo necesitas no lo hagas.Las practicas deeXtreme Programmingtambien conocido comoXP pueden resultar de gran ayuda. Por ejemploTDD La tecnica de Test Driven Design establece que hagamos las pruebas unitarias antes que el codigo. De esta forma conseguiremos tener un codigo probado 100. Ademas tener un codigo testable conlleva varios beneficios.Pair Programing Programar por parejas de tal forma que una persona escriba codigo pero ambas piensen en la solucion. Es un arma de doble filo dos cabezas piensan mejor que una pero ademas si sabes que alguien te esta mirando es muy probable que te lo pienses dos veces antes de hacer una chapuza en el codigo.Y no hay que olvidar herramientas que nos aportanmetricas de codigo que a pesar de no ser factores que deban comprometer un proyecto si que nos pueden ayudar a detectar secciones problematicas.TratarPero no siempre podemos adelantarnos y prevenirla para estos casos solo podemos acogernos al proceso derefactoring. Esto consiste en basandonos en todos los conceptos anteriores rescribir ciertas partes del codigo que consideramos que no estan del todo correctas.Antes de empezar a refactorizar un codigo fuente para pagar la deuda tecnica acumulada es muy importante tenerpruebas del codigo unitarias de integracion. Esto sera nuestra red de seguridad principal para poder llevar a cabo mejoras del codigo estando seguros de queno estamos rompiendo nada.InformarAdemas el pago de deuda tecnica hay queexplicarselo a todos los stakeholderstodos los involucrados en el proyecto de alguna forma y hacerles entenderla aunque a veces sea dificil. En este punto es donde Rodrigo Corral persona muy reconocida en la comunidad de desarrolladores espanola con un historial tan grande de exitos que no caben en este articulo introduce una herramienta que puede servirnos para explicar a un neofito de la programacion que es lo que se hace al pagar la deuda tecnica de un proyectola curva J.En economia se conoce como curva J a esa grafica que representa como para corregir el balance negativoXN de caja se recurre a devaluar el valor D. De esta forma en un principio preve una bajada mayor debido a unos costes mayores por culpa de esta devaluacion. Pero con el paso del tiempo To esta situacion se revertira terminando con un saldo positivo mayor XN. Como la curva resultante tiene una forma semejante a la letra J recibe este nombre aunque personalmente no le encuentro tanto parecido.Asi pues aplicando este concepto al pago de deuda tecnica vamos a poder explicar que vamos a realizar una inversion de tiempo en solucionar parte de esta deuda. En el eje vertical interpretaremos que tenemos la velocidad de produccion de nuestro equipo. Y en el eje horizontal el tiempo. Si invertimos en pagar la deuda tecnica nuestro proyecto va a tener una bajada de velocidad del desarrollo que repercutira en que no avancemos con la produccion de la funcionalidad esperada. Pero al haber conseguido mejorar la calidad del codigo en un futuro recuperaremos la inversion con creces porque sera mas facil afrontar desarrollos futuros.Sobre la rentabilidadPor lo general el pago de deudahay que planificarlo. Encontrar el momento adecuado dentro del ciclo de produccion. Hay que tener presente que no hay que enfrentarse a todos los problemas a la vezpoco a poco sera mas sencilloy productivo. Y sobre todo hay que estar seguro de quese va a obtener un beneficio claro y visibleal invertir en solucionar estos problemas. Lo que se conoce como asegurar elROIReturn of Investment oRetorno de Inversion en castellano.El ROI es un factor indicativo del rendimiento de una inversion y la forma de calcularlo coloquialmente esROI  beneficios  inversion  inversionCuanto mayor sea su valor significa que mejor ha sido el rendimiento obtenido. Para a partir de aqui calcular el porcentaje del rendimiento obtenido podemos multiplicar el ROI por 100. Por ejemplo si invertimos 1000 y obtenemos un beneficio de 3000 entonces 3000  1000  1000  2. Lo que quiere decir que hemos conseguido un rendimineto del 200 de la inversion.La situacion ideal seria poder calcular este factor antes de invertir pero no existe una respuesta concreta y empirica. No obstante lo que recomendamos es basarse en la experiencia y en una gran cantidad de informacion cuanta mas mejor. Por ejemplo preguntando periodicamente al equipo de desarrollo que mejoraria o cambiaria en el proyecto.A lo largo de este articulo hemos explicado que es el codigo que huele como nos lleva a generar deuda tecnica y hemos realizado una introduccion a sus posibles soluciones. En futuras publicaciones trataremos de explicar como afrontar mas concretamente un codigo heredado legacy plagado de deuda tecnica. Asi que os recomendamos estar atentos..."
    } ,
  
    {
      "title"    : "Gestión de excepciones con WCF",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/04/12/gestion-de-excepciones-con-wcf",
      "date"     : "2012-04-12 15:45:34Z",
      "content"  : "Hace unos dias en losforos de Windows Communication Foundation de MSDN un usuario preguntaba por la diferencia entre Faults y Exceptions dentro de esta plataforma. La respuesta me llevo a escribir una pequena introduccion a la gestion de excepciones para los servicios WCF que me gustaria ampliar en este articulo. Para los mas experimentados resultara un texto algo basico. Para los demas sin embargo les puede ayudar a programar siguiendo las buenas practicas en el desarrollo de sus servicios.Para empezar lo que vamos a necesitar es crear un servicio WCF que nos sirva como prueba. Este servicio tendra que cumplir el siguiente contratoServiceContractpublic interface IMyService    OperationContract    int Operationint aY para implementarlo vamos a crear una logica que lance diferentes excepciones segun el valor del parametro que se le pasa al metodo. O lo que es lo mismo vamos a desarrollar un metodo que si le pasas 0 devuelva un ArgumentException si le pasas un numero negativo lance una InvalidOperationException y si no devuelva el propio numero que se le envia. Asi que vamos allaServiceBehaviorInstanceContextMode  InstanceContextMode.PerCallpublic class MyService  IMyService    public int Operationint a            if a  0            throw new ArgumentExceptionCannot be zero    if a lt 0        throw new InvalidOperationExceptionThe parameter must be greater than zero    return aHasta aqui todos podemos ver que estamos desarrollando como siempre lo hemos hecho. Lanzamos las excepciones en nuestro codigo para informar del error que se ha producido. Por lo que con el fin de probarlo vamos a crear una funcion en una aplicacion cliente que sea capaz de llamar a este serviciopublic int CallOperationint i   using var factory  new ChannelFactoryIMyServiceMyServiceEndPointName         var proxy  factory.CreateChannel      return proxy.Operationi   Con este codigo crearemos un canal de conexion con nuestro servicio siempre y cuando todo este configurado correctamente en el app.config o web.config y llamaremos mediante un proxy al metodo Operation.Ahora teniendo en cuenta las excepciones que podemos lanzar vamos a probar nuestro codigopublic void SafeCallOperationint numero   try         var result  CallOperationnumero      System.Console.WriteLineEl resultado es    result      catchArgumentException aex         System.Console.WriteLineHas introducido 0    aex.Message      catchInvalidOperationException iex         System.Console.WriteLineHas introducido un numero menor de cero    iex.Message      catchException ex         System.Console.WriteLineError desconocido    ex.Message   Lo llamamos de tres formas diferentes para obtener los resultados diferentes que esperamosSafeCallOperation0SafeCallOperation1SafeCallOperation2Pero al ejecutar estas llamadas nos encontramos con una salida inesperadaError desconocido Cannot be zeroError desconocido The parameter must be greater than zeroEl resultado es 2Las dos primeras dan como resultado Error desconocido ... y la ultima si que responde con el resultado como esperabamos. Por que no ha respondido capturando las excepciones que hemos lanzado desde el servicioSi analizamos la ejecucion en modo debug nos daremos cuenta de que enviemos la excepcion que enviemos siempre recibimos en el cliente una de tipo FaultExceptionExceptionDetail. De esto deducimos que WCF convierte las excepciones en FaultException y cuando las queremos usar estamos perdiendo informacion importante para diferenciarlas.Esto ocurre porque una excepcion cualquiera no es serializable. Lo que quiere decir que no es materializablesolo puede existir en memoria y por lo tanto tampoco se puede enviar por un canal de comunicacion ni tampoco guardar en un fichero. Para eso existenFaultException y FaultExceptionT.SerializableAttributepublic class FaultExceptionTDetail  FaultExceptionAmbas excepciones son serializables y representan un error en el protocolo SOAP que es el protocolo que usan para comunicarse los servicios web. Al tener esta caracteristica se pueden enviar por el canal de comunicacion entre el servidor y el cliente. Y ademas nos ayudaran a enviar informacion personalizada gracias a queFaultExceptionTes una clase generica que puede transportar cualquier informacion siempre que esta sea un DTO Data Transfer Object.Para adaptar nuestro codigo vamos a crear dos DTOs que manejen las excepciones del servidor. Para que un objeto se convierta en transportable basta con anadir los atributos DataContract al objeto y DataMember a todas las propiedades que se quieran enviar. Como nota las propiedades decoradas con DataMember deben ser de lectura y escritura. Vamos a crear entonces nuestros nuevos objetosDataContractpublic class ArgumentFault   DataMember   public string Argument  get set DataMemberpublic string Message  get set DataContractpublic class InvalidOperationFaultDataMemberpublic string Message  get set Con ArgumentFault gestionaremos las excepciones tipo ArgumentException y con InvalidOperationFault las de InvalidOperationException. Entonces el codigo de nuestro servicio cambiara para gestionar excepciones de tipo FaultException que contengan los objetos Fault que hemos creado. Algo parecido a estoServiceBehaviorInstanceContextMode  InstanceContextMode.PerCallpublic class MyService  IMyService    public int Operationint a            if a  0                    var argumentFault  new ArgumentFault  Argument  a Message  Cannot be zero             throw new FaultExceptionArgumentFaultargumentFault            if a lt 0            var operationFault  new InvalidOperationFault  Message  The parameter must be greater than zero         throw new FaultExceptionltInvalidOperationFaultgtoperationFault        return aAdemas como estamos usando el contrato IMyService deberemos adaptarlo para que conozca las excepcionesfaltas que puede lanzar. Esto se hace usando el atributo FaultContract mediante el cual especificaremos los DTOs que pueden ser enviados dentro de una FaultExceptionServiceContractpublic interface IMyService    OperationContract    FaultContracttypeofArgumentFault    FaultContracttypeofInvalidOperationFault    int Operationint aCon estos pasos ya hemos adaptado nuestro servicio para que pueda enviar excepciones mas descriptivas que puedan ser recogidas mediante diferentes bloques catch. Asi que para terminar con nuestro desarrollo solo quedaria modificar el cliente de tal forma que sea capaz de reconocer las diferentes excepcionesfaltas que provengan del servidorpublic void SafeCallOperationint numero   try         var result  CallOperationnumero      System.Console.WriteLineEl resultado es    result      catchFaultExceptionArgumentFault aex         System.Console.WriteLineHas introducido 0    aex.Detail.Message      catchFaultExceptionInvalidOperationFault iex         System.Console.WriteLineHas introducido un numero menor de cero    iex.Detail.Message      catchException ex         System.Console.WriteLineError desconocido    ex.Message   Se pueden apreciar dos diferencias con respecto el codigo original. La primera es que ahora capturamos siempre excepciones de tipo FaultException. Y la segunda es que la informacion significativa que hemos creado la podemos encontrar en la propiedad Detail de la FaultException generica que hemos capturado.Al ejecutar de nuevo estas tres llamadasSafeCallOperation0SafeCallOperation1SafeCallOperation2Podremos ver que esta vez si que la salida es como estabamos esperando.Has introducido 0 Cannot be zeroHas introducido un numero menor de cero The parameter must be greater than zeroEl resultado es 2Asi que ya no hay excusas para no gestionar correctamente las excepciones en WCF. Podremos desarrollar mas esta solucion. Por ejemplo creando un objeto base para gestion de las diferentes excepciones o incluso desmarcarnos haciendo un serializador de excepciones generico para todas nuestras aplicaciones...Aqui solo exponemos el comportamiento basico y su por que. Explorarlo e implementarlo mejor corre a cargo del cada uno "
    } ,
  
    {
      "title"    : "Reactive Extensions (#codemotion)",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/03/27/reactive-extensions",
      "date"     : "2012-03-27 09:30:32Z",
      "content"  : "El pasado fin de semana quiqu3 y fernandoescolar estuvieron en Madrid en la codemotion en representacion de programando en .net. Ademas tuvieron el placer de impartir una pequena charla de 45 minutos acerca de las reactive extensions. Aprovechando este hecho nos gustaria cerrar de una forma mas o menos elegante la serie de articulos sobre rx aportando un indice de los articulos un breve resumen basado en la charla y varios programas de ejemplo incluida la red social de moda BeerToBeer.Reactive extensionsLas reactive extensions es el nombre que recibe una framework que nos ayudara a desarrollar de una forma novedosa y cercana al lenguaje natural. Fueron creadas por Eric Meijer el padre de Linq. Y basa su forma de trabajar en esta herramienta.Cuando hablamos de Linq Language Integrated Query nos referimos a una libreria de la framework .net de Microsoft que nos ayuda a operar con colecciones listas diccionarios... en resumen enumeracionesmediante un lenguaje de interfaces fluent.Cuando hablamos de interfaces fluent nombre acunado por el propio Eric Evans y Martin Fowler es hablar de un tipo de implementacion de la programacion orientada a objetos que nos ayuda a crear codigo fuente mas legible. Se basa en crear metodos dentro de objetos que nos devuelven al propio objeto que las usa. De esta forma podemos encadenar varias llamadas encadenadas por puntos. Pero mejor vamos a ver un ejemplopublic class FluentClass    private int valuepublic FluentClass WithValueint value  this.value  value return this public FluentClass AddOne  this.value   1 return this public FluentClass SubstractTwo  this.value  2 return this public FluentClass MultiplyThree  this.value  3 return this public FluentClass DrawValue  Console.WriteLinethis.value return this static void Mainstring argsvar fluent  new FluentClass.WithValue9.DrawValue.AddOne.DrawValue.SubstractTwo.DrawValue.MultiplyThree.DrawValueConsole.ReadLineComo se puede observar en este ejemplo cada funcion del objeto fluent despues de operar devuelve el propio objeto. Esto unido con nombre de funcion muy descriptivos hacen que podamos crear una serie de llamadas encadenadas cuyo significado es muy legible.Una vez hemos visto esto podemos entonces acercarnos a como Linq usando esta forma de programar nos ayuda a realizar busquedas filtrados ordenaciones y demas operaciones con conjuntos de valores en .netstatic void Main    var lista  new Listint 3 4 5 78 89 98 123 2 3var listaFiltrada  lista                        .OrderByDescendingx gt x  ordena de forma inversa                        .Wherex gt x lt 50  filtra los valores menores de 50                        .Take2  coge los dos primeros este codigo dibujara en pantalla un 5 y un 4foreach var item in listaFiltrada    Console.WriteLineitemConsole.ReadLineEn este otro ejemplo podemos ver como aplicamos algoritmos de ordenacion de filtrado y seleccion usando este Linq.Y asi vamos llegando hasta el contexto actual de aplicacionLo que nos podemos encontrar actualmente son aplicaciones distribuidas. Pongamos el ejemplo de twitter donde encontramos unos servicios que estan alojados en la nube y una serie de clientes diferentes segun sean plataformas moviles paginas web o aplicaciones de escritorio.Todas estas aplicaciones usan los mismos servicios que son quienes manejan los datos. Para comunicarse con ellos se utiliza un tipo de comunicacion asincrona con lo que conseguimos que nuestra aplicacion no se quede congelada esperando una respuesta del servidor. Y para gestionar la interaccion con el usuario se utilizan los eventos.Es decir nuestras aplicaciones convierten eventos en llamadas a servicios. Pero la forma de manejar unos y otros es muy diferentes y es justo en este punto en el que aparecen las reactive extensions y encontramos su objetivo unificar la forma de gestionar eventos y las llamadas a servicios.Para conseguir esta compleja tarea se recurre a la programacion reactiva. Un nuevo paradigma de programacion que podemos decir que se diferencia de la programacion tradicional en que es un sistema Push y no Pull. Es decir el codigo en la forma de programacion tradicional se maneja bajo demanda cada vez que escribimos una linea y en la programacion reactiva se evalua codigo cuando un acontecimiento que tiene relacion con el ocurre como las notificaciones Push del movil.Y es aqui cuando llegamos a una nueva problematica la de programar de forma reactiva usando un lenguaje secuencial. La solucion nos la aporta esta funcion que definen las reactive extensions y que nos brindan desde los Microsoft Live LabsPara poder explicar esta formula de una forma cercana a la programacion hemos creado una aplicacion BeerToBeer que ya mostramos en la codemotion. Una red social tipo twitter en la que puedes enviar mensajes a proposito de la cerveza que estas bebiendo en ese momento.El ejemplo esta compuesto de una serie de servicios WCF con netTcp http y REST y unos clientes para tres plataformas diferentes HTML 5 Windows Phone 7 y Windows WPF.Descargas de materialEntre los materiales de la charla se encuentra la presentacion que mostramos y la aplicacion de ejemplo BeerToBeer con el codigo de servicios y los clientes. Ademas tambien podreis encontrar otros ejemplos que no mostramos como un EventAggregator reactivo el tipico ejemplo de Drag n Drop o un mini cliente twitter.Descargar el ejemploLa presentacion que mostramosY si crees que las reactive extensions pueden serte de utilidad en tu trabajo o simplemente quieres saber mas sobre las mismas no olvides que en esta misma web tenemos una serie de cinco articulos donde se trata a fondo esta framework.Indice de articulosQue son las reactive extensionsObservables los sujetosLinq crear objetos observablesLinq operaciones con observablesSchedulers y Linq2Events"
    } ,
  
    {
      "title"    : "Rx V &#8211; Schedulers y Linq2Events",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/03/26/rx-v-schedulers-y-linq2events",
      "date"     : "2012-03-26 15:30:57Z",
      "content"  : "Hoy os vamos a proponer el ultimo articulo teorico acerca de las reactive extensions. Una vez hemos definido las rx sabemos que son los sujetos y las operaciones de creacion y Linq ya podemos hablar de el ultimo parametro de la formula que definimos en su dia los Schedulers.Es muy normal en el contexto actual de aplicaciones hablar de hilos de ejecucion Thread de las Task Parallel Library o de sincronismoasincronismo. Y es aqui donde vamos a encontrar la utilidad de los schedulers de las reactive extensionsSchedulersLa traduccion al castellano de esta palabra inglesa podria significar programadores o planificadores. Su funcion en este contexto es decidir donde se van a ejecutar las operaciones. En Rx diferenciamos dos tipos de operacionesCuando Observamos OnObserve Esta es la forma de referirnos al momento el que llega una iteracion o notificacion OnNext.Cuando nos Suscribimos OnSubscribe Aqui nos referimos al momento en el que se ejecuta el proceso de suscripcion Subscribe.Para entender la diferencia entre estas dos operaciones vamos a crear una sentencia que nos lea cada una de las lineas de un ficherovar filePath  cmifichero.txtvar observableLines  Observable.Createstringobserver     using var reader  new StreamReaderfilePath            while reader.EndOfStream                    string line  reader.ReadLine            observer.OnNextline            observer.OnCompleted return  gt  Como podemos ver hemos creado un IObservable de string en el que abriremos un StreamReader que apunta a un fichero de nuestro ordenador. Entonces empezamos a leer linea a linea y enviamos notivicaciones mediante un sujeto. Si ahora quisieramos suscribirnos a este observable para que nos escriba en pantalla todas las lineasobservableLines.Subscribeline  Console.WriteLineEncontraremos que al ejecutar esta linea de codigo el programa se queda congelado en ella hasta que no termina de leer todo el archivo. Esto se debe a que estamos leyendo de forma sincrona en el momento de suscribirnos. Para solucionarlo tendriamos que ejecutar esta operacion en otro contexto como por ejemplo un nuevo thread hilo de ejecucionobservableLines   .SubscribeOnScheduler.NewThread   .Subscribeline  Console.WriteLineAl ejecutar este codigo nos daremos cuenta de que la operacion de leer el archivo se realiza en un nuevo hilo y esto hace que nuestro programa no se quede a la espera de que se termine de leerse hasta el final. Pero si estuvieramos en un programa WPF y en lugar de quererlo escribir por consola lo anadieramos a un control tipo TextBox nos encontrariamos con un nuevo problema.Cuando ejecutamos una aplicacion tipo WPF o WinForms las ventanas y controles son dibujadas por un hilo especial que se encarga de todo lo grafico. La consecuencia de esto es que no se puede modificar un control grafico desde otro hilo diferente a ese. Por lo que si ejecutamos este codigoobservableLines   .SubscribeOnScheduler.NewThread   .Subscribeline  txtContent.Text   lineEn cuanto se lea desde el nuevo hilo la primera linea y la intente anadir a la propiedad Text de nuestro control de Texto nos saltara una excepcion debido a que es una operacion invalida. Este momento en el que se evalua cada una de las iteraciones es lo que antes hemos definido como cuando observamos. Para ello podriamos indicarle a nuestra sentencia que la operacion de observar se ejecute en el contexto del hilo que trabaja con los graficos. En WPF a este hilo se le conoce como el DispatcherobservableLines   .ObserveOnDispatcher   .SubscribeOnScheduler.NewThread   .Subscribeline  txtContent.Text   lineAsi podremos conseguir que nuestro codigo ejecute de forma asincrona la lectura del fichero pero que anada cada una de las lineas usando el hilo de ejecucion de los graficos.Pero no solo existen dos contextos para poder planificar las operaciones de rx. Encontraremos en dependencia de la plataformaDispatcher El hilo de ejecucion principal de las aplicaciones WPF y Silverlight ObserveOnDispatcher SubscribeOnDispatcher.NewThread Nuevo hilo de ejecucion ObserveOnSchedulers.NewThread SubscribeOnSchedulers.NewThread.TaskPool Nueva tarea de la TPL Task Parallel LibraryObserveOnSchedulers.TaskPool SubscribeOnSchedulers.TaskPoolThreadPool Encola la operacion en la ThreadPoolObserveOnSchedulers.ThreadPool SubscribeOnSchedulers. ThreadPool.CurrentThread La ejecutara tan pronto como sea posible en el thread actual ObserveOnSchedulers.CurrentThread SubscribeOnSchedulers.CurrentThread.Immediate Se ejecuta inmediatamente en el thread actualObserveOnSchedulers.Immediate SubscribeOnSchedulers.Immediate.Gracias a todos estos schedulers tendremos un control total de nuestro programa terminamos de explicar la formula de las reactive extensions y por tanto cerramos la explicacion general de esta framework. Pero practicamente todos los ejemplos que hemos visto hasta ahora van relacionados con logica interna o llamadas asincronas y servicios.Asi que ahora vamos a tratar la ultima caracteristica de las reactive extensions su interaccion con eventos.Linq To EventsLa primera vez que nos referimos a los sujetos hablamos de sus semejanzas con la gestion de eventos. Podriamos compararlos de la siguiente formaAmbos tienen su declaracion su suscripcion publicacion y la forma de dejar de escuchar los acontecimientos. Y es gracias a esta caracteristica que las reactive extensions nos proporcionaran una forma de gestionar los eventos con linq.Dentro de las extensiones de rx encontramos dos especificas para crear observables a partir de eventosFromEvent donde especificaremos el proceso de suscripcion evento   handler y desuscripcion evento  handler.FromEventPattern donde solo tendremos que especificar el objeto y el nombre del evento que queremos observar.Una vez podemos observar un evento obtendremos una gran facilidad para gestionarlo mediante sentencias linq. Caracteristica que cuando exponemos mediante un ejemplo queda muy clara.Buscando un tipo de aplicacion que se base en eventos nos hemos encontrado con un programa de dibujo en el que pulsando con el raton podemos dibujar lo que queramos. Asi que vamos a desarrollar un pequeno proyecto a tal fin.Nos hemos decidido por usar WPF como plataforma y el objetivo es crear un programa de dibujo en el que mientras pulsemos con el raton sobre el se ira dibujando en rojo la traza de nuestros movimientos de raton. Por lo que el primer paso que tendremos que dar es crear un nuevo proyecto WPF en nuestro Visual Studio.Automaticamente nos va a generar una ventana de inicio MainWindow sobre la que podremos trabajar. En WPF el control grafico que nos deja dibujar se denomina Canvas como en HTML5. Asi pues abriremos el archivo MainWindow.xaml y anadiremos un control de este tipoWindow xClassReactiveDraw.MainWindow        xmlnshttpschemas.microsoft.comwinfx2006xamlpresentation        xmlnsxhttpschemas.microsoft.comwinfx2006xaml        TitleMainWindow Height350 Width525    Grid        Canvas LoadedCanvas_Loaded BackgroundWhite     GridWindowA nuestro control le hemos capturado el evento Loaded para que una vez este cargado en pantalla se ejecute la funcion Canvas_Loaded en nuestro codebehind. Es por eso que el siguiente paso sera trabajar en esta funcion. Si abrimos el archivo MainWindow.cs podremos empezar a programar.Lo primero que deberiamos hacer en nuestra funcion es especificar el control con el que trabajamosprivate void Canvas_Loadedobject sender RoutedEventArgs e    var canvas  CanvassenderAhora crearemos eventos observables de nuestro raton segun se pulse el boton se deje de pulsar o se mueva por pantallaprivate void Canvas_Loadedobject sender RoutedEventArgs e    var canvas  Canvassender    var mouseDown  Observable                        .FromEventPatternMouseEventArgscanvas MouseLeftButtonDown    var mouseUp  Observable                        .FromEventPatternMouseEventArgscanvas MouseLeftButtonUp    var mouseMove  Observable                        .FromEventPatternMouseEventArgscanvas MouseMove                        .Selectev  ev.EventArgs.GetPositionthisComo podemos ver capturaremos el evento de presionar el boton izquierdo del raton MouseLeftButtonDown el de cuando lo levantamos MouseLeftButtonUp y para cuando movemos el raton MouseMove recogeremos la posicion del mismo.Ahora tendriamos que decirle a nuestro programa que desde que se pulsa el boton hasta que se deja de pulsar recoga el movimiento del ratonvar drawingPoints  from start in mouseDown                        from move in mouseMove.TakeUntilmouseUp                        select new Point                                                                            X  move.X                                        Y  move.Y                                    Creo que la sentencia habla por si sola...Por ultimo solo tendriamos que suscribirnos al observable resultante de la sentencia y hacer que dibuje por ejemplo un punto en pantalladrawingPoints    .Subscribepoint                                            var ellipse  new Ellipse                                                                                      Stroke  Brushes.Red                                             StrokeThickness  5                                                                canvas.Children.Addellipse                       Canvas.SetLeftellipse point.X                       Canvas.SetTopellipse point.Y                    Lo que hacemos es crear un circulo rojo de un tamano de 5 puntos lo anadimos a nuestro canvas y al final lo posicionamos.Al ejecutar nuestro programa veremos que nos dibuja correctamente pero que si movemos muy rapido el raton nos deja espacios. Para solucionar esto lo que vamos a dibujar en realidad son lineas de forma que si movemos muy rapido el puntero tendremos almacenado el ultimo punto y el nuevo dibujando una recta que los una.Para esto vamos a crear dos variablesvar isTheFirst  true  indica si es el primer punto de la trazavar lastPosition  new Point  almacena el punto anterior de la trazaCon el fin de poder gestionar diferentes trazas y no una linea continua cada vez que levantemos el botton izquierdo del raton resetearemos el valor de isTheFirstvar mouseUp  Observable                        .FromEventPatternMouseEventArgscanvas MouseLeftButtonUp                        .Do_  isTheFirst  trueY modificaremos la suscripcion del evento de tal manera que dibujemos lineas basandonos en los datos que almacenamos. El codigo resultante de la funcion seriaprivate void Canvas_Loadedobject sender RoutedEventArgs e    var canvas  Canvassender    var isTheFirst  true    var lastPosition  new Pointvar mouseDown  Observable                    .FromEventPatternltMouseEventArgsgtcanvas MouseLeftButtonDownvar mouseUp  Observable                    .FromEventPatternltMouseEventArgsgtcanvas MouseLeftButtonUp                    .Do_ gt isTheFirst  truevar mouseMove  Observable                    .FromEventPatternltMouseEventArgsgtcanvas MouseMove                    .Selectev gt ev.EventArgs.GetPositionthisvar drawingPoints  from start in mouseDown                    from move in mouseMove.TakeUntilmouseUp                    select new Point                                                                    X  move.X                                    Y  move.Y                                drawingPoints    .Subscribepoint gt                                          var ellipse  new Ellipse                                                                                      Stroke  Brushes.Red                                             StrokeThickness  5                                                                canvas.Children.Addellipse                       Canvas.SetLeftellipse point.X                       Canvas.SetTopellipse point.Y                            if isTheFirst                                                            isTheFirst  false                                lastPosition  point                                return                                                        canvas.Children.Addnew Line                                                                                                            Stroke  Brushes.Red                                                        X1  lastPosition.X                                                        X2  point.X                                                        Y1  lastPosition.Y                                                        Y2  point.Y                                                                                lastPosition  point                        Si volvemos a ejecutar nuestro programa de dibujo veremos que con un codigo muy cercano al lenguaje corriente hemos realizado una operacion que antes de conocer las reactive extensions hubiera sido un verdadero lio de funciones variables y demas.ConclusionesHasta este punto hemos podido explicaros casi todas las caracteristicas de las reactive extensions. Desde en que se basan pasando por sus funciones hasta como usarlas en diferentes contextos.El resumen de toda la informacion expuesta en estos ultimos meses es que las reactive extensions son una herramienta muy potente para el desarrollo. Se basa en un paradigma novedoso y util. Y su funcion es facilitar las operaciones que ya realizabamos usando un lenguaje mas cercano al que usamos para comunicarnos unos con otros.En unos dias y en referencia a este tema publicaremos el ultimo articulo en el que hablaremos de la codemotion haremos un resumen de todo lo que contamos alli crearemos un indice de los articulos y ademas anadiremos mucho codigo fuente con ejemplos de todo tipo para que podais ver las reactive extensions en un ambito de ejecucion.Permaneced atentos "
    } ,
  
    {
      "title"    : "Rx IV &#8211; Linq: operaciones con observables",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/02/15/rx-iv-linq-operaciones-con-observables",
      "date"     : "2012-02-15 10:15:08Z",
      "content"  : "Hemos llegado al cuarto articulo sobre reactive extensions en el que vamos a hablar de operaciones que se pueden realizar con observables. Pero como hace unos dias que no publicabamos nada al respecto vamos a hacer primero una pequena retrospectiva. Hasta ahora hemos tratado de explicar rx como una formula matematicaDentro de esta formula hemos intentado exponer que el componente basico son los sujetos observables y tambien las operaciones basicas de creacionlinq. En este articulo explicaremos mas a fondo el resto de operaciones y operadores que podemos encontrar.Pero antes de entrar en materia vamos a conocer unos diagramas que nos ayudaran a explicar y entender mas facilmente como se comportan las operacionesMarble diagramsCuando hablamos de reactive extensions es muy comun encontrarnos con unos diagramas que nos ayudan a entender las operaciones. En ellos se muestra la evolucion de la tarea de observacion con respecto el tiempo y tienen este formatoDentro de uno de estos diagramas podemos encontrarnos con 4 simbolos diferentesRepresenta una iteracion o un valor en una secuencia observable. IObserverT.OnNextTCuando sucede una transformacion en un valor.xSi una excepcion termina con la secuencia observable. IObserverT.OnErrorExceptionCuando una secuencia observable termina de forma correcta. IObserverT.OnCompletedUna vez tenemos claro como funciona podemos ponernos a explicar los operadores especialesMergeCuando hacemos un Merge de dos objetos observables estamos seleccionando todas las iteraciones OnNext de uno y otro operador.var z  x.MergeyEn este ejemplo el observable z terminara cuando se haya completado OnComplete tanto x como y o cuando una de ellas tenga un error OnError. Una vista del resultado en un diagrama seriaConcatOtra operacion es la concatenacion. Donde el resultado no sera exactamente el mismo que el de un merge.var z  x.ConcatyAl concatenar el observable x con y el resultado seran todas las iteraciones de x OnNext hasta que este se haya completado OnComplete y despues toda las que ocurran en el observabley. Las iteraciones que ocurran en y mientras x no ha terminado seran ignoradas. Y si ocurre un error OnError en x se terminara z con el error. Es un concepto un tanto complejo pero si echamos un vistazo a su diagrama se entiende rapidamenteEn un lenguaje mas vulgar se recogen la iteraciones de x y cuando termina las de y.CatchAl igual que cuando hablamos de un bloque try ... catch este operador recogera un error OnError que se produzca en el primer operandovar z  x.CatchyAl poner esta secuencia el resultado sera semejante a un Concat. Pero la diferencia es que la senal que se utiliza para cambiar deoperando es un error OnError. Podemos decir entonces que se recogen todas las iteraciones OnNext de x hasta que este falla y entonces se empiezan a recoger las de y.Si cualquiera de los dos operandos de la operacion terminan OnComplete el resultado sera que z tambien terminara.OnErrorResumeNextA la gente que ha programado en Visual basic le sonara este comando. Como su buen nombre indica lo que hara es que si ocurre un error se pase al siguiente y si no seguira con el proceso normal.var z  x.OnErrorResumeNextyEs decir realizara la misma operacion que un Concat pero si encuentra un error se comportara como un Catch. O dicho con otras palabras devuelve las iteraciones OnNext de x hasta que ocurre un error OnError o es completado OnComplete momento en el cual empezara a devolver las iteraciones de y.ZipLa operacion Zip puede resultar un poco especial. Podriamos traducirla al castellano como cremallera y la operacion que realiza se asemeja a lo que hace una cremallera al cerrarse. Si nos adentramos en este codigovar z  x.Zipy oneX oneY  oneX   oneYEl resultado esperara una iteracion en alguno de los dos objetos observables. Cuando ocurra esperara a que tenga una respuesta por parte del otro objeto observable. Es decir va emparejando las iteraciones OnNext de uno y otro observable aplicando la conversion que especifiquemosCombineLatestUna operacion que se asemeja con Zip es CombineLatest. Como su nombre indica combinara las ultimas iteraciones que se encuentra.var z  x.CombineLatesty oneX oneY  oneX   oneYCuando nos encontremos este tipo de codigo sabremos que cada vez que ocurre una iteracion OnNext en alguno de los observables la va a combinar con la ultima ocurrida en el otro observable usando para ello la funcion que le hemos pasado como parametroRepeatTeniendo en cuenta que un objeto observable de reactive extensions es ademas enumerable puede ser que necesitemos repetir la ultima secuencia que hemos estado observando. En este contexto podriamos usar un codigo semejante a estevar z  x.Repeat3Donde grabaremos en el objeto observable ztodas las iteraciones OnNext en orden que han ocurrido en x y las repetiremos tantas veces como le indiquemos en este caso 3. Graficamente se representaria asiRetryCuando lo que queremos es repetir la secuencia solo en el caso de que obtengamos un error OnError usaremos Retry.var z  x.Retry3Esta operacion volvera a repetir la secuencia de iteraciones si se produce un error. Y lo intentara tantas veces como le indiquemos. Por supuesto que si no obtiene ningun error no se repetira nunca la secuencia.JoinHablar del metodo Joines lo mismo que hablar de union entre dos objetos. Hasta este punto hemos visto dos formas de realizar uniones con objetos Zip y CombineLatest. La diferencia fundamental de esta funcion es que no solo combina parejas o los ultimos si no mas bien todas las posibles combinaciones hasta el momento de ocurrir.var z  x.Joiny v  x v  y oneX oneY  oneX   oneYEl formato en este caso es la union entre x e y mientras por un lado dure x v  x y por el otro dure y. Y la funcion de union es la especificada en el ultimo parametro.Con el codigo que acabamos de ver cada vez que ocurra una iteracion OnNext buscara todas las iteraciones que han ocurrido en el otro observador y combinara esta con todas las anteriores. Un concepto que queda mucho mas claro al ver el diagramaBufferUno de los metodos mas utiles dentro de las rx es el de Buffer que nos permite crear bloques de observacion en forma de listas genericas en dependencia del tiempo o de un numero de iteraciones. Por ejemplo imaginemos un escenario donde cada segundo recibimos una iteracion pero queremos agruparlas cada 3 segundos con el fin de que el usuario no vea refrescada constantemente la interfaz grafica. En este caso podriamos usar este codigovar z  x.BufferTimeSpan.FromSeconds3Y su resultado seria un objeto observable de este tipo IObservableIListT. O lo que es lo mismo cada iteracion de este nuevo objeto observable contendra una lista de los objetos que se han almacenado en forma de bufferComo vemos en el ejemplo anterior aunque en un segundo no se produzca una iteracion se guardara un buffer con respecto al tiempo. Si lo que deseamos es un resultado semejante a esteLo que tendremos que usar es tambien la funcion Buffer pero esta vez con respecto a un contador y no al tiempovar z  x.Buffer3WindowMuchos encontraran que Window y Buffertienen un formato semejante. En el caso de Buffercreamos una secuencia de paquetes en forma de lista generica. Sin embargo cuando usamosWindow en lugar de listas empaquetamos en objetos observables.var z  x.Window3En este caso el diagrama de canicas seria algo asiPor lo que el tipo de datos que devuelve esta funcion es IObservableIObservableT. Y podemos usarlo con contador o con respecto el tiempo tambienvar k  x.WindowTimeSpan.FromSeconds3SwitchPara terminar de describir metodos nuevos de las reactive extensions hablaremos de Switch que es complementario a Window ya que realiza la operacion contraria. Es decir convierte un objeto IObservableIObservableT en un simple IObservableT.IObservableint x ... var k  x.WindowTimeSpan.FromSeconds5var w  k.SwitchEn este codigo hariamos que el objeto observablex fuera exactamente igual a w. Podriamos decir entonces que Switch es el equivalente al deshacer de un comando Window.Aplicando RxLa mejor de las virtudes de las reactive extensions es su versatilidad. Puede aplicarse en muy diferentes escenarios y solo hay que dejar volar un poco la imaginacion para darnos cuenta de que podemos usarlas para problemas comunes del dia a dia. Por ejemplo hace unos meses Pablo Nunez pablonete propuso via twitter un ejercicio simple de Linq que seguro nos hubiera ocupado unas cuantas lineas y gracias a rx podemos reducirlo a unaEjercicio LINQ Trocear una lista  a 1 b 2 ... en sublistas   a 1   b 2 ......La solucion que podriamos deducir con lo estudiado hasta hora podria servar list  new Liststring  1 a 2 b 3 c 4 d ... var result  new ListListstringlist.ToObservable.Buffer2.ObserveOnScheduler.Immediate.Subscriberesult.AddEsto no quiere decir que esta sea la mejor solucion solo una mas. Lo que queremos dejar claro en este articulo es que reactive extensions son unas nuevas herramientas aplicables a la mayor parte de los escenarios que manejamos hoy en dia. Nos aportan nuevas operaciones y por lo tanto mas versatilidad al lenguaje y la plataforma de desarrollo que usamos.Antes de terminar quisiera instar al lector a experimentar mas metodos dentro de las extensiones de rx. Ya que aunque en este articulo hemos tratado muchos de ellos no son los unicos. Y solo conociendolos descubriremos todas sus aplicaciones.Por hoy me despido y os invito a seguir esta linea de articulos y mas aun el siguiente donde de verdad la cosa se pone entretenida con la ultima variable de la funcion que nos definen las reactive extensions Schedulers."
    } ,
  
    {
      "title"    : "Sobre WCF, DTO, EF, POCO y los principios de la programación",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/01/22/sobre-wcf-dto-ef-poco-y-los-principios-de-la-programacion",
      "date"     : "2012-01-22 18:45:36Z",
      "content"  : "Creo que este es el primer articulo que vamos a ver en programandonet a modo editorial. Esto es el resultado de una conversacion en el grupo de arquitectos .netde LinkedIn en la que se hablaba sobre usar POCOs de entity framework como DTO para servicios WCF que se inicio hace unos dias. Y es que no me hubiera gustado que algunas conclusiones y escritos se perdieran en este grupo y por eso hemos creido interesante publicarlas aqui para tenerlas a mano.Para aquellas personas que no esten familiarizadas con estas siglas haremos un breve glosarioPOCO Son las siglas de Plain Old C Object y se refieren a clases simples que no dependen de ninguna framework. Es un termino derivado del concepto del mundo Java POJO.EF Entity Framework es un ORM disenado por Microsoft sirve para convertir en un lenguaje orientado a objetos como c una base de datos relacional. En sus ultimas versiones permite el uso de POCOs como entidades de las bases de datos.DTO Se refiere a Data Transfer Objects y es un objeto que por definicion se envia y recibe dentro de un servicio. Nuestro companero pbousan hace referencia a este articulo de Martin Fowler donde explica el concepto.WCF Es la abreviacion de Windows Communication Foundation que es una API de Microsoft para crear aplicaciones orientadas a los servicios SOA. De lo que deducimos que los objetos que se envian o reciben en estos servicios son DTOs.Si retomamos la cuestion inicial la primera conclusion que desarrollamos fue que si usas objetos POCO de Entity Framework como DTO le estas dando dos responsabilidades a tu objeto. Por un lado sirve de entidad del ORM y por otro de transferencia de WCF. Por lo que estarias incumpliendo la S de los principios SOLID Single responsibility principle SRP. Por lo tanto la respuesta es que no deberiamos hacerlo.Ademas al darle estas dos responsabilidades a un objeto obligas a que tus servicios envien toda la informacion que almacenas en la base de datos y quiza solo hace falta una parte de esa informacion. Entonces corres el riesgo de enviar demasiada informacion con un servicio o demasiada poca haciendo necesario llamar a dos metodos diferentes. Es decir un alto riesgo de crear servicios poco eficientes.Pero existe el contexto en el que los datos almacenados sean exactamente los mismos que queremos enviar en nuestros servicios. En este sentido jc_quijano defendio una postura que se amparaba en el principio de la programacion KISS Keep It Simple Stupid. Es decir es mas rapido y facil desarrollar un POCO para EF y reutilizarlo como DTO para nuestros servicios. Ademas si se anaden DTOs a este concepto estamos anadiendo una capa nueva de complejidad a la aplicacion. Y como no es la estructura de los objetos lo que me definen su responsabilidad si no el uso que le doy a esos objetos estructuralmente tampoco estoy rompiendo ningun principio SOLID.Esta filosofia me parece correcta a la hora de desarrollar aplicaciones con un ciclo de vida muy corto. Como puede ser una herramienta que ayuda a corregir un problema puntual y que una vez corregido podemos deshacernos de esta. De esa forma no tendria un mantenimiento largo y no se convertiria en un problema el acoplamiento que crea usar un objeto para dos tareas diferentes.Y es que KISS no es un principio mas importante que el SRP o que cualquiera de SOLID. Ni un principio de SOLID es mas importante que cualquier otro principio incluido KISS. Los principios SOLID te llevan a programar mejor e indirectamente a empezar a seguir otros principios como KISS o YAGNI. Mi opinion y recomendacion es seguirlos todos.El principio KISS no debe confundirse con programar poco si no que lo que programes hacerlo simple. No anadir 20 capas a una solucion que con 2 va de sobra. Pero tampoco hacer una clase maestra que lleve todo el peso de la aplicacion y que tenga 100 propiedades.Vamos a proponer un ejercicio de imaginacion para desarrollar un ejemploSi tenemos nuestro proyecto en orden como fan confeso de TDD con los test unitarios correspondientes podemos ser capaces de medir facilmente la complejidad de la solucion. Estara determinada por el numero de tests unitarios que habra que modificar en el momento en el que se produzca un cambio.Imaginemos un escenario en el que por alguna razon nuestro servicio tiene que cambiar a consecuencia de una nueva especificacion de nuestro product owner. Algo que creemos que es comun en los proyectos con un ciclo de vida mediolargo.Si tenemos la solucion que usa objetos DTO especificos este cambio podria implicar modificar un test unitario y a su vez anadir una propiedad a nuestro DTO. Pero si nuestra solucion no sigue el SRP es decir que usamos los objetos POCO tambien como DTOs el escenario es diferente. Cambiaremos un test despues el DTOPOCO y despues nos fallarian un par de tests unitarios por decir un numero que estan relacionados con la forma de almacenar los datos. Y para terminar tendriamos que anadir sin que tenga por que ser necesario un campo mas a nuestra base de datos. Ahora imaginemos las consecuencias de este cambio multiplicada por el numero de cambios que suceden en una aplicacion cualquiera que no tenga un corto ciclo de vida claro D.La conclusion que podemos sacar de todo esto es que el codigo que ha resultado mas facil de seguir para el resto de los programadores mas mantenible y en consecuencia mas simple ha sido el desacoplado con sus DTOs por un lado y su EF por otro. Tener una buena estructura es programar de forma simple. Si cada objeto hace lo que debe hacer no es dificil entender una aplicacion. Si las clases empiezan a tener mas de una responsabilidad podremos encontrar resultados inesperados cuando hacemos un cambio.El principio de responsabilidad unica nos ha llevado a hacer un codigo mas simple aunque hayamos que tenido que programar mas lineas de codigo. Por lo que personalmente prefiero seguir todos los principios y hacer aplicaciones que tengan objetos POCO por un lado y DTO por otro.Y por si seguir los principios de la programacion no parece un argumento lo suficiente solido para argumentar separar estas dos responsabilidades en dos objetos una reflexion personal No creo que un ORM sea la solucion que deba ser visible si no mas bien una pasarela porqueSi generas una serie de objetos a partir de una base de datos estos no son la abstraccion mas correcta ya que la base de datos utiliza unas herramientas totalmente diferentes a nuestra Framework de programacion y se basa en otras premisas para resolver los problemas. No es lo mismo normalizar que crear un buen dominio. Y es mas una buena base de datos normalizada seguro que no es el mejor dominio de una aplicacion.Si generas la base de datos a partir de tu dominio con EFCodeFirst por ejemplo la base de datos no sera la mejor ya que de nuevo la abstraccion no es la mas correcta. Y si creas un buen dominio al convertirlo a base de datos lo mas probable es que no este normalizada en absoluto.De estas dos premisas la deduccion que he sacado es que el acceso a datos tiene que ser una parte aislada del resto de la aplicacion. Esta en una capa de nuestra arquitectura llamada persistencia. Y tendremos abstrayendonos de este ORM a un modelo de dominio donde aplicaremos normas de negocio yo de servicios DTO para comunicarselo al mundo. De esta forma cada parte de la aplicacion hara lo que debe de la forma mas eficiente para su cometido...Eso si como arquitectos debemos encontrar la solucion que mas se adecua al escenario de nuestro proyecto. No sobredimensionar un programa que nos sirve para calcular numeros primos con 5 capas. Pero tampoco crear una aplicacion que va a ser mantenidaexpandida durante 20 anos y este totalmente acoplada.Ahora bien todo esto es una opinion que puede ser modificada por unos argumentos que consideremos mejoresQue opinais vosotrosPodeis seguirla conversacion en LinkedIna ver en que termina derivando ya que creo que es muy interesante..."
    } ,
  
    {
      "title"    : "Rx III &#8211; Linq: crear objetos observables",
      "category" : "",
      "tags"     : "",
      "url"      : "/2012/01/02/rx-iii-linq-crear-objetos-observables",
      "date"     : "2012-01-02 12:30:10Z",
      "content"  : "Ano nuevo articulo nuevo. Despues de dos articulos cargados de conceptos teoricos y pruebas ha llegado el momento de empezar a sacarle partido de verdad a la reactive framework. Hasta ahora hemos visto cuales son los principios en los que se fundamenta Rx los objetos que vamos a tener que utilizar los sujetos e incluso introdujimos el uso de una clase llamada Observable que contiene metodos y extensiones para que todo esto sea mas sencillo.Si mediante el navegador de objetos o cualquier otra herramienta de estudio de ensamblados abrimos el archivo System.Reactive.dll en el namespace System.Reactive.Linq encontramos una clase estatica cargada de metodos y extensiones llamada Observable. Dentro de este objeto se encuentra definido el segundo parametro de la formula que define las reactive extensions Linq.Para empezar Observable contiene extensiones con si no todos la mayoria de los operadores que ya conocemos de Linq. Pero en este caso los encontraremos aplicados a objetos tipoIObservableT.var observable  new ReplaySubjectintobservable .OnNext1observable .OnNext2observable .OnNext3observable .OnCompletedComo pudimos ver en el anterior articulo tambien encontraremos metodos para crear estos objetosIObservableT. Podriamos poner un ejemplo rapido con la extension ToObservable metodo que sirve para crear un observable a partir de un IEnumerable como listas o colecciones. Para generar un codigo semejante al anterior deberiamos hacer algo asivar enumerable  new Listint  1 2 3 var observable  enumerable.ToObservableUsando cualquiera de los dos objetos que hemos creado podriamos aplicar funciones de Linq como Where o Sumobservable    .Wherei  i  3    .Sum    .SubscribeConsole.WriteLineCon este codigo estariamos filtrando las iteraciones menores de 3. Y ademas sumariamos sus valores 1 2  3. Para terminar escribimos en la consola el valor final.A estos operadores que podriamos definir como estandares de Linq se les anaden muchos otros propios de Rx. Para poder echar un vistazo a los mas importantes vamos a intentar dividirlos entre creadores y operadores. En este articulo trataremos los primeros ya que es un tema suficientemente extenso. Y los categorizaremos encreate rango tiempo asincrono y eventos.CreadoresComo metodos para crear objetos observables podemos encontrar las cuatro funciones que mencionamos en el articulo anteriorvar neverObservable  Observable.Neverint igual quevar s  new SubjectintLa funcion Never genera un observable en el que nunca hay una iteracion ni termina y se diferencia de Empty en que este ultimo si que termina la tareavar emptyObservable  Observable.Emptyint igual quevar s  new Subjectints.OnCompleteCuando usemos Return tendra lugar la iteracion que digamos y se terminara el procesovar returnObservable  Observable.Return1 igual quevar s  new ReplaySubjectints.OnNext1s.OnCompleteY dentro de estos cuatro metodos simples el ultimo es Throw que lanzara un error que le especificamosvar throwObservable  Observable.Throwintnew Exception igual quevar s  new ReplaySubjectints.OnErrornew ExceptionCreateLa funcion por excelencia para crear cualquier tipo de objeto observable es Create. Tiene tanta versatilidad que el resto de creadores se pueden implementar con mayor o menor dificultad usando esto. Un ejemplo claro seria el de la enumeracion que se convierte en observable un sujetovar subject  new ReplaySubjectintsubject.OnNext1subject.OnNext2subject.OnCompleted lo mismo que una enumeracion a observablevar enumerable  new Listint  1 2 var observableEnumeration  enumerable.ToObservable y tambien podemos hacerlo con Createvar created  Observable.Createintobservable observable.OnNext1observable.OnNext2observable.OnCompletedreturn  gt  preComo podemos ver cuando utilizamos Create se nos solicita una funcion que como parametro de entrada usa un IObserverT que no es mas que la parte observadora de un sujeto y tiene que devolver una accion. En este caso devolvemos una accion vacia pero lo ideal seria devolver una accion que realizara la tarea de desuscripcion. Para ello se nos provee del objeto Disposable.RangoOtra forma de crear este tipo de objetos observables es especificando un rango. Para esta tarea la funcion mas simple que encontraremos es Range. En este ejemplo crearemos iteraciones del 0 al 9var rangeObservable  Observable.Range0 10Como deciamos anteriormente podriamos crear este mismo objeto usando el metodo Createvar createObservable  Observable.Createintobservable for var i  0 i  10 i  observable.OnNextiobservable.OnCompletedreturn  gt  O tambien podriamos usar otra funcion como Generatevar forObservable  Observable.Generate0 i  i  10 i  i   1 i  iGenerate es una funcion muy parecida a un bucle for. El primer parametro es el valor inicial el segundo una expresion que siempre que ocurra se ira al siguiente paso. La tercera es el metodo que se realiza en cada uno de los pasos y por ultimo usaremos una expresion para devolver el objeto que itera. Lo podriamos traducir asi Generate0  i  i  10 i  i   1 i  iforvar i  0 i  10 i  i   1   yield return iTiempoTambien podemos crear observables dependiendo del tiempo. Una forma basica que nos crearia un observable con iteraciones cada una unidad especifica de tiempo es Intervalvar timeObserver  Observable.IntervalTimeSpan.FromSeconds1Con este codigo crearemos una iteracion cada un segundo. Ademas esta iteracion sera incremental empezando desde 0 cada vez que ocurra se le sumara uno.Otra posibilidad es usar Timer que a las personas que hayan trabajado con el Timer de System.Threading se les hara familiar. Aqui podremos asignar el momento en el que queremos que empiece ademas de su intervalovar timerObserver  Observable.TimerTimeSpan.FromMinutes1 TimeSpan.FromSeconds1Aqui creariamos un observable que empezara a iterar dentro de un minuto. Y a partir de entonces cada segundo tendra lugar una nueva iteracion.AsincronoLas reactive extensions traen una gran variedad de creadores de observables asincronos. Por ejemplo ToAsync es una funcion que convertira cualquier codigo en asincrono y observablevar asyncObservable  Observable.ToAsyncstring intthis.FuncionQueTardaMuchoparametro de entradaComo vemos podremos podemos incluso llamar a funciones con parametros.Otra forma de crear observables asincronos sera llamando a funciones que ya son asincronas. Un ejemplo podria ser recogiendo la respuesta de una peticion web var asyncObservable  Observable.FromAsyncPatternWebResponserequest.BeginGetResponse request.EndGetResponseEn este codigo llamaremos a la funcion de GetResponse asincrona y recibiremos un observable de WebResponse que sera la respuesta de la peticion de la web.La forma mas simple de crear observables asincronos es el uso de la funcion Startvar startObservable  Observable.Start  ProcedimientoQueTardaMuchoY para terminar con este apartado podriamos recoger tareas del TPL Task Parallel Library y convertirlas en observables asincronos. Siempre y cuando usemos el namespace System.Reactive.Threading.Tasksvar parallelObservable  Task.Factory.StartNew  ProcedimientoQueTardaMucho.ToObservableEventosComo ultima subcategoria dentro de los creadores de observables encontrariamos crearlos a partir de eventos. Es decir se adjuntanattachan a un evento y devuelven sus publicaciones en forma de observables. Para esta tarea tendremos la funcion FromEventPatternvar clicks  Observable.FromEventPatternRoutedEventHandler RoutedEventArgs                        routedHandler  MyButton.Click   routedHandler                        routedHandler  MyButton.Click  routedHandlerEl codigo anterior nos mostraria como recoger las pulsaciones de un boton en WPF o Silverlight mediante Rx. La versatilidad de estas seria poder operar con los clicks. Por ejemplo podriamos hacer que en cada pulsacion de las 10 primeras se incremente en uno el contenidoint counter  0clicks.Take10.Subscribee                                     counter                      MyButton.Content  counter.ToString                La mayor parte de estas funciones para crear observables tienen sobrecargas que nos aportaran mayor funcionalidad. Ademas no son las unicas herramientas de las que disponemos aunque si probablemente las mas significativas. Como siempre a partir de aqui os invitamos a que exploreis que otros metodos de creacion existen o incluso a ensayar con la funcion Create para encontrar diferentes resultados.En el siguiente articulo seguiremos explicando los temas referentes a Linq y trataremos los operadores propios de reactive extensions.Ademasconoceremos los Marble diagramsasi que esperamos que no os lo perdais."
    } ,
  
    {
      "title"    : "Rx II &#8211; Observables: los sujetos",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/12/19/rx-ii-observables-los-sujetos",
      "date"     : "2011-12-19 10:00:06Z",
      "content"  : "Hace unos dias empezamos a hablar de las Reactive eXtensions. Estudiamos su contexto y las bases teoricas en las que se fundamentan. Ademas expusimos laformulacreada por los Microsoft Live Labs para definirla. Y terminamos el articulo comentando que resolveriamos esta formula en futuras publicaciones. Hoy es el dia de resolver el primer parametro Observables.Pero antes vamos a ver como podemos instalar y referenciar las Rx en nuestro proyecto.Instalando RxExisten dos formas basicas de distribucion de las extensiones reactivas. La primera es desde la propia web de microsoft medianteeste enlace. Aqui te podras descargar un paquete con los ensamblados principales ademas de los especificos para cada distribucion.La segunda opcion es utilizar nuget desde donde podras descargarte los diferentes ensamblados en su version estable o en la experimental haciendo una busqueda de rx en su interfaz de gestion de paquetesDentro del ambito de este articulo bastara con crear un proyecto nuevo de consola e instalar desde nuget el paquete RxMain. O si ya descargamos todos los ensamblados deberiamos anadir la referencia a System.Reactive.Una vez tenemos esto ya podemos remangarnos y empezar a trabajarObservablesCuando decimos observables en la formula que define las reactive extensions nos referimos a los objetos del patron observable Observador y Observable. Y dentro de la framework 4.0 en el namespace System podremos encontrar los contratos que nos exponen su comportamientoPor lo tanto ya tenemos las normas para empezar a trabajar. Como prueba vamos a crear una clase base generica que implemente la interfaz IObservablepublic class ObservableT  IObservableT   private readonly IListIObserverT observersprotected Observablethis.observers  new ListIObserverTpublic IDisposable SubscribeIObserverT observerif observers.Containsobserverobservers.AddobserverCon el fin de implementar la interfaz IObservable hemos creado el metodo de suscripcion del contrato. Ahi almacenaremos en una lista privada de observadores a todos los que se han suscrito a nuestro objeto observable. Pero para poder completar esta clase deberiamos ser capaces de enviar notificaciones a nuestros observadores. Y que notificaciones esperara un observador Si nos detenemos a estudiar la interfaz de IObserver veremos que un observador estara atento a 3 tipos de sucesosOnComplete que se lanzara cuando se termine de observarOnError que se lanzara cuando ocurra un error mientras se observaOnNext que nos avisa de que haya ocurrido algunotro acontecimiento dentro de la tarea de observacionPor lo que sabiendo esto podemos anadir una serie de metodos en nuestra clase Observable para que ya tenga implementada la notificacionllamada a todos los observadores suscritospublic void OnNextT value   foreachvar observer in observers         observer.OnNextvalue   public void OnErrorException errorforeachvar observer in observersobserver.OnErrorerrorpublic void OnCompletedforeachvar observer in observersobserver.OnCompletedUna vez la hemos completado nos damos cuenta de que los metodos que hemos creado satisfacen el contrato de la interfazIObserver. Por lo que podriamos estar hablando de que para poder implementar estas interfaces necesitaremos un objeto que indirectamente sea a la vezIObservableeIObserver.Dentro de reactive extensionsse ha creado una interfaz que nos define este nuevo contratoISubject.public interface ISubjectT  ISubjectT Tpublic interface ISubjectT S  IObservableT IObserverSY ademas encontraremos de serie cuatro implementaciones base de esta interfazSubjectT la implementacion baseReplaySubjectT recuerda todas las publicaciones para cualquier subscriptorBehaviorSubjectT recuerda solo la ultima publicacion para los subscriptoresAsyncSubjectT es asincrono que publica solo el ultimo valor una vez se ha completadoSujetosDentro de todas las traducciones que tieneSubjectsen nuestro idioma es probable que la que se nos haga mas simple sea sujetos. Como deciamos un sujeto sera un objeto observable que con el fin de poder notificar de una forma simple a los observadores implementara tambien la interfaz de IObserver.La funcionalidad de un sujeto se puede resumir como que puede notificar a sus observadores una serie de acontecimientos en forma de iteracionISubjectint subjectIObserverint observersubject.Subscribeobserver  se suscribe un observadorsubject.OnNext1  notificamos la iteracion 1subject.OnNext2  notificamos la iteracion 2 subject.OnErrornew Exception  podemos interrumpir la iteracion con una excepcionsubject.OnNext3  notificamos la iteracion 3subject.OnCompleted  notificamos que ya no hay mas iteracionesPero ademas Rx nos va a proveer de una serie de extensiones para facilitarnos la suscripcion que nos van a ayudar a crear dinamicamente observadores Type System.ObservableExtensions Assembly System.Reactive Version1.0.10621.0 Cultureneutral PublicKeyToken31bf3856ad364e35 Assembly location System.Reactive.dllpublic static class ObservableExtensionspublic static IDisposable SubscribeTSourcethis IObservableTSource sourcepublic static IDisposable SubscribeltTSourcegtthis IObservableltTSourcegt source ActionltTSourcegt onNextpublic static IDisposable SubscribeltTSourcegtthis IObservableltTSourcegt source ActionltTSourcegt onNext ActionltExceptiongt onErrorpublic static IDisposable SubscribeltTSourcegtthis IObservableltTSourcegt source ActionltTSourcegt onNext Action onCompletedpublic static IDisposable SubscribeltTSourcegtthis IObservableltTSourcegt source ActionltTSourcegt onNext ActionltExceptiongt onError Action onCompletedQue se podran explotar mediante expresiones lambdasubject.Subscribeiterator  Console.WriteLineIterator    iteratorsubject.Subscribeiterator  Console.WriteLineIterator    iterator  Console.WriteLineCompletesubject.Subscribeiterator  Console.WriteLineIterator    iteratorexception  Console.WriteLineException    exception.Message  Console.WriteLineCompleteComo vemos el comportamiento final de un sujeto observable resulta semejante en funcionalidad a lo que podria ser un evento. Partiendo de este codigoISubjectstring Changed new Subjectstring  un sujetoevent Actionstring Loaded  un eventoActionstring onNotified  s  Console.WriteLines  creamos nuestro handlerPor un lado podemos suscribirnos a un sujeto o adjuntarnos a un eventovar disposable  Changed.SubscribeonNotified  nos suscribimos al sujetoLoaded   onNotified  nos atachamos al eventoTambien podemos lanzar un evento o publicar en un sujetoChanged.OnNexthello  publicamosLoadedhello  lanzamos el eventoY para terminar podemos desuscribirnos para no volver a tener notificacionesdisposable.Dispose  eliminamos el resultado de la suscripcionLoaded  onNotified  nos desadjuntamos del eventoUna vez hemos aclarado su funcionamiento podemos ver el comportamiento de esos sujetos que encontramos en las reactive extensions.SubjectComo deciamos anteriormente un objeto SujectT es la implementacion base de un sujeto de Rx. Su funcionamiento es simple notifica de todas las iteraciones una vez nos hemos suscrito. Por ejemplo si hacemos estovar subject  new Subjectintsubject.OnNext1subject.Subscribeiterator  Console.WriteLineIterator    iteratorsubject.OnNext2subject.OnNext3Esperaremos que la salida seaIterator 2Iterator 3Es decir que no tiene ningun tipo de memoria simplemente se dedica a notificar a los suscriptores actuales las iteraciones que ocurren en ese momento.ReplaySubjectEl tipo ReplaySubjectT es un sujeto que recuerda las iteraciones para todos los suscriptoresvar replaySubject  new ReplaySubjectintreplaySubject.OnNext1replaySubject.OnNext2replaySubject.Subscribeiterator  Console.WriteLineIterator    iteratorreplaySubject.OnNext3En este codigo a pesar de suscribirnos una vez se han ejecutado dos iteraciones la salida por consola sera estaIterator 1Iterator 2Iterator 3BehaviorSubjectUn tipo de sujeto muy especial es el BehaviorSubjectT que tiene la propiedad de recordar solo la ultima iteracion acontecida por lo que si realizamos un ejemplo semejante al anteriorvar behaviorSubject  new BehaviorSubjectint0behaviorSubject.OnNext1behaviorSubject.OnNext2behaviorSubject.Subscribeiterator  Console.WriteLineIterator    iteratorbehaviorSubject.OnNext3La salida sera estaIterator 2Iterator 3Un detalle a tener en cuenta en este sujeto es que requiere de inicio un valor que sera la primera iteracion que recuerde. En el codigo anterior lo en el constructor le hemos pasado como parametro le valor 0 por lo que si nos llegamos a suscribir al principio hubieramos recibido la linea Iterator 0.AsyncSubjectEl ultimo de los sujetos que vienen con Rx es el AsyncSubjectT. Como su nombre indica este es un sujeto asincrono. Esto implica que no notifica nada hasta que no se ha completado OnComplete. Y una vez esto ocurre solo notifica del ultimo iterador. Si lanzamos este codigovar asyncSubject  new AsyncSubjectintasyncSubject.Subscribeiterator  Console.WriteLineIterator    iteratorasyncSubject.OnNext1asyncSubject.OnNext2asyncSubject.OnNext3No obtendremos ninguna informacion en la consola de salida. Pero en el momento que anadamos a este codigo la llamadaasyncSubject.OnCompletedObtendremos en la consola la notificacion de la ultima iteracion dentro del proceso totalIterator 3Podemos ver que es muy interesante el uso que se le puede dar a estos objetos y como algunos nos dan pistas sobre los diferentes escenarios donde aplicarlos. Pero lo que es verdaderamente util es utilizar las extensiones que nos ofrece Rx para crear estos sujetosCreando sujetosLa forma mas rapida de crear un sujeto es convertirlo desde un enumerable.public static IObservableTSource ToObservableTSourcethis IEnumerableTSource sourceUsando la extension ToObservable podremos convertir cualquier iteracion en un sujeto observable de reactive extensions y al suscribirnos podremos recibir la publicacion de cada una de las iteraciones.var subject  new ReplaySubjectintsubject.OnNext1subject.OnNext2subject.OnCompleted esto es lo mismo que el anterior sujetovar enumerable  new Listint  1 2 var observableEnumeration  enumerable.ToObservableEsta extension es tan solo la punta del iceberg de Rx. Existe un objeto llamado Observable que contiene extensiones y metodos que nos ayudaran a crear y operar con sujetos. Por ejemplovar neverObservable  Observable.Neverint  var s  new Subjectintvar emptyObservable  Observable.Emptyint  var s  new Subjectint s.OnCompletevar returnObservable  Observable.Return1  var s  new ReplaySubjectint s.OnNext1 s.OnCompletevar throwObservable  Observable.Throwintnew Exception  var s  new ReplaySubjectint s.OnErrornew ExceptionPero comentar todo lo que nos ofrece este objeto pertenece al siguiente articulo sobre reactive extensions que esperamos que leais dentro de unos dias."
    } ,
  
    {
      "title"    : "Rx I &#8211; Qué son las Reactive eXtensions",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/12/12/rx-i-que-son-las-reactive-extensions",
      "date"     : "2011-12-12 10:15:01Z",
      "content"  : "Dentro de las librerias de moda para .net la posiblemente mas dificil de explicar tiene el nombre de Reactive eXtensions o Rx. Una idea que nace en el contexto actual de aplicaciones distribuidas. Donde la informacion se encuentra en la nube y mediante llamadas asincronas podemos procesarla usando clientes que interaccionan con el usuario basandose en eventos.Encontramos ejemplos de esto en cualquiera de las aplicaciones mas comunes que usamos. Por ejemplo el cliente de Whatsapp de nuestro telefono movil el TweetDeck para ver el twitter o el widget del escritorio que nos informa del tiempo.Todas estas aplicaciones recogen datos de una serie de servicios que se encuentran en internet. Y mediante la interaccion con el usuario los tratan. Ademas como en la mayor parte de las tecnologias y APIs para interfaces graficas para entender que quiere hacer el usuario se basan en eventos como un onClick del raton sobre un boton concreto.Entonces llegamos al igual que un fisico en busca de una teoria unificada al origen de las reactive extensions la creacion de un lenguaje comun para gestionar llamadas asincronas a un servicio y los eventos que ocurren en la interfaz grafica.Es posible que esto que estamos hablando a muchos os suene ya que gracias a Second Nug hicimos un WebCast sobre el tema hace unos meses. A lo largo de la serie de articulos con la que comenzamos hoy intentaremos extender esa charla ademas de abordar los diferentes temas con mas calma y perspectiva.Programacion reactivaSi estudiamos detenidamente el nombre de la libreria que nos ocupa encontramos dos palabrasReactive que hace referencia a la programacion reactiva.Extensions implica que son extensiones para la framework de .net.Reactive programming es un paradigma de programacion basado en los flujos de datos y la propagacion de los cambios. Esto significa que deberiamos poder crear flujos de datos con facilidad usando el propio lenguaje de programacion y nuestro propio entorno de ejecucion deberia ser capaz de propagarse a traves de los cambios que se producen en estos flujos de datos.Ahora paremonos un momento a coger aire... Vamos a ver un ejemplo practico para entender todo esto imaginemos un contexto de programacion secuencial tradicional.int a b cb  10c  2a  b   c  a  12c  4  a  12b  3  a  12Como vemos una vez asignamos el valor de la variable a como una suma de b mas c se evalua esa suma. Y aunque cambiemos el valor de b o c el valor de a ya ha sido evaluado y sigue siendo el mismo que la primera vez 12.La programacion reactiva propone un comportamiento diferente. Algo parecido a una tabla de excel donde nosotros asignamos a la celda A1 para que calcule el valor de B1 mas C1. Aqui si cambiamos el valor de la columna B1 o C1 el valor de A1 se actualiza evalua automaticamente.Ahora ya podemos decir queRx es ese conjunto de herramientas que extienden el lenguaje secuencial tradicional que conocemos para poder crear codigo de programacion reactiva.Vamos a ver como lo hace .Reactive FrameworkLa Reactive Framework ha sido desarrollada en los Microsoft Live Labs y es otra de las creaciones de Erik Meijer el padre de Linq.Como hemos visto el objetivo de Rx es unir y simplificar la programacion basada en eventos complejos y la asincrona proporcionandonos un nuevo modelo de programacion para estos escenarios. Para conseguirlo se propuso un principio fundamental crear la dualidad entre el patron iterator y el patron observer.La propia gente de los Microsoft Live Labs la define asiDonde observableshace referencia al patron observer. Linqlo usamos para gestionar de una forma sencilla el patron iterator. Y schedulers se refiere a la planificacion de estas tareas en diferentes contextos de ejecucion.Para empezar en este articulo trataremos los dos patrones base de la formula para en futuras publicaciones poder dar una explicacion de cada uno de sus parametros.IteratorComenzamos con los conceptos base refrescando el patron iterator que nos provee de una forma de acceso a los elementos de un objeto agregado de forma secuencial sin exponer su representacion interna.En todas las versiones de la framework de .net se implementa este patron usando el contrato IEnumerable. Y normalmente cada una de las iteraciones de un enumerable la gestionamos con el bucle foreachvar iterable  new Listint  1 2 3 4 5 6 7 8 9 10 foreach var iterator in iterable   if iterator  10         System.Console.WriteLineiterator   System.Console.ReadLineGracias a Linq se ha encontrado una sintaxis comoda y rapida para trabajar con este tipo de objetos. Por lo que una forma de hacer lo mismo mediante esta tecnologia seriavar query  iterable.Whereiterator  iterator  10.Selectiterator   System.Console.WriteLineiterator return iterator  tengo que compilar la query para que se ejecute.query.ToArraySystem.Console.ReadLineObserverEl otro concepto base es el patron observador tambien conocido como publicacionsubscripcion o modelopatron. Este surge de una serie de sencillos conceptos Un objeto observable puede ser vigilado por objetos observadores. De esta forma un observable almacena referencias sus observadores y tendra la capacidad de notificarles cambios.Gracias a este patron podemos crear una especie de referencias debiles entre objetos. Ya que los observadores no guardan ninguna relacion con el observado.Para ver un ejemplo de como funciona este patron vamos a crear una clase que observara el estado del stock de un almacen. La llamaremos StockObserverpublic class StockObserver   public string Name  get set public void NotifyStock sender int unitsConsole.WriteLine0 The stock 1 has now 2 units this.Name sender.Name unitsEsta clase nos notificara de los cambios en el stock. Pero no vale nada si no creamos objetos observables a los que suscribirse. En este caso haremos una clase para cada tipo de stock que sea observable y nos podamos suscribir a ellapublic class Stock   private readonly IListStockObserver observersprivate int unitspublic Stockthis.observers  new ListStockObserverpublic string Name  get set public int Unitsgetreturn this.units  set       if this.units  value             this.units  value        this.NotifyObservers       public void SubscribeStockObserver observerSystem.Console.WriteLineobserver.Name    suscrito a    this.Namethis.observers.Addobserverpublic void UnsubscribeStockObserver observerif this.observers.Containsobserverthis.observers.Removeobserverprivate void NotifyObserversforeach var observer in this.observersobserver.Notifythis this.unitsUna vez tenemos nuestros objetos observador y observable podemos crear un pequeno programa que los utilice creamos stocksvar manzanas  new Stock  Name  Manzanas var peras  new Stock  Name  Peras var armarios  new Stock  Name  Armarios  creamos observadoresvar fruteria  new StockObserver var almacen  new StockObserver manzanas.Subscribefruteriamanzanas.Subscribealmacenperas.Subscribefruteriaperas.Subscribealmacenarmarios.SubscribealmacenConsole.WriteLineInicializa manzanas y peras a 20 unidadesmanzanas.Units  20peras.Units  20for int i  0 i  5 i  if i  2  0Console.WriteLineResto una manzanamanzanas.UnitselseConsole.WriteLineResto una peraperas.UnitsThread.Sleep1000Console.WriteLineInicializa armarios a 3 unidadesarmarios.Units  3Thread.Sleep1000Console.WriteLineSuma un armarioarmarios.Units  4System.Console.WriteLinepulsa intro...System.Console.ReadLineEn este pequeno programa veremos como creamos diferentes stocks de algunas frutas y armarios. Entonces crearemos dos observadores uno que observa el stock de las frutas y otro que observa el stock de todo el almacen incluidas las frutas. Suscribiremos los stocks a los observadores que proceda y jugamos con los valores para ver como son los observadores quienes nos informacion de los cambios en su zona de actuacion.Y hasta aqui este capitulo introductorio a las reactive extensions. En proximas publicaciones intentaremos hablar mas de ellas y resolver uno a uno todos los operadores que forman parte de la formula que la define. Por lo que espero que no os perdais el siguiente capitulo de Observables Subjects."
    } ,
  
    {
      "title"    : "Usando la palabra clave &#8220;ref&#8221;",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/12/07/usando-la-palabra-clave-ref",
      "date"     : "2011-12-07 11:45:20Z",
      "content"  : "No hace mucho estabamos tomando unas cervezas algunos desarrolladores de .net de Barcelona. Entonces salio el tema de los talibanes del codigo. Cosas como fxCop o styleCop. Aunque estas herramientas y sus reglas dan para varios articulos de debate el caso es que surgio una norma del code analysis fxCop que recomienda no usar la palabra en clave ref.En ese momento varios afirmamos que estaba bien porque no era necesario al menos con objetos. Entonces uno de los comensales Jaume Jornet jaumejornet rapidamente respondio con un es un dobleasterisco de c. A partir de ahi ya le habiamos comprado la respuesta pero puso un ejemplo rapido usando un dibujo del heap y otro del stack que para las personas que siempre han usado lenguajes manejados puede resultar muy util.Como es un tema muy interesante vamos a intentar ampliar la explicacion que nos dio de Jaume y comentar el uso de la palabra en clave ref.Antes de empezar vamos a explicar vagamente que es la stack y que el heapStack es la pila. Una seccion de memoria secuencial Last In First Out y de acceso rapido. Ahi se almacenaran estructuras struct int long bool ... y punteros a datos mas complejos que se almacenan en heap o a la propia stack. Una pila tiene un tamano fijo y es de acceso rapido.Heap es la memoria como comunmente la conocemos. Puede crecer segun las necesidades es de acceso aleatorio no sabemos cuanto tardara en realizar la operacion. Aqui se almacenan los objetos y datos complejos.Esta descripcion quiza no sea la mas exacta y tampoco la que mas aclara. Asi que vamos a ver un ejemplopublic class Person   public string Name  get set    public int Age  get set int a  3long b  4var c  new PersonEn el momento de ejecutar ese codigo quedaria asi almacenado en azul stack y en rojo heapPara los que no vienen de lenguajes como c o c   cuando vemos el simbolo asterisco delante del nombre de una variable significa que es donde se almacena la direccion de memoria el lugar en el heap del objeto. Pero el objeto verdaderamente no esta ahi. Solo es una referencia a la zona del heap donde esta almacenado. Un puntero.Ahora declaramos una funcion de esta formapublic void Method1int i Person p   i  12   p.Name  FernandoMethod1a cEsta funcion cuando es llamada genera dos nuevas entradas en la stack una de tipo entero int con el valor de a copiado y otra de tipo puntero a persona con la misma direccion de memoria que la anteriorEntonces vemos que si dentro de la funcion cambiamos el valor del entero este es una copia y se cambia en la pila stack. Pero si cambiamos el valor del nombre de la persona aunque el puntero sea una copia referencia la misma direccion de memoria que la variable c.Por esta razon cuando termine la ejecucion del metodo la variable a seguira valiendo 3 pero el nombre de c sera Fernando. Y de aqui viene ese dicho que reza que en c y otros lenguajes manejados los objetos se pasan siempre por referencia. Porque lo cierto es que se copia la referencia o direccion en el heap de ese objeto en la pila.Si queremos cambar el valor del entero tendremos que pasarlo por referencia usando la palabra en clave refpublic void Method2ref int i Person p   i  12   p.Name  FernandoMethod2ref a cAl escribir la palabra ref la variable que se crea en la pila en lugar de ser un entero sera una referencia un puntero a la direccion de la pila donde se encuentra el entero original. Y su ejecucion causaria estoComo antes hemos dicho que el objeto p ya se pasaba por referencia sin necesitad del comandoref que pasaria si lo reasignamos a un nuevo valorpublic void Method3ref int i Person p   i  12   p.Name  fernando   p  new Person   p.Name  pabloMethod3ref a cEn este contexto el objeto p es una copia de la direccion de memoria del objeto c original. Y al asignarle una nueva instancia se cambia la direccion de memoria y genera un nuevo bloque en el heap. Pero el objeto c permanece apuntando al originalAsi pues llegamos al ultimo punto que pasaria si quisieramos cambiar la instancia original Entonces tendriamos que crear el famoso doblepuntero. O lo que es lo mismo una referencia del objeto Personpublic void Method4int i ref Person p   p  new Person   p.Name  pabloMethod4a ref cAsi al salir del metodo cuarto el valor de c seria la direccion de memoria del nuevo objeto person con nombre pabloAhora os invito a que hagais estas pruebas y observeis por vosotros mismos este comportamiento."
    } ,
  
    {
      "title"    : "TDD en entornos .net #bdc11",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/11/21/tdd-en-entornos-net-bdc11",
      "date"     : "2011-11-21 15:00:08Z",
      "content"  : "La semana pasada los dias 17 18 y 19 estuvimos en la Barcelona Developers Conference.El primer dia por la manana tuve el placer de ademas de escuchar grandes ponencias dar una charla sobre TDD en entornos .net. Y de coletilla para el mundo real. Esta ultima frase que hacia del titulo algo parecido a una pelicula mala de antena 3 por la tarde venia a raiz de que mucha gente habla de TDD y casi nadie de como se aplica realmente en el desarrollo diario.Durante poco mas de una hora intente que los asistentes entendieran el por que de esta metodologia como nos beneficia y como aplicarla en el desarrollo diario.Si no pudisteis asistir os dire que TDD son las siglas de Test Driven Development o en castellano desarrollo guiado por las pruebas. Y es una tecnica de desarrollo de eXtreme Programing que se basa en dos pasos primero escribir la pruebas y despues refactorizar.La programacion extrema engloba una serie de metodologias de programacion agiles basadas en que a lo largo de un desarrollo van a ocurrir cambios de especificaciones y en lugar de intentar prevenir esto con una gran cantidad de codigo que en ocasiones sobra se decide adaptarse a esos cambios en cualquier fase del ciclo de vida del proyecto.Sobre refactorizacion es una tecnica de la ingenieria del software que reestructura el codigo fuente sin cambiar su funcionalidad. Es decir cambiar nombres de variables y metodos reordenar el codigo dividir una funcion en varias . Algo que se conoce comunmente como limpiar el codigo.Las pruebas se escriben generalmente como pruebas unitarias o unit tests en ingles. Una prueba unitaria es una forma de probar que un determinado bloque de codigo fuente funciona correctamente. Es decir mediante un pequeno programa comprobamos que cada porcion de codigo fuente escrita funciona correctamente por separado. Y aunque estas pruebas no nos garantizan el funcionamiento global de la aplicacion pueden ser completadas con otro tipo de tests como los de integracion que comprueban que los diferentes artefactos funcionan correctamente unos con otros.Para que se considere una prueba unitaria valida esta debe serAutomatizable que significa que se puede ejecutar sin intervencion humana. CIRapida una prueba de este tipo no debe tardar un gran espacio de tiempo tiene que ser practicamente instantaneo el resultado de su ejecucion.Repetible es decir que la podemos ejecutar tantas veces como queramos sin obtener resultados diferentes cada vez. una prueba que solo se puede ejecutar una vez no es una buena prueba.Independiente La ejecucion de una prueba no debe influir en el resto de pruebas.Profesional y decimos profesional porque una prueba ha de hacerse con el mismo esfuerzo y dedicacion que se haria cualquier otra parte de codigo. Hay que mantenerla y refactorizarla si es necesario.Y como en mucho proyectos se tiende a olvidar este punto lo repetimos   una prueba unitaria es parte del codigo de la aplicacion.La estructura de un test unitario es simpleArrange Disponer lo necesario para la pruebaAct Realizar el acto que dara un resultado. La prueba en si.Assert comprobar que el resultado de la prueba es correcto.Triple A como los videojuegos buenos.Pero vamos a volver al concepto inicial desarrollo guiado por las pruebas.Generalmente expresado como Red  Green  Refactor expone un diagrama de actividad simplificado de esta tecnica.Quiere decir que primero vamos a hacer un test unitario comprobaremos que falla Red se completa el codigo de la forma mas simple solo para que este pase Green y una vez funciona se refactoriza el codigo para asi limpiarlo de malos nombres espacios de nombre confusos o de codigo repetitivo.Pero hemos dicho que esta es la version simple.En la version real nosotros nos encontrariamos dentro de un proyecto. Como estaremos usando metodologias agiles algo como scrum tendremos un product backlog para el sprint actual. Dentro de este encontraremos las historias de usuario que nos hablan de un requerimiento del software que estamos desarrollando. Y este requerimiento lo dividiremos en una o varias especificaciones.Una vez tenemos claro lo que tenemos que probar la especificacion realizamos nuestro test unitario para acto seguido comprobar que este falla. Es muy importante comprobar esto porque si no falla de buenas a primeras es porque nuestro test es posible que no compruebe nada en realidad.Una vez tenemos nuestro test fallando podemos ponernos a codificar la minima cantidad de codigo necesaria para que se vea cumplido. O seguir el principioKISS Keep It Simple Stupid.El siguiente paso seria comprobar que todas las pruebas que tenemos escritas pasan. Es decir no solo la actual si no todas las demas porque podemos haber roto algo.A partir de aqui se nos permitira hacer una refactorizacion. Hay que serpragmaticosymodificar el codigo pero no su comportamiento. Y aplicar el principio DRY Dont Repeat Yourself. Entonces volvemos a comprobar de nuevo que todos los tests pasany pasamos a la siguiente especificacion.Me gustaria poneros un ejemplo en la bdc11 si lo hicimos aunque muy rapido. Aunque creo que esto queda muy claro en vivo leyendo es posible que no aclare demasiado... Asi que explicare como TDD nos llevo a crear clases y propiedades. Como tuvimos que desacoplar artefactos usando interfaces y al final nos decidimos a usar inyeccion de dependencias. Pero solo cuando el propio test nos llevo a ello.Para terminar con este enorme post comentar los beneficios de esta tecnicaEsto que vemos es una curva de coste. Se suele utilizar para representar el coste de desarrollo de una aplicacion en dependencia de la metodologia utilizada. Muy comun cuando la tematica de la charla es sobredeuda tecnica.En el eje vertical vemos el coste y en el horizontal el tiempo. La linea naranja es el desarrollo usando test driven development y la roja el metodo tradicional de codificar.La linea roja empieza a tener resultados muy rapidamente pero el coste de desarrollo se va haciendo mas grande con el tiempo. Al final existe una cantidad enorme de codigo sin ningun test que lo compruebe. Y el mas minimo cambio tiene un impacto en el coste muy grande.Por otro lado vemos en un principio un desarrollo TDD es mas costoso y tardara mas tener resultados semejantes al otro. Eso sin contar con el tiempo de aprendizaje del equipo. Pero la curva se estabiliza rapidamente. Y una vez estabilizada y con una serie de test de respaldo las modificaciones no se hacen a ciegas y ademas son mas simples ya que esta tecnica nos ha hecho seguir otra serie de principios de desarrollo comoYAGNI You Aint Gonna Need It. TDD nos ayuda a prevenir la escritura de codigo que no va a ser utilizado en nuestra aplicacion. Basicamente porque solo escribimos lo minimo necesario para cubrir las especificaciones.Menosuso del debugger se dice que los buenos programadores de TDD dejan de usar el debugger ya que no lo necesitan. Todo su codigo ya esta probado. PInterface Segregation Principle es un principio SOLID la i concretamente que dice que debemos segregar las responsabilidades en interfaces. TDD nos obliga a separar componente segun responsabilidad porque los test unitarios como norma no pueden comprobar mas de una funcionalidad en un solo artefacto unitario.Dependency Injection el patron de inyeccion de dependencias dice que deberiamos crear un contendor que sea quien controle y resuelva las dependencias de todos los artefactos de nuestra aplicacion. Y seguiremos este patron para facilitarnos la creacion de objetos mediante interfaces correctamente segregadas.Y estos dos ultimos punto nos llevan a seguir otro principio de SOLID Dependency Inversion Principle. Que dice que una clase deberia de depender de las abstracciones y no al contrario.Desarrollaremos siguiendo los principios agiles dividimos el problema en pequenos pasos y lo vamos solucionando uno a uno. Buscaremos especificaciones y las codificaremos una a una con su test unitario. Nos adaptaremos a los cambios rapidamente.Y algo mas evidente como que tendremos una mayor cobertura de codigo con test unitarios. Esto nos aportara una red de seguridad de cara a los cambios ya que si alguien cambia algo que hace fallar una especificacion por antigua que esta sea un test fallara.Y hasta aqui puedo escribir debajo encontrareis el archivo con la presentacion de la ponencia..."
    } ,
  
    {
      "title"    : "Introducción a Windows Phone 7",
      "category" : "",
      "tags"     : "",
      "url"      : "/2011/11/20/introduccion-a-windows-phone-7",
      "date"     : "2011-11-20 21:30:03Z",
      "content"  : "Windows Phone 7 es un sistema operativo movil desarrollado por Microsoft como sustituto de Windows Mobile. Pero se diferencia de este ultimo en que esta pensando para un mercado generalista en detrimento del empresarial. En adicion no mantiene compatibilidad con su antecesor debido a que es un desarrollo nuevo y no una evolucion.Dentro de las caracteristicas que hacen diferente a Windows Phone 7 del resto de productos que existen en el mercado encontramosLa novedosa pantalla de inicio. Esta se basa en mosaicos dinamicos que nos muestran informacion en tiempo real como mensajes recibidos llamadas correos electronicos etc. Esta novedosa interfaz recibe el nombre de Metro.El sistema tiene integrado el navegador Internet Explorer Mobile. Que en su ultima version incorpora compatibilidad con HTML5.Tambien se favorece de los diferentes servicios de Bing como es el buscador y los mapas.Los hubs que clasifican acciones y agrupan aplicaciones correspondientes con una actividad concreta. Asi pues encontraremos hubs de contactos imagenes camara musica video la suite de Office juegos y el marketplace.El marketplace es el lugar donde se agrupan las aplicaciones y contenidos disponibles. Es decir un servicio web donde poder buscar comprar y descargar las aplicaciones musica peliculas y juegos para nuestro telefono movil. Semejante a la App Store de IOS o al Market de android. Pero con un espectro algo mas amplio.Windows Phone 7 apuesta por un modelo base de hardware para garantizar compatibilidad. O dicho de otra forma una serie de caracteristicas base que se pueden ampliar pero no recortar en un terminal movil para que pueda instalar Windows Phone 7.A estas especificaciones se le llaman Chasis. Y aqui podemos ver la primera versionLas razones de hacer esto es poder garantizar que todo telefono con Windows Phone 7 de la misma experiencia satisfactoria al usuario. Al contrario de lo que puede pasar con algun terminal de bajas prestaciones android. Pero Microsoft tampoco desea crear un sistema cerrado que solo funcione en un solo terminal como podria ser un iPhone.Podriamos decir que es un punto intermedio entre las filosofias de sus competidores mas directos.de donde viene windows phone 7En un principio el sistema para moviles de Microsoft era una evolucion del sistema de pocket PC. Se manejaba con un puntero y nos ofrecia todas las caracteristicas que un ordenador de sobremesa ayudandose de herramientas para la sincronizacion entre ambos.Este sistema parecia un poco anticuado y fue cuando se dieron cuenta de que tenian que crear algo nuevo. Alli nacio el proyecto con nombre en clave Photon. Donde aunarian las caracteristicas de un Smartphone y un PocketPC en un sistema revolucionario.Lo cierto es que este desarrollo no fue a parar a buen puerto asi que en 2008 Microsoft reorganiza el equipo inicial y proponen el desarrollo del sistema Windows Mobile 7 con MultiTouch.El ano 2009 era el designado para su lanzamiento pero al no conseguir resultados a tiempo se decide entre tanto sacar una nueva version de Windows Mobile la 6.5.Mientras el desarrollo del nuevo sistema multitouch seguia y se le cambiaba el nombre a Windows Phone 7 Series y seria presentado a lo largo del primer semestre del ano 2010 en ferias como la World Mobile Congress de Barcelona o el MIX de ese mismo ano.En septiembre de 2010 se publica el primer juego de herramientas para desarrolladores de Windows Phone 7. El SDK. Y entre octubre y noviembre se lanzan a la venta los primeros terminales con el novedoso sistema de Microsoft instalado.A principios de este ano 2011 se anuncia un pacto entre Nokia y Microsoft para competir contra sistemas Android y el iPhone. Nokia vendera telefonos con Windows Phone 7 e incluira del buscador Bing en sus terminales. Mientras que Microsoft heredara el sistema de mapas de la compania finlandesa. Esto no implica que Nokia vaya a dejar su sistema operativo el Symbian. Pero si puede significar un gran impulso para la plataforma de Microsoft y para los ya no tan novedosos terminales de Nokia.Y a finales de septiembre ocurre el ultimo gran hito y posiblemente el mas importante por ahora en la vida de Windows Phone 7 La aparicion de la version 7.1 renombrada por cuestiones comerciales a 7.5 y cuyo nombre en clave es Mango."
    } 
  
]
